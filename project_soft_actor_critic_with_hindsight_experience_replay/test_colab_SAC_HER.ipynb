{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_colab_SAC_HER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoOEob3gB00H",
        "outputId": "ff995a3f-1dd2-47f1-8b14-18ed2aa87ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/gym/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gym[mujoco]"
      ],
      "metadata": {
        "id": "OBwd4nBjeHbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "25d78179-25e4-4fb0-c94d-d00cfc8bbbaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[mujoco]\n",
            "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
            "\u001b[K     |████████████████████████████████| 734 kB 5.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.21.6)\n",
            "Collecting imageio>=2.14.1\n",
            "  Downloading imageio-2.21.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 41.6 MB/s \n",
            "\u001b[?25hCollecting mujoco==2.2.0\n",
            "  Downloading mujoco-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 35.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (3.1.6)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.4-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 19.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.2.0)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852320 sha256=e68da552a8cb2bff902ae72c949f136d3fee55db4b18d37c0f8e908706376ae1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a8/81/4ba83fc99a5637e27f4e16da10f9e15ff61f77ce524d23a8d7\n",
            "Successfully built gym\n",
            "Installing collected packages: pillow, glfw, mujoco, imageio, gym\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed glfw-2.5.4 gym-0.25.2 imageio-2.21.2 mujoco-2.2.0 pillow-9.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SparseReacher2D:\n",
        "\n",
        "    def __init__(self, delta=0.015):\n",
        "        self._env = gym.make(\"Reacher-v4\")\n",
        "        self._delta = delta\n",
        "\n",
        "    def observation_space_shape(self):\n",
        "        return self._env.observation_space.shape\n",
        "\n",
        "    def environment(self):\n",
        "        return self._env\n",
        "\n",
        "    def step(self, actions):\n",
        "        return self._env.step(actions)\n",
        "\n",
        "    def reset(self):\n",
        "        return self._env.reset()\n",
        "\n",
        "    # reacher gives the vector from fingertip to target instead of fingertip coordinates\n",
        "    # we need to extract the achieved goal out of this vector\n",
        "    def achieved_goal(self, state):\n",
        "        x_t, y_t = self.desired_goal(state)\n",
        "        x_v, y_v = state[8], state[9]  # fingertip to target vector\n",
        "        x_g, y_g = x_v + x_t, y_v + y_t  # fingertip coordinates\n",
        "        return x_g, y_g\n",
        "\n",
        "    def desired_goal(self, state):\n",
        "        return state[4], state[5]\n",
        "\n",
        "    def set_goal(self, state, goal):\n",
        "        x_g, y_g = goal\n",
        "        x, y = self.achieved_goal(state)\n",
        "        new_state = np.array(state)\n",
        "        new_state[4], new_state[5] = x_g, y_g\n",
        "        new_state[8], new_state[9] = x - x_g, y - y_g  # create fingertip to goal vector\n",
        "        return new_state\n",
        "\n",
        "    # check if distance between goal and fingertip is lower than epsilon\n",
        "    def reward(self, state):\n",
        "        return -1 if np.linalg.norm([state[8], state[9]]) > self._delta else 0\n",
        "\n",
        "    def success(self, state):\n",
        "        return self.reward(state) >= -0.0"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oCYunP1G2nNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install panda_gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "IORQR0jiBLcK",
        "outputId": "d1d64f45-115b-4cd5-9007-2d19687734c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting panda_gym\n",
            "  Downloading panda_gym-2.0.4-py3-none-any.whl (26 kB)\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting gym<=0.23,>=0.22\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[K     |████████████████████████████████| 624 kB 65.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from panda_gym) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from panda_gym) (1.21.6)\n",
            "Collecting gym-robotics\n",
            "  Downloading gym-robotics-0.1.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 50.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23,>=0.22->panda_gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23,>=0.22->panda_gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23,>=0.22->panda_gym) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23,>=0.22->panda_gym) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23,>=0.22->panda_gym) (3.8.1)\n",
            "Building wheels for collected packages: gym, gym-robotics\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697661 sha256=a6372d2582e9a12a0e67f5c025d6ab9f3c9602701fa92a4ba5d47e437a757fde\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/7e/16/4d727df048fdb96518ec5c02266e55b98bc398837353852a6a\n",
            "  Building wheel for gym-robotics (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-robotics: filename=gym_robotics-0.1.0-py3-none-any.whl size=1449837 sha256=790ab4499068688096178ab3affb6cea82674ad9c0da0494af7c231912e6dde4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/b7/09/0b8ea8078e26e9bdae985f1972f4e9545b97e4bf282411f241\n",
            "Successfully built gym gym-robotics\n",
            "Installing collected packages: gym, pybullet, gym-robotics, panda-gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 gym-robotics-0.1.0 panda-gym-2.0.4 pybullet-3.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import panda_gym\n",
        "\n",
        "\n",
        "class SparseReacher3D:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._env = gym.make(\"PandaReach-v2\")\n",
        "\n",
        "    def observation_space_shape(self):\n",
        "        return (9,)\n",
        "\n",
        "    def environment(self):\n",
        "        return self._env\n",
        "\n",
        "    def _select_observations(self, observation):\n",
        "        return np.concatenate((observation['observation'], observation['desired_goal']))\n",
        "\n",
        "    def step(self, actions):\n",
        "        observation, reward, done, info = self._env.step(actions.numpy())  # converting to numpy before would be better\n",
        "        return self._select_observations(observation), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return self._select_observations(self._env.reset())\n",
        "\n",
        "    def achieved_goal(self, state):\n",
        "        return np.array([state[0], state[1], state[2]])\n",
        "\n",
        "    def desired_goal(self, state):\n",
        "        return np.array([state[6], state[7], state[8]])\n",
        "\n",
        "    def set_goal(self, state, goal):\n",
        "        new_state = np.array(state)\n",
        "        new_state[6], new_state[7], new_state[8] = goal\n",
        "        return new_state\n",
        "\n",
        "    def reward(self, state):\n",
        "        return self._env.compute_reward(self.achieved_goal(state), self.desired_goal(state), {})\n",
        "\n",
        "    def success(self, state):\n",
        "        return self.reward(state) >= -0.0"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ir5XR3eM2nNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, state_dims, action_dims, max_size=1000000, batch_size=256):\n",
        "        self._max_size = max_size\n",
        "        self._batch_size = batch_size\n",
        "        self._size = 0\n",
        "        self._current_position = 0\n",
        "        self._state_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._state_prime_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._action_memory = np.zeros((self._max_size, action_dims))\n",
        "        self._reward_memory = np.zeros((self._max_size, 1))\n",
        "        self._done_memory = np.zeros((self._max_size, 1), dtype=bool)\n",
        "\n",
        "    def size(self):\n",
        "        return self._size\n",
        "\n",
        "    def ready(self):\n",
        "        return self._size >= self._batch_size\n",
        "\n",
        "    def add_transition(self, state, action, reward, state_, done):\n",
        "        self._state_memory[self._current_position] = state\n",
        "        self._state_prime_memory[self._current_position] = state_\n",
        "        self._action_memory[self._current_position] = action\n",
        "        self._reward_memory[self._current_position] = reward\n",
        "        self._done_memory[self._current_position] = done\n",
        "        # self.un_norm_r[self.current_position] = r\n",
        "        # self.r = (self.un_norm_r - np.mean(self.un_norm_r)) / (np.std(self.un_norm_r) + 1e-10)\n",
        "        if self._size < self._max_size:\n",
        "            self._size += 1\n",
        "        self._current_position = (self._current_position + 1) % self._max_size\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch_indices = np.random.choice(self._size, self._batch_size, replace=False)\n",
        "        states = tf.convert_to_tensor(self._state_memory[batch_indices], dtype=tf.float32)\n",
        "        states_prime = tf.convert_to_tensor(self._state_prime_memory[batch_indices], dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self._action_memory[batch_indices], dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(self._reward_memory[batch_indices], dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(self._done_memory[batch_indices], dtype=tf.float32)\n",
        "        return states, actions, rewards, states_prime, dones\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Concatenate\n",
        "\n",
        "\n",
        "def create_policy_network(learning_rate, state_dim, action_dim):\n",
        "    inputs = keras.Input(shape=state_dim)\n",
        "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    mu = Dense(action_dim, activation=None)(x)\n",
        "    sigma = Dense(action_dim, activation=tf.nn.softplus)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=(mu, sigma))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_q_network(learning_rate, state_dim, action_dim):\n",
        "    inputs_s = keras.Input(shape=state_dim)\n",
        "    inputs_a = keras.Input(shape=action_dim)\n",
        "    x = Concatenate()([inputs_s, inputs_a])\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    out = Dense(1, activation=None)(x)\n",
        "    model = keras.Model(inputs=(inputs_s, inputs_a), outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "# from ExperienceReplayBuffer import ExperienceReplayBuffer\n",
        "import tensorflow as tf\n",
        "from tensorflow import math as tfm\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# input actions are always between (−1, 1)\n",
        "def default_scaling(actions):\n",
        "    return actions\n",
        "\n",
        "\n",
        "# input actions are always between (−1, 1)\n",
        "def multiplicative_scaling(actions, factors):\n",
        "    return actions * factors\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, environment, state_dim, action_dim,\n",
        "                 actor_network_generator, critic_network_generator, action_scaling=default_scaling,\n",
        "                 learning_rate=0.0003, gamma=0.99, tau=0.005, reward_scale=1, alpha=0.2,\n",
        "                 batch_size=256, max_replay_buffer_size=1000000):\n",
        "        self._environment = environment\n",
        "        self._action_dim = action_dim\n",
        "        self._action_scaling = action_scaling\n",
        "        self._gamma = gamma\n",
        "        self._tau = tau\n",
        "        self._reward_scale = reward_scale\n",
        "        self._alpha = alpha\n",
        "        self._batch_size = batch_size\n",
        "        self._mse = tf.keras.losses.MeanSquaredError()\n",
        "        self._reply_buffer = ExperienceReplayBuffer(state_dim, action_dim, max_replay_buffer_size, batch_size)\n",
        "        self._actor = actor_network_generator(learning_rate)\n",
        "        self._critic_1 = critic_network_generator(learning_rate)\n",
        "        self._critic_2 = critic_network_generator(learning_rate)\n",
        "        self._critic_1_t = critic_network_generator(learning_rate)\n",
        "        self._critic_2_t = critic_network_generator(learning_rate)\n",
        "        self._wight_init()\n",
        "\n",
        "    def reply_buffer(self):\n",
        "        return self._reply_buffer\n",
        "\n",
        "    def environment(self):\n",
        "        return self._environment\n",
        "\n",
        "    def _wight_init(self):\n",
        "        self._critic_1.set_weights(self._critic_1_t.weights)\n",
        "        self._critic_2.set_weights(self._critic_2_t.weights)\n",
        "\n",
        "    def update_target_weights(self):\n",
        "        self._weight_update(self._critic_1_t, self._critic_1)\n",
        "        self._weight_update(self._critic_2_t, self._critic_2)\n",
        "\n",
        "    def _weight_update(self, target_network, network):\n",
        "        new_wights = []\n",
        "        for w_t, w in zip(target_network.weights, network.weights):\n",
        "            new_wights.append((1 - self._tau) * w_t + self._tau * w)\n",
        "        target_network.set_weights(new_wights)\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, states_prime, dones = self._reply_buffer.sample_batch()\n",
        "        self.train_step_critic(states, actions, rewards, states_prime, dones)\n",
        "        self.train_step_actor(states)\n",
        "        self.update_target_weights()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_critic(self, states, actions, rewards, states_prime, dones):\n",
        "        actions_prime, log_probs = self.sample_actions_form_policy(states_prime)\n",
        "        q1 = self._critic_1_t((states_prime, actions_prime))\n",
        "        q2 = self._critic_2_t((states_prime, actions_prime))\n",
        "        q_r = tfm.minimum(q1, q2) - self._alpha * log_probs\n",
        "        targets = self._reward_scale * rewards + self._gamma * (1 - dones) * q_r\n",
        "        self._critic_update(self._critic_1, states, actions, targets)\n",
        "        self._critic_update(self._critic_2, states, actions, targets)\n",
        "\n",
        "    def _critic_update(self, critic, states, actions, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q = critic((states, actions))\n",
        "            loss = 0.5 * self._mse(targets, q)\n",
        "        gradients = tape.gradient(loss, critic.trainable_variables)\n",
        "        critic.optimizer.apply_gradients(zip(gradients, critic.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_actor(self, states):\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_new, log_probs = self.sample_actions_form_policy(states)\n",
        "            q1 = self._critic_1((states, actions_new))\n",
        "            q2 = self._critic_2((states, actions_new))\n",
        "            loss = tfm.reduce_mean(self._alpha * log_probs - tfm.minimum(q1, q2))\n",
        "            # equal to loss = -tfm.reduce_mean(tfm.minimum(q1, q2) - self._alpha * log_probs)\n",
        "        gradients = tape.gradient(loss, self._actor.trainable_variables)\n",
        "        self._actor.optimizer.apply_gradients(zip(gradients, self._actor.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def sample_actions_form_policy(self, state):\n",
        "        mu, sigma = self._actor(state)\n",
        "        # MultivariateNormalDiag(loc=mus, scale_diag=sigmas) other option\n",
        "        distribution = tfd.Normal(mu, sigma)\n",
        "        actions = distribution.sample()\n",
        "        log_probs = distribution.log_prob(actions)\n",
        "        actions = tfm.tanh(actions)\n",
        "        log_probs -= tfm.log(1 - tfm.pow(actions, 2) + 1e-6)  # + 1e-6 because log undefined for 0\n",
        "        log_probs = tfm.reduce_sum(log_probs, axis=-1, keepdims=True)\n",
        "        return actions, log_probs\n",
        "\n",
        "    def act_deterministic(self, state):\n",
        "        actions_prime, _ = self._actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def act_stochastic(self, state):\n",
        "        actions_prime, _ = self.sample_actions_form_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def _act(self, actions):\n",
        "        scaled_actions = self._action_scaling(actions)  # scaled actions from (-1, 1) according (to environment)\n",
        "        observation_prime, reward, done, _ = self._environment.step(scaled_actions[0])\n",
        "        return actions, observation_prime, reward, done\n",
        "\n",
        "    def train(self, epochs, environment_steps=1, training_steps=1, pre_sampling_steps=1024):\n",
        "        print(f\"Random exploration for {pre_sampling_steps} steps!\")\n",
        "        observation = self._environment.reset()\n",
        "        ret = 0\n",
        "        for _ in range(max(pre_sampling_steps, self._batch_size)):\n",
        "            actions = tf.random.uniform((self._action_dim,), minval=-1, maxval=1)\n",
        "            scaled_actions = self._action_scaling(actions)  # scaled actions from (-1, 1) according (to environment)\n",
        "            observation_prime, reward, done, _ = self._environment.step(scaled_actions)\n",
        "            ret += reward\n",
        "            self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "            if done:\n",
        "                print(\"print\", ret)\n",
        "                ret = 0\n",
        "                observation = self._environment.reset()\n",
        "            else:\n",
        "                observation = observation_prime\n",
        "        print(\"print\", ret)\n",
        "\n",
        "        print(\"start training!\")\n",
        "        returns = []\n",
        "        observation = self._environment.reset()\n",
        "        done = 0\n",
        "        ret = 0\n",
        "        epoch = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < environment_steps or self._reply_buffer.size() < self._batch_size:\n",
        "                if done:\n",
        "                    observation = self._environment.reset()\n",
        "                    returns.append(ret)\n",
        "                    print(\"epoch:\", epoch, \"steps:\", steps, \"return:\", ret, \"avg return:\", np.average(returns[-50:]))\n",
        "                    ret = 0\n",
        "                    epoch += 1\n",
        "                    if epoch >= epochs:\n",
        "                        print(\"training finished!\")\n",
        "                        return\n",
        "                actions, observation_prime, reward, done = self.act_stochastic(observation)\n",
        "                self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "                observation = observation_prime\n",
        "                steps += 1\n",
        "                ret += reward\n",
        "                i += 1\n",
        "            for _ in range(training_steps):\n",
        "                self.learn()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def final_goal_sampling_strategy(trajectory, current_index, environment):\n",
        "    _, _, s_p, _ = trajectory[-1]\n",
        "    g = environment.achieved_goal(s_p)\n",
        "    return [g]\n",
        "\n",
        "\n",
        "def k_final_goal_sampling_strategy(trajectory, current_index, environment, k=4):\n",
        "    return final_goal_sampling_strategy(trajectory, current_index, environment) * k\n",
        "\n",
        "\n",
        "def future_goal_sampling_strategy(trajectory, current_index, environment, k=4):\n",
        "    goals = []\n",
        "    for _ in range(k):\n",
        "        i = np.random.randint(current_index, len(trajectory))\n",
        "        _, _, s_p, _ = trajectory[i]\n",
        "        goals.append(environment.achieved_goal(s_p))\n",
        "    return goals\n",
        "\n",
        "\n",
        "def no_goal_sampling_strategy(trajectory, current_index, environment):\n",
        "    return []\n",
        "\n",
        "\n",
        "class HindsightExperienceReplayBuffer:\n",
        "\n",
        "    def __init__(self, agent, goal_sampling_strategy=final_goal_sampling_strategy):\n",
        "        self._agent = agent\n",
        "        self._goal_sampling_strategy = goal_sampling_strategy\n",
        "        self._replay_buffer = agent.reply_buffer()\n",
        "        self._environment = self._agent.environment()\n",
        "\n",
        "    def evaluate(self, steps, epoch, successes, avg_returns):\n",
        "        success_cnt = 0\n",
        "        rets = []\n",
        "        for _ in range(steps):\n",
        "            state = self._environment.reset()\n",
        "            done = False\n",
        "            ret = 0\n",
        "            while not done:\n",
        "                _, state, reward, done = self._agent.act_deterministic(state)\n",
        "                ret += self._environment.reward(state)\n",
        "                if self._environment.success(state):\n",
        "                    success_cnt += 1\n",
        "                    done = True\n",
        "            rets.append(ret)\n",
        "        avg_return = np.average(rets)\n",
        "        success_rate = success_cnt / steps\n",
        "        successes.append(success_rate)\n",
        "        avg_returns.append(avg_return)\n",
        "        print(f\"epoch {epoch}: avg return={avg_return}, success rate={success_rate} (with {steps} evaluation steps)\")\n",
        "\n",
        "    def train(self, epochs=200, cycles=50, episodes=16, n=40, t=1000,\n",
        "              eval_steps=100, save_eval=False, eval_name='evaluation'):\n",
        "        successes = []\n",
        "        avg_returns = []\n",
        "        self.evaluate(eval_steps, 0, successes, avg_returns)\n",
        "        for e in range(1, epochs + 1):\n",
        "            for _ in range(cycles):\n",
        "                for _ in range(episodes):\n",
        "                    state = self._environment.reset()\n",
        "                    trajectory = []\n",
        "                    dones = 0\n",
        "                    j = 0\n",
        "                    while not dones and j < t:\n",
        "                        actions, state_prime, r, dones = self._agent.act_stochastic(state)\n",
        "                        trajectory.append((state, actions, state_prime, dones))\n",
        "                        state = state_prime\n",
        "                        j += 1\n",
        "                        if self._environment.success(state):\n",
        "                            dones = True\n",
        "                    for i, (state, actions, state_prime, dones) in enumerate(trajectory):\n",
        "                        reward = self._environment.reward(state_prime)\n",
        "                        self._replay_buffer.add_transition(state, actions, reward, state_prime, dones)\n",
        "                        goals = self._goal_sampling_strategy(trajectory, i, self._environment)\n",
        "                        for g in goals:\n",
        "                            state_new = self._environment.set_goal(state, g)\n",
        "                            state_prime_new = self._environment.set_goal(state_prime, g)\n",
        "                            reward_new = self._environment.reward(state_prime_new)\n",
        "                            self._replay_buffer.add_transition(state_new, actions, reward_new, state_prime_new, dones)\n",
        "                if self._replay_buffer.ready():\n",
        "                    for i in range(n):\n",
        "                        self._agent.learn()\n",
        "            self.evaluate(eval_steps, e, successes, avg_returns)\n",
        "        if save_eval:\n",
        "            data = {'epoch': range(epochs + 1), 'success rate': successes, 'average return': avg_returns}\n",
        "            df = pd.DataFrame.from_dict(data)\n",
        "            df.to_csv(f'{eval_name}.csv')\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "\n",
        "# from final_project.Networks.GenericMLPs1D import create_policy_network, create_q_network\n",
        "# from SoftActorCriticAgent import Agent, multiplicative_scaling\n",
        "# from Environments.SparseReacher3D import SparseReacher3D\n",
        "# from HER import HindsightExperienceReplayBuffer, future_goal_sampling_strategy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.keras.backend.clear_session()\n",
        "    environment = SparseReacher3D()\n",
        "    env = environment.environment()\n",
        "    state_dim = environment.observation_space_shape()\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    action_scaling = env.action_space.high\n",
        "    print(\"state_dim=\", state_dim, \"action_dim=\", action_dim, \"action_scaling:\", action_scaling)\n",
        "    agent = Agent(environment=environment, state_dim=state_dim, action_dim=action_dim, alpha=0.05,\n",
        "                  action_scaling=partial(multiplicative_scaling, factors=action_scaling),\n",
        "                  actor_network_generator=partial(create_policy_network, state_dim=state_dim[0], action_dim=action_dim),\n",
        "                  critic_network_generator=partial(create_q_network, state_dim=state_dim[0], action_dim=action_dim))\n",
        "    her = HindsightExperienceReplayBuffer(agent, goal_sampling_strategy=future_goal_sampling_strategy)\n",
        "    her.train(epochs=5)"
      ],
      "metadata": {
        "id": "Y_RXCQIVPyVo",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a443258d-4a0c-4619-8864-f088140eebbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_dim= (9,) action_dim= 3 action_scaling: [1. 1. 1.]\n",
            "epoch 0: avg return=-49.08, success rate=0.02 (with 100 evaluation steps)\n",
            "epoch 1: avg return=-1.85, success rate=1.0 (with 100 evaluation steps)\n",
            "epoch 2: avg return=-1.82, success rate=1.0 (with 100 evaluation steps)\n",
            "epoch 3: avg return=-1.59, success rate=1.0 (with 100 evaluation steps)\n",
            "epoch 4: avg return=-1.64, success rate=1.0 (with 100 evaluation steps)\n",
            "epoch 5: avg return=-1.71, success rate=1.0 (with 100 evaluation steps)\n"
          ]
        }
      ]
    }
  ]
}