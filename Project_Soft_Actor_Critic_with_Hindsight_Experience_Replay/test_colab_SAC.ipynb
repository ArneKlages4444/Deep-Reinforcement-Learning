{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_colab_SAC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoOEob3gB00H",
        "outputId": "ade8ae0d-42c9-4e23-d801-9641dc837b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/gym/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[mujoco]\n",
            "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
            "\u001b[K     |████████████████████████████████| 734 kB 4.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (0.0.8)\n",
            "Collecting imageio>=2.14.1\n",
            "  Downloading imageio-2.21.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 37.1 MB/s \n",
            "\u001b[?25hCollecting mujoco==2.2.0\n",
            "  Downloading mujoco-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 22.0 MB/s \n",
            "\u001b[?25hCollecting glfw\n",
            "  Downloading glfw-2.5.4-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.2.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (3.1.6)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.8.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852319 sha256=d37688473f8543e1dcea9b99f32f6dd6eafa9835997314981d0a3050f317d3c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a8/81/4ba83fc99a5637e27f4e16da10f9e15ff61f77ce524d23a8d7\n",
            "Successfully built gym\n",
            "Installing collected packages: pillow, glfw, mujoco, imageio, gym\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed glfw-2.5.4 gym-0.25.2 imageio-2.21.2 mujoco-2.2.0 pillow-9.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install gym[mujoco]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "b9hF7N706aXK",
        "outputId": "fb08b548-d53b-4888-c8d7-e730997e2c4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, state_dims, action_dims, max_size=1000000, batch_size=256):\n",
        "        self._max_size = max_size\n",
        "        self._batch_size = batch_size\n",
        "        self._size = 0\n",
        "        self._current_position = 0\n",
        "        self._state_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._state_prime_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._action_memory = np.zeros((self._max_size, action_dims))\n",
        "        self._reward_memory = np.zeros((self._max_size, 1))\n",
        "        self._done_memory = np.zeros((self._max_size, 1), dtype=bool)\n",
        "\n",
        "    def size(self):\n",
        "        return self._size\n",
        "\n",
        "    def ready(self):\n",
        "        return self._size >= self._batch_size\n",
        "\n",
        "    def add_transition(self, state, action, reward, state_, done):\n",
        "        self._state_memory[self._current_position] = state\n",
        "        self._state_prime_memory[self._current_position] = state_\n",
        "        self._action_memory[self._current_position] = action\n",
        "        self._reward_memory[self._current_position] = reward\n",
        "        self._done_memory[self._current_position] = done\n",
        "        # self.un_norm_r[self.current_position] = r\n",
        "        # self.r = (self.un_norm_r - np.mean(self.un_norm_r)) / (np.std(self.un_norm_r) + 1e-10)\n",
        "        if self._size < self._max_size:\n",
        "            self._size += 1\n",
        "        self._current_position = (self._current_position + 1) % self._max_size\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch_indices = np.random.choice(self._size, self._batch_size, replace=False)\n",
        "        states = tf.convert_to_tensor(self._state_memory[batch_indices], dtype=tf.float32)\n",
        "        states_prime = tf.convert_to_tensor(self._state_prime_memory[batch_indices], dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self._action_memory[batch_indices], dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(self._reward_memory[batch_indices], dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(self._done_memory[batch_indices], dtype=tf.float32)\n",
        "        return states, actions, rewards, states_prime, dones\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Concatenate\n",
        "\n",
        "\n",
        "def create_policy_network(learning_rate, state_dim, action_dim):\n",
        "    inputs = keras.Input(shape=state_dim)\n",
        "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    mu = Dense(action_dim, activation=None)(x)\n",
        "    sigma = Dense(action_dim, activation=tf.nn.softplus)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=(mu, sigma))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_q_network(learning_rate, state_dim, action_dim):\n",
        "    inputs_s = keras.Input(shape=state_dim)\n",
        "    inputs_a = keras.Input(shape=action_dim)\n",
        "    x = Concatenate()([inputs_s, inputs_a])\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    out = Dense(1, activation=None)(x)\n",
        "    model = keras.Model(inputs=(inputs_s, inputs_a), outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "# from ExperienceReplayBuffer import ExperienceReplayBuffer\n",
        "import tensorflow as tf\n",
        "from tensorflow import math as tfm\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# input actions are always between (−1, 1)\n",
        "def default_scaling(actions):\n",
        "    return actions\n",
        "\n",
        "\n",
        "# input actions are always between (−1, 1)\n",
        "def multiplicative_scaling(actions, factors):\n",
        "    return actions * factors\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, environment, state_dim, action_dim,\n",
        "                 actor_network_generator, critic_network_generator, action_scaling=default_scaling,\n",
        "                 learning_rate=0.0003, gamma=0.99, tau=0.005, reward_scale=1, alpha=0.2,\n",
        "                 batch_size=256, max_replay_buffer_size=1000000):\n",
        "        self._environment = environment\n",
        "        self._action_dim = action_dim\n",
        "        self._action_scaling = action_scaling\n",
        "        self._gamma = gamma\n",
        "        self._tau = tau\n",
        "        self._reward_scale = reward_scale\n",
        "        self._alpha = alpha\n",
        "        self._batch_size = batch_size\n",
        "        self._mse = tf.keras.losses.MeanSquaredError()\n",
        "        self._reply_buffer = ExperienceReplayBuffer(state_dim, action_dim, max_replay_buffer_size, batch_size)\n",
        "        self._actor = actor_network_generator(learning_rate)\n",
        "        self._critic_1 = critic_network_generator(learning_rate)\n",
        "        self._critic_2 = critic_network_generator(learning_rate)\n",
        "        self._critic_1_t = critic_network_generator(learning_rate)\n",
        "        self._critic_2_t = critic_network_generator(learning_rate)\n",
        "        self._wight_init()\n",
        "\n",
        "    def reply_buffer(self):\n",
        "        return self._reply_buffer\n",
        "\n",
        "    def environment(self):\n",
        "        return self._environment\n",
        "\n",
        "    def _wight_init(self):\n",
        "        self._critic_1.set_weights(self._critic_1_t.weights)\n",
        "        self._critic_2.set_weights(self._critic_2_t.weights)\n",
        "\n",
        "    def update_target_weights(self):\n",
        "        self._weight_update(self._critic_1_t, self._critic_1)\n",
        "        self._weight_update(self._critic_2_t, self._critic_2)\n",
        "\n",
        "    def _weight_update(self, target_network, network):\n",
        "        new_wights = []\n",
        "        for w_t, w in zip(target_network.weights, network.weights):\n",
        "            new_wights.append((1 - self._tau) * w_t + self._tau * w)\n",
        "        target_network.set_weights(new_wights)\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, states_prime, dones = self._reply_buffer.sample_batch()\n",
        "        self.train_step_critic(states, actions, rewards, states_prime, dones)\n",
        "        self.train_step_actor(states)\n",
        "        self.update_target_weights()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_critic(self, states, actions, rewards, states_prime, dones):\n",
        "        actions_prime, log_probs = self.sample_actions_form_policy(states_prime)\n",
        "        q1 = self._critic_1_t((states_prime, actions_prime))\n",
        "        q2 = self._critic_2_t((states_prime, actions_prime))\n",
        "        q_r = tfm.minimum(q1, q2) - self._alpha * log_probs\n",
        "        targets = self._reward_scale * rewards + self._gamma * (1 - dones) * q_r\n",
        "        self._critic_update(self._critic_1, states, actions, targets)\n",
        "        self._critic_update(self._critic_2, states, actions, targets)\n",
        "\n",
        "    def _critic_update(self, critic, states, actions, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q = critic((states, actions))\n",
        "            loss = 0.5 * self._mse(targets, q)\n",
        "        gradients = tape.gradient(loss, critic.trainable_variables)\n",
        "        critic.optimizer.apply_gradients(zip(gradients, critic.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_actor(self, states):\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_new, log_probs = self.sample_actions_form_policy(states)\n",
        "            q1 = self._critic_1((states, actions_new))\n",
        "            q2 = self._critic_2((states, actions_new))\n",
        "            loss = tfm.reduce_mean(self._alpha * log_probs - tfm.minimum(q1, q2))\n",
        "            # equal to loss = -tfm.reduce_mean(tfm.minimum(q1, q2) - self._alpha * log_probs)\n",
        "        gradients = tape.gradient(loss, self._actor.trainable_variables)\n",
        "        self._actor.optimizer.apply_gradients(zip(gradients, self._actor.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def sample_actions_form_policy(self, state):\n",
        "        mu, sigma = self._actor(state)\n",
        "        # MultivariateNormalDiag(loc=mus, scale_diag=sigmas) other option\n",
        "        distribution = tfd.Normal(mu, sigma)\n",
        "        actions = distribution.sample()\n",
        "        log_probs = distribution.log_prob(actions)\n",
        "        actions = tfm.tanh(actions)\n",
        "        log_probs -= tfm.log(1 - tfm.pow(actions, 2) + 1e-6)  # + 1e-6 because log undefined for 0\n",
        "        log_probs = tfm.reduce_sum(log_probs, axis=-1, keepdims=True)\n",
        "        return actions, log_probs\n",
        "\n",
        "    def act_deterministic(self, state):\n",
        "        actions_prime, _ = self._actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def act_stochastic(self, state):\n",
        "        actions_prime, _ = self.sample_actions_form_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def _act(self, actions):\n",
        "        scaled_actions = self._action_scaling(actions)  # scaled actions from (-1, 1) according (to environment)\n",
        "        observation_prime, reward, done, _ = self._environment.step(scaled_actions[0])\n",
        "        return actions, observation_prime, reward, done\n",
        "\n",
        "    def train(self, epochs, environment_steps=1, training_steps=1, pre_sampling_steps=1024):\n",
        "        print(f\"Random exploration for {pre_sampling_steps} steps!\")\n",
        "        observation = self._environment.reset()\n",
        "        ret = 0\n",
        "        for _ in range(max(pre_sampling_steps, self._batch_size)):\n",
        "            actions = tf.random.uniform((self._action_dim,), minval=-1, maxval=1)\n",
        "            scaled_actions = self._action_scaling(actions)  # scaled actions from (-1, 1) according (to environment)\n",
        "            observation_prime, reward, done, _ = self._environment.step(scaled_actions)\n",
        "            ret += reward\n",
        "            self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "            if done:\n",
        "                ret = 0\n",
        "                observation = self._environment.reset()\n",
        "            else:\n",
        "                observation = observation_prime\n",
        "        print(\"start training!\")\n",
        "        returns = []\n",
        "        observation = self._environment.reset()\n",
        "        done = 0\n",
        "        ret = 0\n",
        "        epoch = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < environment_steps or self._reply_buffer.size() < self._batch_size:\n",
        "                if done:\n",
        "                    observation = self._environment.reset()\n",
        "                    returns.append(ret)\n",
        "                    print(\"epoch:\", epoch, \"steps:\", steps, \"return:\", ret, \"avg return:\", np.average(returns[-50:]))\n",
        "                    ret = 0\n",
        "                    epoch += 1\n",
        "                    if epoch >= epochs:\n",
        "                        print(\"training finished!\")\n",
        "                        return\n",
        "                actions, observation_prime, reward, done = self.act_stochastic(observation)\n",
        "                self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "                observation = observation_prime\n",
        "                steps += 1\n",
        "                ret += reward\n",
        "                i += 1\n",
        "            for _ in range(training_steps):\n",
        "                self.learn()\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "# from final_project.Networks.GenericMLPs1D import create_policy_network, create_q_network\n",
        "# from SoftActorCriticAgent import Agent, multiplicative_scaling\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.keras.backend.clear_session()\n",
        "    env = gym.make('InvertedPendulum-v4')\n",
        "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space.shape[0], \"action_scaling:\",\n",
        "          env.action_space.high)\n",
        "\n",
        "    agent = Agent(environment=env, state_dim=env.observation_space.shape, action_dim=env.action_space.shape[0],\n",
        "                  action_scaling=partial(multiplicative_scaling, factors=env.action_space.high),\n",
        "                  actor_network_generator=partial(create_policy_network, state_dim=env.observation_space.shape[0],\n",
        "                                                  action_dim=env.action_space.shape[0]),\n",
        "                  critic_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0],\n",
        "                                                   action_dim=env.action_space.shape[0]))\n",
        "    agent.train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3kl_SYnB-E_",
        "outputId": "3b1b399c-d28e-4999-9e25-36457104d853"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_dim= (4,) action_dim= 1 action_scaling: [3.]\n",
            "Random exploration for 1024 steps!\n",
            "start training!\n",
            "epoch: 0 steps: 3 return: 3.0 avg return: 3.0\n",
            "epoch: 1 steps: 8 return: 5.0 avg return: 4.0\n",
            "epoch: 2 steps: 12 return: 4.0 avg return: 4.0\n",
            "epoch: 3 steps: 17 return: 5.0 avg return: 4.25\n",
            "epoch: 4 steps: 23 return: 6.0 avg return: 4.6\n",
            "epoch: 5 steps: 26 return: 3.0 avg return: 4.333333333333333\n",
            "epoch: 6 steps: 32 return: 6.0 avg return: 4.571428571428571\n",
            "epoch: 7 steps: 37 return: 5.0 avg return: 4.625\n",
            "epoch: 8 steps: 47 return: 10.0 avg return: 5.222222222222222\n",
            "epoch: 9 steps: 53 return: 6.0 avg return: 5.3\n",
            "epoch: 10 steps: 57 return: 4.0 avg return: 5.181818181818182\n",
            "epoch: 11 steps: 61 return: 4.0 avg return: 5.083333333333333\n",
            "epoch: 12 steps: 65 return: 4.0 avg return: 5.0\n",
            "epoch: 13 steps: 68 return: 3.0 avg return: 4.857142857142857\n",
            "epoch: 14 steps: 71 return: 3.0 avg return: 4.733333333333333\n",
            "epoch: 15 steps: 74 return: 3.0 avg return: 4.625\n",
            "epoch: 16 steps: 77 return: 3.0 avg return: 4.529411764705882\n",
            "epoch: 17 steps: 83 return: 6.0 avg return: 4.611111111111111\n",
            "epoch: 18 steps: 86 return: 3.0 avg return: 4.526315789473684\n",
            "epoch: 19 steps: 89 return: 3.0 avg return: 4.45\n",
            "epoch: 20 steps: 94 return: 5.0 avg return: 4.476190476190476\n",
            "epoch: 21 steps: 98 return: 4.0 avg return: 4.454545454545454\n",
            "epoch: 22 steps: 101 return: 3.0 avg return: 4.391304347826087\n",
            "epoch: 23 steps: 104 return: 3.0 avg return: 4.333333333333333\n",
            "epoch: 24 steps: 107 return: 3.0 avg return: 4.28\n",
            "epoch: 25 steps: 112 return: 5.0 avg return: 4.3076923076923075\n",
            "epoch: 26 steps: 120 return: 8.0 avg return: 4.444444444444445\n",
            "epoch: 27 steps: 125 return: 5.0 avg return: 4.464285714285714\n",
            "epoch: 28 steps: 133 return: 8.0 avg return: 4.586206896551724\n",
            "epoch: 29 steps: 136 return: 3.0 avg return: 4.533333333333333\n",
            "epoch: 30 steps: 141 return: 5.0 avg return: 4.548387096774194\n",
            "epoch: 31 steps: 144 return: 3.0 avg return: 4.5\n",
            "epoch: 32 steps: 147 return: 3.0 avg return: 4.454545454545454\n",
            "epoch: 33 steps: 151 return: 4.0 avg return: 4.4411764705882355\n",
            "epoch: 34 steps: 155 return: 4.0 avg return: 4.428571428571429\n",
            "epoch: 35 steps: 176 return: 21.0 avg return: 4.888888888888889\n",
            "epoch: 36 steps: 179 return: 3.0 avg return: 4.837837837837838\n",
            "epoch: 37 steps: 187 return: 8.0 avg return: 4.921052631578948\n",
            "epoch: 38 steps: 194 return: 7.0 avg return: 4.9743589743589745\n",
            "epoch: 39 steps: 204 return: 10.0 avg return: 5.1\n",
            "epoch: 40 steps: 208 return: 4.0 avg return: 5.073170731707317\n",
            "epoch: 41 steps: 212 return: 4.0 avg return: 5.0476190476190474\n",
            "epoch: 42 steps: 215 return: 3.0 avg return: 5.0\n",
            "epoch: 43 steps: 225 return: 10.0 avg return: 5.113636363636363\n",
            "epoch: 44 steps: 228 return: 3.0 avg return: 5.066666666666666\n",
            "epoch: 45 steps: 236 return: 8.0 avg return: 5.130434782608695\n",
            "epoch: 46 steps: 240 return: 4.0 avg return: 5.1063829787234045\n",
            "epoch: 47 steps: 246 return: 6.0 avg return: 5.125\n",
            "epoch: 48 steps: 251 return: 5.0 avg return: 5.122448979591836\n",
            "epoch: 49 steps: 268 return: 17.0 avg return: 5.36\n",
            "epoch: 50 steps: 293 return: 25.0 avg return: 5.8\n",
            "epoch: 51 steps: 309 return: 16.0 avg return: 6.02\n",
            "epoch: 52 steps: 318 return: 9.0 avg return: 6.12\n",
            "epoch: 53 steps: 322 return: 4.0 avg return: 6.1\n",
            "epoch: 54 steps: 336 return: 14.0 avg return: 6.26\n",
            "epoch: 55 steps: 379 return: 43.0 avg return: 7.06\n",
            "epoch: 56 steps: 432 return: 53.0 avg return: 8.0\n",
            "epoch: 57 steps: 442 return: 10.0 avg return: 8.1\n",
            "epoch: 58 steps: 475 return: 33.0 avg return: 8.56\n",
            "epoch: 59 steps: 533 return: 58.0 avg return: 9.6\n",
            "epoch: 60 steps: 605 return: 72.0 avg return: 10.96\n",
            "epoch: 61 steps: 652 return: 47.0 avg return: 11.82\n",
            "epoch: 62 steps: 827 return: 175.0 avg return: 15.24\n",
            "epoch: 63 steps: 884 return: 57.0 avg return: 16.32\n",
            "epoch: 64 steps: 1035 return: 151.0 avg return: 19.28\n",
            "epoch: 65 steps: 1103 return: 68.0 avg return: 20.58\n",
            "epoch: 66 steps: 1173 return: 70.0 avg return: 21.92\n",
            "epoch: 67 steps: 1252 return: 79.0 avg return: 23.38\n",
            "epoch: 68 steps: 1313 return: 61.0 avg return: 24.54\n",
            "epoch: 69 steps: 1390 return: 77.0 avg return: 26.02\n",
            "epoch: 70 steps: 1443 return: 53.0 avg return: 26.98\n",
            "epoch: 71 steps: 1538 return: 95.0 avg return: 28.8\n",
            "epoch: 72 steps: 1615 return: 77.0 avg return: 30.28\n",
            "epoch: 73 steps: 1691 return: 76.0 avg return: 31.74\n",
            "epoch: 74 steps: 1749 return: 58.0 avg return: 32.84\n",
            "epoch: 75 steps: 1828 return: 79.0 avg return: 34.32\n",
            "epoch: 76 steps: 1885 return: 57.0 avg return: 35.3\n",
            "epoch: 77 steps: 1956 return: 71.0 avg return: 36.62\n",
            "epoch: 78 steps: 2014 return: 58.0 avg return: 37.62\n",
            "epoch: 79 steps: 2072 return: 58.0 avg return: 38.72\n",
            "epoch: 80 steps: 2130 return: 58.0 avg return: 39.78\n",
            "epoch: 81 steps: 2192 return: 62.0 avg return: 40.96\n",
            "epoch: 82 steps: 2267 return: 75.0 avg return: 42.4\n",
            "epoch: 83 steps: 2364 return: 97.0 avg return: 44.26\n",
            "epoch: 84 steps: 2413 return: 49.0 avg return: 45.16\n",
            "epoch: 85 steps: 2460 return: 47.0 avg return: 45.68\n",
            "epoch: 86 steps: 2546 return: 86.0 avg return: 47.34\n",
            "epoch: 87 steps: 2616 return: 70.0 avg return: 48.58\n",
            "epoch: 88 steps: 2686 return: 70.0 avg return: 49.84\n",
            "epoch: 89 steps: 2748 return: 62.0 avg return: 50.88\n",
            "epoch: 90 steps: 2804 return: 56.0 avg return: 51.92\n",
            "epoch: 91 steps: 2879 return: 75.0 avg return: 53.34\n",
            "epoch: 92 steps: 2944 return: 65.0 avg return: 54.58\n",
            "epoch: 93 steps: 3005 return: 61.0 avg return: 55.6\n",
            "epoch: 94 steps: 3073 return: 68.0 avg return: 56.9\n",
            "epoch: 95 steps: 3120 return: 47.0 avg return: 57.68\n",
            "epoch: 96 steps: 3227 return: 107.0 avg return: 59.74\n",
            "epoch: 97 steps: 3296 return: 69.0 avg return: 61.0\n",
            "epoch: 98 steps: 3346 return: 50.0 avg return: 61.9\n",
            "epoch: 99 steps: 3478 return: 132.0 avg return: 64.2\n",
            "training finished!\n"
          ]
        }
      ]
    }
  ]
}