{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoOEob3gB00H",
        "outputId": "1c70d9ce-9c1a-43eb-a9d1-7397f7fd20c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/gym/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[mujoco]\n",
            "  Downloading gym-0.26.1.tar.gz (719 kB)\n",
            "\u001b[K     |████████████████████████████████| 719 kB 7.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.5.0)\n",
            "Collecting imageio>=2.14.1\n",
            "  Downloading imageio-2.22.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 49.9 MB/s \n",
            "\u001b[?25hCollecting mujoco==2.2.0\n",
            "  Downloading mujoco-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 23.8 MB/s \n",
            "\u001b[?25hCollecting glfw\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 53.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.2.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (3.1.6)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.1-py3-none-any.whl size=826209 sha256=87b2c1db5f848414adb669791779047ccb1b94e912f68ca9cec2c080607372b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/f0/10/6f06af57d047770ee4b45f9408dbb90bb55916892e8e9fbc86\n",
            "Successfully built gym\n",
            "Installing collected packages: pillow, glfw, mujoco, imageio, gym\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed glfw-2.5.5 gym-0.26.1 imageio-2.22.0 mujoco-2.2.0 pillow-9.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install gym[mujoco]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "b9hF7N706aXK",
        "outputId": "0ccd9eff-301a-4073-a503-7a579030ba1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, state_dims, action_dims, max_size=1000000, batch_size=256):\n",
        "        self._max_size = max_size\n",
        "        self._batch_size = batch_size\n",
        "        self._size = 0\n",
        "        self._current_position = 0\n",
        "        self._state_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._state_prime_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._action_memory = np.zeros((self._max_size, action_dims))\n",
        "        self._reward_memory = np.zeros((self._max_size, 1))\n",
        "        self._done_memory = np.zeros((self._max_size, 1), dtype=np.bool)\n",
        "\n",
        "    def size(self):\n",
        "        return self._size\n",
        "\n",
        "    def add_transition(self, state, action, reward, state_, done):\n",
        "        self._state_memory[self._current_position] = state\n",
        "        self._state_prime_memory[self._current_position] = state_\n",
        "        self._action_memory[self._current_position] = action\n",
        "        self._reward_memory[self._current_position] = reward\n",
        "        self._done_memory[self._current_position] = done\n",
        "        if self._size < self._max_size:\n",
        "            self._size += 1\n",
        "        self._current_position = (self._current_position + 1) % self._max_size\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch_indices = np.random.choice(self._size, self._batch_size, replace=False)\n",
        "        states = tf.convert_to_tensor(self._state_memory[batch_indices], dtype=tf.float32)\n",
        "        states_prime = tf.convert_to_tensor(self._state_prime_memory[batch_indices], dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self._action_memory[batch_indices], dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(self._reward_memory[batch_indices], dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(self._done_memory[batch_indices], dtype=tf.float32)\n",
        "        return states, actions, rewards, states_prime, dones\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Concatenate\n",
        "\n",
        "\n",
        "def create_policy_network(learning_rate, state_dim, action_dim):\n",
        "    inputs = keras.Input(shape=state_dim)\n",
        "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    out = Dense(action_dim, activation=tf.nn.tanh)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_q_network(learning_rate, state_dim, action_dim):\n",
        "    inputs_s = keras.Input(shape=state_dim)\n",
        "    inputs_a = keras.Input(shape=action_dim)\n",
        "    x = Concatenate()([inputs_s, inputs_a])\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    out = Dense(1, activation=None)(x)\n",
        "    model = keras.Model(inputs=(inputs_s, inputs_a), outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "# from ExperienceReplayBuffer import ExperienceReplayBuffer\n",
        "import tensorflow as tf\n",
        "from tensorflow import math as tfm\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# input should be between (−1, 1)\n",
        "def default_scaling(actions):\n",
        "    return actions\n",
        "\n",
        "\n",
        "# input should be between (−1, 1)\n",
        "def multiplicative_scaling(actions, factors):\n",
        "    return actions * factors\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, environment, state_dim, action_dim,\n",
        "                 actor_network_generator, critic_network_generator, action_scaling=default_scaling,\n",
        "                 learning_rate=0.0003, gamma=0.99, tau=0.005,\n",
        "                 exploration_noise=0.1, a_low=-1, a_high=1,\n",
        "                 batch_size=256, max_replay_buffer_size=1000000):\n",
        "        self._environment = environment\n",
        "        self._action_dim = action_dim\n",
        "        self._action_scaling = action_scaling\n",
        "        self._gamma = gamma\n",
        "        self._tau = tau\n",
        "        self.exploration_noise = exploration_noise\n",
        "        self.a_low = a_low\n",
        "        self.a_high = a_high\n",
        "        self._batch_size = batch_size\n",
        "        self._mse = tf.keras.losses.MeanSquaredError()\n",
        "        self._reply_buffer = ExperienceReplayBuffer(state_dim, action_dim, max_replay_buffer_size, batch_size)\n",
        "        self._actor = actor_network_generator(learning_rate)\n",
        "        self._actor_t = actor_network_generator(learning_rate)\n",
        "        self._critic = critic_network_generator(learning_rate)\n",
        "        self._critic_t = critic_network_generator(learning_rate)\n",
        "        self._wight_init()\n",
        "\n",
        "    def reply_buffer(self):\n",
        "        return self._reply_buffer\n",
        "\n",
        "    def environment(self):\n",
        "        return self._environment\n",
        "\n",
        "    def _wight_init(self):\n",
        "        self._actor.set_weights(self._actor_t.weights)\n",
        "        self._critic.set_weights(self._critic_t.weights)\n",
        "\n",
        "    def update_target_weights(self):\n",
        "        self._weight_update(self._actor_t, self._actor)\n",
        "        self._weight_update(self._critic_t, self._critic)\n",
        "\n",
        "    def _weight_update(self, target_network, network):\n",
        "        new_wights = []\n",
        "        for w_t, w in zip(target_network.weights, network.weights):\n",
        "            new_wights.append((1 - self._tau) * w_t + self._tau * w)\n",
        "        target_network.set_weights(new_wights)\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, states_prime, dones = self._reply_buffer.sample_batch()\n",
        "        self.train_step_critic(states, actions, rewards, states_prime, dones)\n",
        "        self.train_step_actor(states)\n",
        "        self.update_target_weights()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_critic(self, states, actions, rewards, states_prime, dones):\n",
        "        actions_prime = self._actor_t(states)\n",
        "        q = self._critic_t((states_prime, actions_prime))\n",
        "        targets = rewards + self._gamma * (1 - dones) * q\n",
        "        with tf.GradientTape() as tape:\n",
        "            q = self._critic((states, actions))\n",
        "            loss = self._mse(targets, q)\n",
        "        gradients = tape.gradient(loss, self._critic.trainable_variables)\n",
        "        self._critic.optimizer.apply_gradients(zip(gradients, self._critic.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_actor(self, states):\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_new = self._actor(states)\n",
        "            q = self._critic((states, actions_new))\n",
        "            loss = -tfm.reduce_mean(q)\n",
        "        gradients = tape.gradient(loss, self._actor.trainable_variables)\n",
        "        self._actor.optimizer.apply_gradients(zip(gradients, self._actor.trainable_variables))\n",
        "\n",
        "    def _action_clipping(self, actions):\n",
        "        return tf.clip_by_value(actions, self.a_low, self.a_high)\n",
        "\n",
        "    def sample_actions_form_policy(self, state):\n",
        "        actions = self._actor(state)\n",
        "        # or noise from sampling form tfp normal distribution with a sigma vector to get different noise per action\n",
        "        noise = tf.random.normal(actions.get_shape(), 0, self.exploration_noise)\n",
        "        clip_actions = self._action_clipping(actions + noise)\n",
        "        return clip_actions\n",
        "\n",
        "    def act_deterministic(self, state):\n",
        "        actions_prime = self._actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def act_stochastic(self, state):\n",
        "        actions_prime = self.sample_actions_form_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def _act(self, actions):\n",
        "        scaled_actions = self._action_scaling(actions)  # scaled actions from (-1, 1) according (to environment)\n",
        "        observation_prime, reward, done, _, _ = self._environment.step(scaled_actions[0])\n",
        "        return actions, observation_prime, reward, done\n",
        "\n",
        "    def train(self, epochs, environment_steps=1, training_steps=1, pre_sampling_steps=0):\n",
        "        print(f\"Random exploration for {pre_sampling_steps} steps!\")\n",
        "        observation, _ = self._environment.reset()\n",
        "        ret = 0\n",
        "        for _ in range(max(pre_sampling_steps, self._batch_size)):\n",
        "            actions = tf.random.uniform((self._action_dim,), minval=self.a_low, maxval=self.a_high)\n",
        "            actions = self._action_scaling(actions)\n",
        "            observation_prime, reward, done, _, _ = self._environment.step(actions)\n",
        "            ret += reward\n",
        "            self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "            if done:\n",
        "                print(\"print\", ret)\n",
        "                ret = 0\n",
        "                observation, _ = self._environment.reset()\n",
        "            else:\n",
        "                observation = observation_prime\n",
        "        print(\"print\", ret)\n",
        "\n",
        "        print(\"start training!\")\n",
        "        returns = []\n",
        "        observation, _ = self._environment.reset()\n",
        "        done = 0\n",
        "        ret = 0\n",
        "        epoch = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < environment_steps or self._reply_buffer.size() < self._batch_size:\n",
        "                if done:\n",
        "                    observation, _ = self._environment.reset()\n",
        "                    returns.append(ret)\n",
        "                    print(\"epoch:\", epoch, \"steps:\", steps, \"return:\", ret, \"avg return:\", np.average(returns[-50:]))\n",
        "                    ret = 0\n",
        "                    epoch += 1\n",
        "                    if epoch >= epochs:\n",
        "                        print(\"training finished!\")\n",
        "                        return\n",
        "                actions, observation_prime, reward, done = self.act_stochastic(observation)\n",
        "                self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "                observation = observation_prime\n",
        "                steps += 1\n",
        "                ret += reward\n",
        "                i += 1\n",
        "            for _ in range(training_steps):\n",
        "                self.learn()\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "# from GenericMLPs1D import create_policy_network, create_q_network\n",
        "# from DDPGAgent import Agent, multiplicative_scaling\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.keras.backend.clear_session()\n",
        "    env = gym.make('InvertedPendulum-v4')\n",
        "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space.shape[0], \"action_scaling:\",\n",
        "          env.action_space.high)\n",
        "\n",
        "    agent = Agent(environment=env, state_dim=env.observation_space.shape, action_dim=env.action_space.shape[0],\n",
        "                  action_scaling=partial(multiplicative_scaling, factors=env.action_space.high),\n",
        "                  actor_network_generator=partial(create_policy_network, state_dim=env.observation_space.shape[0],\n",
        "                                                  action_dim=env.action_space.shape[0]),\n",
        "                  critic_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0],\n",
        "                                                   action_dim=env.action_space.shape[0]))\n",
        "    agent.train(10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u3kl_SYnB-E_",
        "outputId": "68895c47-1592-4f36-9e1f-26a4a27e7d3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_dim= (4,) action_dim= 1 action_scaling: [3.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random exploration for 0 steps!\n",
            "print 3.0\n",
            "print 4.0\n",
            "print 4.0\n",
            "print 16.0\n",
            "print 3.0\n",
            "print 6.0\n",
            "print 5.0\n",
            "print 3.0\n",
            "print 3.0\n",
            "print 15.0\n",
            "print 3.0\n",
            "print 3.0\n",
            "print 5.0\n",
            "print 5.0\n",
            "print 7.0\n",
            "print 7.0\n",
            "print 8.0\n",
            "print 4.0\n",
            "print 5.0\n",
            "print 3.0\n",
            "print 3.0\n",
            "print 6.0\n",
            "print 9.0\n",
            "print 4.0\n",
            "print 3.0\n",
            "print 9.0\n",
            "print 4.0\n",
            "print 3.0\n",
            "print 3.0\n",
            "print 10.0\n",
            "print 5.0\n",
            "print 4.0\n",
            "print 3.0\n",
            "print 7.0\n",
            "print 4.0\n",
            "print 6.0\n",
            "print 8.0\n",
            "print 16.0\n",
            "print 7.0\n",
            "print 3.0\n",
            "print 5.0\n",
            "print 3.0\n",
            "print 4.0\n",
            "print 4.0\n",
            "print 4.0\n",
            "print 7.0\n",
            "start training!\n",
            "epoch: 0 steps: 8 return: 8.0 avg return: 8.0\n",
            "epoch: 1 steps: 14 return: 6.0 avg return: 7.0\n",
            "epoch: 2 steps: 19 return: 5.0 avg return: 6.333333333333333\n",
            "epoch: 3 steps: 23 return: 4.0 avg return: 5.75\n",
            "epoch: 4 steps: 27 return: 4.0 avg return: 5.4\n",
            "epoch: 5 steps: 30 return: 3.0 avg return: 5.0\n",
            "epoch: 6 steps: 33 return: 3.0 avg return: 4.714285714285714\n",
            "epoch: 7 steps: 36 return: 3.0 avg return: 4.5\n",
            "epoch: 8 steps: 39 return: 3.0 avg return: 4.333333333333333\n",
            "epoch: 9 steps: 42 return: 3.0 avg return: 4.2\n",
            "epoch: 10 steps: 45 return: 3.0 avg return: 4.090909090909091\n",
            "epoch: 11 steps: 48 return: 3.0 avg return: 4.0\n",
            "epoch: 12 steps: 51 return: 3.0 avg return: 3.923076923076923\n",
            "epoch: 13 steps: 54 return: 3.0 avg return: 3.857142857142857\n",
            "epoch: 14 steps: 57 return: 3.0 avg return: 3.8\n",
            "epoch: 15 steps: 60 return: 3.0 avg return: 3.75\n",
            "epoch: 16 steps: 63 return: 3.0 avg return: 3.7058823529411766\n",
            "epoch: 17 steps: 66 return: 3.0 avg return: 3.6666666666666665\n",
            "epoch: 18 steps: 69 return: 3.0 avg return: 3.6315789473684212\n",
            "epoch: 19 steps: 72 return: 3.0 avg return: 3.6\n",
            "epoch: 20 steps: 75 return: 3.0 avg return: 3.5714285714285716\n",
            "epoch: 21 steps: 78 return: 3.0 avg return: 3.5454545454545454\n",
            "epoch: 22 steps: 81 return: 3.0 avg return: 3.5217391304347827\n",
            "epoch: 23 steps: 84 return: 3.0 avg return: 3.5\n",
            "epoch: 24 steps: 87 return: 3.0 avg return: 3.48\n",
            "epoch: 25 steps: 90 return: 3.0 avg return: 3.4615384615384617\n",
            "epoch: 26 steps: 93 return: 3.0 avg return: 3.4444444444444446\n",
            "epoch: 27 steps: 96 return: 3.0 avg return: 3.4285714285714284\n",
            "epoch: 28 steps: 99 return: 3.0 avg return: 3.413793103448276\n",
            "epoch: 29 steps: 102 return: 3.0 avg return: 3.4\n",
            "epoch: 30 steps: 105 return: 3.0 avg return: 3.3870967741935485\n",
            "epoch: 31 steps: 108 return: 3.0 avg return: 3.375\n",
            "epoch: 32 steps: 111 return: 3.0 avg return: 3.3636363636363638\n",
            "epoch: 33 steps: 114 return: 3.0 avg return: 3.3529411764705883\n",
            "epoch: 34 steps: 117 return: 3.0 avg return: 3.342857142857143\n",
            "epoch: 35 steps: 120 return: 3.0 avg return: 3.3333333333333335\n",
            "epoch: 36 steps: 123 return: 3.0 avg return: 3.324324324324324\n",
            "epoch: 37 steps: 126 return: 3.0 avg return: 3.3157894736842106\n",
            "epoch: 38 steps: 129 return: 3.0 avg return: 3.3076923076923075\n",
            "epoch: 39 steps: 132 return: 3.0 avg return: 3.3\n",
            "epoch: 40 steps: 135 return: 3.0 avg return: 3.292682926829268\n",
            "epoch: 41 steps: 138 return: 3.0 avg return: 3.2857142857142856\n",
            "epoch: 42 steps: 141 return: 3.0 avg return: 3.2790697674418605\n",
            "epoch: 43 steps: 144 return: 3.0 avg return: 3.272727272727273\n",
            "epoch: 44 steps: 147 return: 3.0 avg return: 3.2666666666666666\n",
            "epoch: 45 steps: 150 return: 3.0 avg return: 3.260869565217391\n",
            "epoch: 46 steps: 153 return: 3.0 avg return: 3.25531914893617\n",
            "epoch: 47 steps: 156 return: 3.0 avg return: 3.25\n",
            "epoch: 48 steps: 159 return: 3.0 avg return: 3.2448979591836733\n",
            "epoch: 49 steps: 162 return: 3.0 avg return: 3.24\n",
            "epoch: 50 steps: 165 return: 3.0 avg return: 3.14\n",
            "epoch: 51 steps: 168 return: 3.0 avg return: 3.08\n",
            "epoch: 52 steps: 171 return: 3.0 avg return: 3.04\n",
            "epoch: 53 steps: 174 return: 3.0 avg return: 3.02\n",
            "epoch: 54 steps: 177 return: 3.0 avg return: 3.0\n",
            "epoch: 55 steps: 180 return: 3.0 avg return: 3.0\n",
            "epoch: 56 steps: 183 return: 3.0 avg return: 3.0\n",
            "epoch: 57 steps: 186 return: 3.0 avg return: 3.0\n",
            "epoch: 58 steps: 189 return: 3.0 avg return: 3.0\n",
            "epoch: 59 steps: 192 return: 3.0 avg return: 3.0\n",
            "epoch: 60 steps: 195 return: 3.0 avg return: 3.0\n",
            "epoch: 61 steps: 198 return: 3.0 avg return: 3.0\n",
            "epoch: 62 steps: 201 return: 3.0 avg return: 3.0\n",
            "epoch: 63 steps: 204 return: 3.0 avg return: 3.0\n",
            "epoch: 64 steps: 207 return: 3.0 avg return: 3.0\n",
            "epoch: 65 steps: 210 return: 3.0 avg return: 3.0\n",
            "epoch: 66 steps: 213 return: 3.0 avg return: 3.0\n",
            "epoch: 67 steps: 216 return: 3.0 avg return: 3.0\n",
            "epoch: 68 steps: 219 return: 3.0 avg return: 3.0\n",
            "epoch: 69 steps: 222 return: 3.0 avg return: 3.0\n",
            "epoch: 70 steps: 225 return: 3.0 avg return: 3.0\n",
            "epoch: 71 steps: 228 return: 3.0 avg return: 3.0\n",
            "epoch: 72 steps: 231 return: 3.0 avg return: 3.0\n",
            "epoch: 73 steps: 234 return: 3.0 avg return: 3.0\n",
            "epoch: 74 steps: 237 return: 3.0 avg return: 3.0\n",
            "epoch: 75 steps: 240 return: 3.0 avg return: 3.0\n",
            "epoch: 76 steps: 243 return: 3.0 avg return: 3.0\n",
            "epoch: 77 steps: 246 return: 3.0 avg return: 3.0\n",
            "epoch: 78 steps: 249 return: 3.0 avg return: 3.0\n",
            "epoch: 79 steps: 252 return: 3.0 avg return: 3.0\n",
            "epoch: 80 steps: 255 return: 3.0 avg return: 3.0\n",
            "epoch: 81 steps: 258 return: 3.0 avg return: 3.0\n",
            "epoch: 82 steps: 261 return: 3.0 avg return: 3.0\n",
            "epoch: 83 steps: 264 return: 3.0 avg return: 3.0\n",
            "epoch: 84 steps: 267 return: 3.0 avg return: 3.0\n",
            "epoch: 85 steps: 270 return: 3.0 avg return: 3.0\n",
            "epoch: 86 steps: 273 return: 3.0 avg return: 3.0\n",
            "epoch: 87 steps: 276 return: 3.0 avg return: 3.0\n",
            "epoch: 88 steps: 279 return: 3.0 avg return: 3.0\n",
            "epoch: 89 steps: 282 return: 3.0 avg return: 3.0\n",
            "epoch: 90 steps: 285 return: 3.0 avg return: 3.0\n",
            "epoch: 91 steps: 288 return: 3.0 avg return: 3.0\n",
            "epoch: 92 steps: 291 return: 3.0 avg return: 3.0\n",
            "epoch: 93 steps: 294 return: 3.0 avg return: 3.0\n",
            "epoch: 94 steps: 297 return: 3.0 avg return: 3.0\n",
            "epoch: 95 steps: 300 return: 3.0 avg return: 3.0\n",
            "epoch: 96 steps: 303 return: 3.0 avg return: 3.0\n",
            "epoch: 97 steps: 306 return: 3.0 avg return: 3.0\n",
            "epoch: 98 steps: 309 return: 3.0 avg return: 3.0\n",
            "epoch: 99 steps: 312 return: 3.0 avg return: 3.0\n",
            "epoch: 100 steps: 315 return: 3.0 avg return: 3.0\n",
            "epoch: 101 steps: 318 return: 3.0 avg return: 3.0\n",
            "epoch: 102 steps: 321 return: 3.0 avg return: 3.0\n",
            "epoch: 103 steps: 324 return: 3.0 avg return: 3.0\n",
            "epoch: 104 steps: 327 return: 3.0 avg return: 3.0\n",
            "epoch: 105 steps: 330 return: 3.0 avg return: 3.0\n",
            "epoch: 106 steps: 333 return: 3.0 avg return: 3.0\n",
            "epoch: 107 steps: 336 return: 3.0 avg return: 3.0\n",
            "epoch: 108 steps: 339 return: 3.0 avg return: 3.0\n",
            "epoch: 109 steps: 342 return: 3.0 avg return: 3.0\n",
            "epoch: 110 steps: 345 return: 3.0 avg return: 3.0\n",
            "epoch: 111 steps: 348 return: 3.0 avg return: 3.0\n",
            "epoch: 112 steps: 351 return: 3.0 avg return: 3.0\n",
            "epoch: 113 steps: 354 return: 3.0 avg return: 3.0\n",
            "epoch: 114 steps: 357 return: 3.0 avg return: 3.0\n",
            "epoch: 115 steps: 360 return: 3.0 avg return: 3.0\n",
            "epoch: 116 steps: 363 return: 3.0 avg return: 3.0\n",
            "epoch: 117 steps: 366 return: 3.0 avg return: 3.0\n",
            "epoch: 118 steps: 369 return: 3.0 avg return: 3.0\n",
            "epoch: 119 steps: 372 return: 3.0 avg return: 3.0\n",
            "epoch: 120 steps: 375 return: 3.0 avg return: 3.0\n",
            "epoch: 121 steps: 378 return: 3.0 avg return: 3.0\n",
            "epoch: 122 steps: 381 return: 3.0 avg return: 3.0\n",
            "epoch: 123 steps: 384 return: 3.0 avg return: 3.0\n",
            "epoch: 124 steps: 387 return: 3.0 avg return: 3.0\n",
            "epoch: 125 steps: 390 return: 3.0 avg return: 3.0\n",
            "epoch: 126 steps: 393 return: 3.0 avg return: 3.0\n",
            "epoch: 127 steps: 396 return: 3.0 avg return: 3.0\n",
            "epoch: 128 steps: 399 return: 3.0 avg return: 3.0\n",
            "epoch: 129 steps: 402 return: 3.0 avg return: 3.0\n",
            "epoch: 130 steps: 405 return: 3.0 avg return: 3.0\n",
            "epoch: 131 steps: 408 return: 3.0 avg return: 3.0\n",
            "epoch: 132 steps: 411 return: 3.0 avg return: 3.0\n",
            "epoch: 133 steps: 414 return: 3.0 avg return: 3.0\n",
            "epoch: 134 steps: 417 return: 3.0 avg return: 3.0\n",
            "epoch: 135 steps: 420 return: 3.0 avg return: 3.0\n",
            "epoch: 136 steps: 423 return: 3.0 avg return: 3.0\n",
            "epoch: 137 steps: 426 return: 3.0 avg return: 3.0\n",
            "epoch: 138 steps: 429 return: 3.0 avg return: 3.0\n",
            "epoch: 139 steps: 432 return: 3.0 avg return: 3.0\n",
            "epoch: 140 steps: 435 return: 3.0 avg return: 3.0\n",
            "epoch: 141 steps: 438 return: 3.0 avg return: 3.0\n",
            "epoch: 142 steps: 441 return: 3.0 avg return: 3.0\n",
            "epoch: 143 steps: 444 return: 3.0 avg return: 3.0\n",
            "epoch: 144 steps: 447 return: 3.0 avg return: 3.0\n",
            "epoch: 145 steps: 450 return: 3.0 avg return: 3.0\n",
            "epoch: 146 steps: 453 return: 3.0 avg return: 3.0\n",
            "epoch: 147 steps: 456 return: 3.0 avg return: 3.0\n",
            "epoch: 148 steps: 459 return: 3.0 avg return: 3.0\n",
            "epoch: 149 steps: 462 return: 3.0 avg return: 3.0\n",
            "epoch: 150 steps: 465 return: 3.0 avg return: 3.0\n",
            "epoch: 151 steps: 468 return: 3.0 avg return: 3.0\n",
            "epoch: 152 steps: 471 return: 3.0 avg return: 3.0\n",
            "epoch: 153 steps: 474 return: 3.0 avg return: 3.0\n",
            "epoch: 154 steps: 477 return: 3.0 avg return: 3.0\n",
            "epoch: 155 steps: 480 return: 3.0 avg return: 3.0\n",
            "epoch: 156 steps: 483 return: 3.0 avg return: 3.0\n",
            "epoch: 157 steps: 486 return: 3.0 avg return: 3.0\n",
            "epoch: 158 steps: 489 return: 3.0 avg return: 3.0\n",
            "epoch: 159 steps: 492 return: 3.0 avg return: 3.0\n",
            "epoch: 160 steps: 495 return: 3.0 avg return: 3.0\n",
            "epoch: 161 steps: 498 return: 3.0 avg return: 3.0\n",
            "epoch: 162 steps: 501 return: 3.0 avg return: 3.0\n",
            "epoch: 163 steps: 504 return: 3.0 avg return: 3.0\n",
            "epoch: 164 steps: 507 return: 3.0 avg return: 3.0\n",
            "epoch: 165 steps: 510 return: 3.0 avg return: 3.0\n",
            "epoch: 166 steps: 513 return: 3.0 avg return: 3.0\n",
            "epoch: 167 steps: 516 return: 3.0 avg return: 3.0\n",
            "epoch: 168 steps: 519 return: 3.0 avg return: 3.0\n",
            "epoch: 169 steps: 522 return: 3.0 avg return: 3.0\n",
            "epoch: 170 steps: 525 return: 3.0 avg return: 3.0\n",
            "epoch: 171 steps: 528 return: 3.0 avg return: 3.0\n",
            "epoch: 172 steps: 531 return: 3.0 avg return: 3.0\n",
            "epoch: 173 steps: 534 return: 3.0 avg return: 3.0\n",
            "epoch: 174 steps: 537 return: 3.0 avg return: 3.0\n",
            "epoch: 175 steps: 540 return: 3.0 avg return: 3.0\n",
            "epoch: 176 steps: 543 return: 3.0 avg return: 3.0\n",
            "epoch: 177 steps: 546 return: 3.0 avg return: 3.0\n",
            "epoch: 178 steps: 549 return: 3.0 avg return: 3.0\n",
            "epoch: 179 steps: 552 return: 3.0 avg return: 3.0\n",
            "epoch: 180 steps: 555 return: 3.0 avg return: 3.0\n",
            "epoch: 181 steps: 558 return: 3.0 avg return: 3.0\n",
            "epoch: 182 steps: 561 return: 3.0 avg return: 3.0\n",
            "epoch: 183 steps: 564 return: 3.0 avg return: 3.0\n",
            "epoch: 184 steps: 567 return: 3.0 avg return: 3.0\n",
            "epoch: 185 steps: 570 return: 3.0 avg return: 3.0\n",
            "epoch: 186 steps: 573 return: 3.0 avg return: 3.0\n",
            "epoch: 187 steps: 576 return: 3.0 avg return: 3.0\n",
            "epoch: 188 steps: 579 return: 3.0 avg return: 3.0\n",
            "epoch: 189 steps: 582 return: 3.0 avg return: 3.0\n",
            "epoch: 190 steps: 585 return: 3.0 avg return: 3.0\n",
            "epoch: 191 steps: 588 return: 3.0 avg return: 3.0\n",
            "epoch: 192 steps: 591 return: 3.0 avg return: 3.0\n",
            "epoch: 193 steps: 595 return: 4.0 avg return: 3.02\n",
            "epoch: 194 steps: 599 return: 4.0 avg return: 3.04\n",
            "epoch: 195 steps: 605 return: 6.0 avg return: 3.1\n",
            "epoch: 196 steps: 619 return: 14.0 avg return: 3.32\n",
            "epoch: 197 steps: 636 return: 17.0 avg return: 3.6\n",
            "epoch: 198 steps: 647 return: 11.0 avg return: 3.76\n",
            "epoch: 199 steps: 655 return: 8.0 avg return: 3.86\n",
            "epoch: 200 steps: 669 return: 14.0 avg return: 4.08\n",
            "epoch: 201 steps: 675 return: 6.0 avg return: 4.14\n",
            "epoch: 202 steps: 688 return: 13.0 avg return: 4.34\n",
            "epoch: 203 steps: 701 return: 13.0 avg return: 4.54\n",
            "epoch: 204 steps: 743 return: 42.0 avg return: 5.32\n",
            "epoch: 205 steps: 768 return: 25.0 avg return: 5.76\n",
            "epoch: 206 steps: 794 return: 26.0 avg return: 6.22\n",
            "epoch: 207 steps: 824 return: 30.0 avg return: 6.76\n",
            "epoch: 208 steps: 859 return: 35.0 avg return: 7.4\n",
            "epoch: 209 steps: 893 return: 34.0 avg return: 8.02\n",
            "epoch: 210 steps: 943 return: 50.0 avg return: 8.96\n",
            "epoch: 211 steps: 992 return: 49.0 avg return: 9.88\n",
            "epoch: 212 steps: 1042 return: 50.0 avg return: 10.82\n",
            "epoch: 213 steps: 1096 return: 54.0 avg return: 11.84\n",
            "epoch: 214 steps: 1181 return: 85.0 avg return: 13.48\n",
            "epoch: 215 steps: 1263 return: 82.0 avg return: 15.06\n",
            "epoch: 216 steps: 1370 return: 107.0 avg return: 17.14\n",
            "epoch: 217 steps: 1442 return: 72.0 avg return: 18.52\n",
            "epoch: 218 steps: 1503 return: 61.0 avg return: 19.68\n",
            "epoch: 219 steps: 1563 return: 60.0 avg return: 20.82\n",
            "epoch: 220 steps: 1633 return: 70.0 avg return: 22.16\n",
            "epoch: 221 steps: 1724 return: 91.0 avg return: 23.92\n",
            "epoch: 222 steps: 1838 return: 114.0 avg return: 26.14\n",
            "epoch: 223 steps: 1918 return: 80.0 avg return: 27.68\n",
            "epoch: 224 steps: 2009 return: 91.0 avg return: 29.44\n",
            "epoch: 225 steps: 2094 return: 85.0 avg return: 31.08\n",
            "epoch: 226 steps: 2175 return: 81.0 avg return: 32.64\n",
            "epoch: 227 steps: 2240 return: 65.0 avg return: 33.88\n",
            "epoch: 228 steps: 2298 return: 58.0 avg return: 34.98\n",
            "epoch: 229 steps: 2355 return: 57.0 avg return: 36.06\n",
            "epoch: 230 steps: 2414 return: 59.0 avg return: 37.18\n",
            "epoch: 231 steps: 2468 return: 54.0 avg return: 38.2\n",
            "epoch: 232 steps: 2519 return: 51.0 avg return: 39.16\n",
            "epoch: 233 steps: 2576 return: 57.0 avg return: 40.24\n",
            "epoch: 234 steps: 2628 return: 52.0 avg return: 41.22\n",
            "epoch: 235 steps: 2675 return: 47.0 avg return: 42.1\n",
            "epoch: 236 steps: 2725 return: 50.0 avg return: 43.04\n",
            "epoch: 237 steps: 2780 return: 55.0 avg return: 44.08\n",
            "epoch: 238 steps: 2821 return: 41.0 avg return: 44.84\n",
            "epoch: 239 steps: 2868 return: 47.0 avg return: 45.72\n",
            "epoch: 240 steps: 2915 return: 47.0 avg return: 46.6\n",
            "epoch: 241 steps: 2959 return: 44.0 avg return: 47.42\n",
            "epoch: 242 steps: 3007 return: 48.0 avg return: 48.32\n",
            "epoch: 243 steps: 3055 return: 48.0 avg return: 49.2\n",
            "epoch: 244 steps: 3102 return: 47.0 avg return: 50.06\n",
            "epoch: 245 steps: 3142 return: 40.0 avg return: 50.74\n",
            "epoch: 246 steps: 3189 return: 47.0 avg return: 51.4\n",
            "epoch: 247 steps: 3238 return: 49.0 avg return: 52.04\n",
            "epoch: 248 steps: 3304 return: 66.0 avg return: 53.14\n",
            "epoch: 249 steps: 3356 return: 52.0 avg return: 54.02\n",
            "epoch: 250 steps: 3420 return: 64.0 avg return: 55.02\n",
            "epoch: 251 steps: 3476 return: 56.0 avg return: 56.02\n",
            "epoch: 252 steps: 3526 return: 50.0 avg return: 56.76\n",
            "epoch: 253 steps: 3578 return: 52.0 avg return: 57.54\n",
            "epoch: 254 steps: 3640 return: 62.0 avg return: 57.94\n",
            "epoch: 255 steps: 3729 return: 89.0 avg return: 59.22\n",
            "epoch: 256 steps: 3834 return: 105.0 avg return: 60.8\n",
            "epoch: 257 steps: 3909 return: 75.0 avg return: 61.7\n",
            "epoch: 258 steps: 4006 return: 97.0 avg return: 62.94\n",
            "epoch: 259 steps: 4068 return: 62.0 avg return: 63.5\n",
            "epoch: 260 steps: 4137 return: 69.0 avg return: 63.88\n",
            "epoch: 261 steps: 4216 return: 79.0 avg return: 64.48\n",
            "epoch: 262 steps: 4300 return: 84.0 avg return: 65.16\n",
            "epoch: 263 steps: 4417 return: 117.0 avg return: 66.42\n",
            "epoch: 264 steps: 4511 return: 94.0 avg return: 66.6\n",
            "epoch: 265 steps: 4550 return: 39.0 avg return: 65.74\n",
            "epoch: 266 steps: 4598 return: 48.0 avg return: 64.56\n",
            "epoch: 267 steps: 4709 return: 111.0 avg return: 65.34\n",
            "epoch: 268 steps: 4762 return: 53.0 avg return: 65.18\n",
            "epoch: 269 steps: 4837 return: 75.0 avg return: 65.48\n",
            "epoch: 270 steps: 4888 return: 51.0 avg return: 65.1\n",
            "epoch: 271 steps: 4946 return: 58.0 avg return: 64.44\n",
            "epoch: 272 steps: 4996 return: 50.0 avg return: 63.16\n",
            "epoch: 273 steps: 5062 return: 66.0 avg return: 62.88\n",
            "epoch: 274 steps: 5139 return: 77.0 avg return: 62.6\n",
            "epoch: 275 steps: 5202 return: 63.0 avg return: 62.16\n",
            "epoch: 276 steps: 5271 return: 69.0 avg return: 61.92\n",
            "epoch: 277 steps: 5371 return: 100.0 avg return: 62.62\n",
            "epoch: 278 steps: 5432 return: 61.0 avg return: 62.68\n",
            "epoch: 279 steps: 5507 return: 75.0 avg return: 63.04\n",
            "epoch: 280 steps: 5571 return: 64.0 avg return: 63.14\n",
            "epoch: 281 steps: 5631 return: 60.0 avg return: 63.26\n",
            "epoch: 282 steps: 5692 return: 61.0 avg return: 63.46\n",
            "epoch: 283 steps: 5752 return: 60.0 avg return: 63.52\n",
            "epoch: 284 steps: 5811 return: 59.0 avg return: 63.66\n",
            "epoch: 285 steps: 5873 return: 62.0 avg return: 63.96\n",
            "epoch: 286 steps: 5937 return: 64.0 avg return: 64.24\n",
            "epoch: 287 steps: 6005 return: 68.0 avg return: 64.5\n",
            "epoch: 288 steps: 6079 return: 74.0 avg return: 65.16\n",
            "epoch: 289 steps: 6149 return: 70.0 avg return: 65.62\n",
            "epoch: 290 steps: 6225 return: 76.0 avg return: 66.2\n",
            "epoch: 291 steps: 6316 return: 91.0 avg return: 67.14\n",
            "epoch: 292 steps: 6403 return: 87.0 avg return: 67.92\n",
            "epoch: 293 steps: 6497 return: 94.0 avg return: 68.84\n",
            "epoch: 294 steps: 6595 return: 98.0 avg return: 69.86\n",
            "epoch: 295 steps: 6790 return: 195.0 avg return: 72.96\n",
            "epoch: 296 steps: 6958 return: 168.0 avg return: 75.38\n",
            "epoch: 297 steps: 7241 return: 283.0 avg return: 80.06\n",
            "epoch: 298 steps: 7393 return: 152.0 avg return: 81.78\n",
            "epoch: 299 steps: 7957 return: 564.0 avg return: 92.02\n",
            "epoch: 300 steps: 8866 return: 909.0 avg return: 108.92\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bbef04327fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    243\u001b[0m                   critic_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0],\n\u001b[1;32m    244\u001b[0m                                                    action_dim=env.action_space.shape[0]))\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-bbef04327fc1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, environment_steps, training_steps, pre_sampling_steps)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bbef04327fc1>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bbef04327fc1>\u001b[0m in \u001b[0;36mupdate_target_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bbef04327fc1>\u001b[0m in \u001b[0;36m_weight_update\u001b[0;34m(self, target_network, network)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnew_wights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mnew_wights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_wights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    726\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0;32m--> 719\u001b[0;31m           self.handle, self._dtype)\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 480\u001b[0;31m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[0m\u001b[1;32m    481\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}