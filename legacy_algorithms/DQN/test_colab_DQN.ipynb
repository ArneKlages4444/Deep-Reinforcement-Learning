{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoOEob3gB00H",
        "outputId": "b517dd62-2bdf-42b2-b766-42b67eabfffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/gym/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[box2d]\n",
            "  Downloading gym-0.26.1.tar.gz (719 kB)\n",
            "\u001b[K     |████████████████████████████████| 719 kB 29.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d_py-2.3.5-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 37.7 MB/s \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 77 kB/s \n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.0.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (4.1.1)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.1-py3-none-any.whl size=826212 sha256=9524dc1c7d3b95d259011da32255b962ab41572b23562916fa750e58c6f37676\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/f0/10/6f06af57d047770ee4b45f9408dbb90bb55916892e8e9fbc86\n",
            "Successfully built gym\n",
            "Installing collected packages: swig, pygame, gym, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 gym-0.26.1 pygame-2.1.0 swig-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyvJYoQ7xngU",
        "outputId": "9f8e9ab4-c782-4c73-a7dc-c81bf3625476"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, state_dims, action_dims, max_size=1000000, batch_size=256):\n",
        "        self._max_size = max_size\n",
        "        self._batch_size = batch_size\n",
        "        self._size = 0\n",
        "        self._current_position = 0\n",
        "        self._state_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._state_prime_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._action_memory = np.zeros((self._max_size, 1))  # deiscrete actions\n",
        "        self._reward_memory = np.zeros((self._max_size, 1))\n",
        "        self._done_memory = np.zeros((self._max_size, 1), dtype=np.bool)\n",
        "\n",
        "    def size(self):\n",
        "        return self._size\n",
        "\n",
        "    def add_transition(self, state, action, reward, state_, done):\n",
        "        self._state_memory[self._current_position] = state\n",
        "        self._state_prime_memory[self._current_position] = state_\n",
        "        self._action_memory[self._current_position] = action\n",
        "        self._reward_memory[self._current_position] = reward\n",
        "        self._done_memory[self._current_position] = done\n",
        "        # self.un_norm_r[self.current_position] = r\n",
        "        # self.r = (self.un_norm_r - np.mean(self.un_norm_r)) / (np.std(self.un_norm_r) + 1e-10)\n",
        "        if self._size < self._max_size:\n",
        "            self._size += 1\n",
        "        self._current_position = (self._current_position + 1) % self._max_size\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch_indices = np.random.choice(self._size, self._batch_size, replace=False)\n",
        "        states = tf.convert_to_tensor(self._state_memory[batch_indices], dtype=tf.float32)\n",
        "        states_prime = tf.convert_to_tensor(self._state_prime_memory[batch_indices], dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self._action_memory[batch_indices], dtype=tf.int32)  # deiscrete actions\n",
        "        rewards = tf.convert_to_tensor(self._reward_memory[batch_indices], dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(self._done_memory[batch_indices], dtype=tf.float32)\n",
        "        return states, actions, rewards, states_prime, dones\n",
        "\n",
        "\n",
        "from tensorflow import losses\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def create_q_network(learning_rate, state_dim=8, action_dim=4):\n",
        "    inputs = keras.Input(shape=state_dim)\n",
        "    x = Dense(256, activation=\"relu\")(inputs)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    out = Dense(action_dim, activation=None)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=losses.mse)\n",
        "    return model\n",
        "\n",
        "\n",
        "# from ExperienceReplayBuffer import ExperienceReplayBuffer\n",
        "import tensorflow as tf\n",
        "from tensorflow import math as tfm\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, environment, state_dim, action_dim, q_network_generator,\n",
        "                 learning_rate=0.0003, gamma=0.99, tau=0.005,\n",
        "                 epsilon=1, epsilon_decay=0.99, min_epsilon=0.05,\n",
        "                 batch_size=256, max_replay_buffer_size=1000000):\n",
        "        self._environment = environment\n",
        "        self._action_dim = action_dim\n",
        "        self._gamma = gamma\n",
        "        self._tau = tau\n",
        "        self._epsilon = epsilon\n",
        "        self._epsilon_decay = epsilon_decay\n",
        "        self._min_epsilon = min_epsilon\n",
        "        self._batch_size = batch_size\n",
        "        self._mse = tf.keras.losses.MeanSquaredError()\n",
        "        self._reply_buffer = ExperienceReplayBuffer(state_dim, action_dim, max_replay_buffer_size, batch_size)\n",
        "        self._q_network = q_network_generator(learning_rate)\n",
        "        self._q_network_t = q_network_generator(learning_rate)\n",
        "        self._wight_init()\n",
        "\n",
        "    def reply_buffer(self):\n",
        "        return self._reply_buffer\n",
        "\n",
        "    def environment(self):\n",
        "        return self._environment\n",
        "\n",
        "    def _wight_init(self):\n",
        "        self._q_network_t.set_weights(self._q_network.weights)\n",
        "\n",
        "    def update_target_weights(self):\n",
        "        self._weight_update(self._q_network_t, self._q_network)\n",
        "\n",
        "    def _weight_update(self, target_network, network):\n",
        "        new_wights = []\n",
        "        for w_t, w in zip(target_network.weights, network.weights):\n",
        "            new_wights.append((1 - self._tau) * w_t + self._tau * w)\n",
        "        target_network.set_weights(new_wights)\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, states_prime, dones = self._reply_buffer.sample_batch()\n",
        "        self.train_step(states, actions, rewards, states_prime, dones)\n",
        "        self.update_target_weights()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, states_prime, dones):\n",
        "        q_values_prime = self._q_network_t(states_prime)\n",
        "        max_q = tf.reduce_max(q_values_prime, axis=-1, keepdims=True)\n",
        "        targets = rewards + self._gamma * (1 - dones) * max_q  # (1-d) : no q if done\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self._q_network(states)\n",
        "            q_values_of_actions = tf.gather(q_values, actions, axis=-1, batch_dims=1)\n",
        "            loss = self._mse(targets, q_values_of_actions)\n",
        "        gradients = tape.gradient(loss, self._q_network.trainable_variables)\n",
        "        self._q_network.optimizer.apply_gradients(zip(gradients, self._q_network.trainable_variables))\n",
        "\n",
        "    # alternative but ugly\n",
        "    def train_step2(self, state, action, rewards, state_prime, dones):\n",
        "        q_values = self._q_network_t(state_prime)\n",
        "        max_q = tf.reduce_max(q_values, axis=-1).numpy()\n",
        "        t = rewards + self._gamma * (1 - dones) * max_q  # (1-d) : no q if done\n",
        "\n",
        "        t_batch = self._q_network(state).numpy()\n",
        "        batch_index = np.arange(self._batch_size, dtype=np.int32)\n",
        "\n",
        "        t_batch[batch_index, action] = t\n",
        "        self._q_network.train_on_batch(state, t_batch)\n",
        "\n",
        "    def sample_actions1(self, state):  # sample with e greedy policy, alternative would be Thompson sampling\n",
        "        if np.random.random() <= self._epsilon:\n",
        "            actions = tf.random.uniform((1,), minval=0, maxval=self._action_dim, dtype=tf.int32)\n",
        "        else:\n",
        "            actions = self._deterministic_action(state)\n",
        "        self.epsilon = self._epsilon * self._epsilon_decay if self._epsilon > self._min_epsilon else self._min_epsilon\n",
        "        return actions\n",
        "\n",
        "    def sample_actions(self, state):\n",
        "        q_values = self._q_network(state)\n",
        "        distribution = tfd.Categorical(logits=q_values)\n",
        "        return distribution.sample()\n",
        "\n",
        "    def _deterministic_action(self, state):\n",
        "        return tf.argmax(self._q_network(tf.convert_to_tensor(state, dtype=tf.float32)), axis=-1)\n",
        "\n",
        "    def act_deterministic(self, state):\n",
        "        actions_prime = self._deterministic_action(tf.convert_to_tensor(state, dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def act_stochastic(self, state):\n",
        "        actions_prime = self.sample_actions(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def _act(self, actions):\n",
        "        observation_prime, reward, done, _, _ = self._environment.step(actions.numpy()[0])\n",
        "        return actions, observation_prime, reward, done\n",
        "\n",
        "    def train(self, epochs, environment_steps=1, training_steps=1, pre_sampling_steps=0):\n",
        "        print(f\"Random exploration for {pre_sampling_steps} steps!\")\n",
        "        observation, _ = self._environment.reset()\n",
        "        ret = 0\n",
        "        for _ in range(max(pre_sampling_steps, self._batch_size)):\n",
        "            actions = tf.random.uniform((1,), minval=0, maxval=self._action_dim, dtype=tf.int32)\n",
        "            observation_prime, reward, done, _, _ = self._environment.step(actions.numpy()[0])\n",
        "            ret += reward\n",
        "            self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "            if done:\n",
        "                print(\"print\", ret)\n",
        "                ret = 0\n",
        "                observation, _ = self._environment.reset()\n",
        "            else:\n",
        "                observation = observation_prime\n",
        "        print(\"print\", ret)\n",
        "\n",
        "        print(\"start training!\")\n",
        "        returns = []\n",
        "        observation, _ = self._environment.reset()\n",
        "        done = 0\n",
        "        ret = 0\n",
        "        epoch = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < environment_steps or self._reply_buffer.size() < self._batch_size:\n",
        "                if done:\n",
        "                    observation, _ = self._environment.reset()\n",
        "                    returns.append(ret)\n",
        "                    print(\"epoch:\", epoch, \"steps:\", steps, \"return:\", ret, \"avg return:\", np.average(returns[-50:]))\n",
        "                    ret = 0\n",
        "                    epoch += 1\n",
        "                    if epoch >= epochs:\n",
        "                        print(\"training finished!\")\n",
        "                        return\n",
        "                actions, observation_prime, reward, done = self.act_stochastic(observation)\n",
        "                self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "                observation = observation_prime\n",
        "                steps += 1\n",
        "                ret += reward\n",
        "                i += 1\n",
        "            for _ in range(training_steps):\n",
        "                self.learn()\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "# from Networks.LunaLanderNetwork import create_q_network\n",
        "# from DQNAgent import Agent\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.keras.backend.clear_session()\n",
        "    env = gym.make(\"LunarLander-v2\")\n",
        "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space)\n",
        "\n",
        "    agent = Agent(environment=env, state_dim=env.observation_space.shape, action_dim=4,\n",
        "                  q_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0], action_dim=4), batch_size=256)\n",
        "    agent.train(10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u3kl_SYnB-E_",
        "outputId": "6836a021-04dd-4b94-c38e-26623c45059e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_dim= (8,) action_dim= Discrete(4)\n",
            "Random exploration for 0 steps!\n",
            "print -297.42340168639737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "print -161.4860191312706\n",
            "print -14.869750413537542\n",
            "start training!\n",
            "epoch: 0 steps: 88 return: 30.855235133004783 avg return: 30.855235133004783\n",
            "epoch: 1 steps: 204 return: -54.196082381617515 avg return: -11.670423624306366\n",
            "epoch: 2 steps: 376 return: -166.83234015902195 avg return: -63.39106246921156\n",
            "epoch: 3 steps: 498 return: -215.023563779379 avg return: -101.29918779675342\n",
            "epoch: 4 steps: 621 return: -199.42255571043523 avg return: -120.9238613794898\n",
            "epoch: 5 steps: 803 return: 22.283696324620152 avg return: -97.05593509547147\n",
            "epoch: 6 steps: 1194 return: -77.40962665828505 avg return: -94.24931960444484\n",
            "epoch: 7 steps: 1408 return: -85.65711927042575 avg return: -93.17529456269244\n",
            "epoch: 8 steps: 1591 return: -597.2996430331536 avg return: -149.18911105941035\n",
            "epoch: 9 steps: 1860 return: 1.4037125302213553 avg return: -134.12982870044718\n",
            "epoch: 10 steps: 2599 return: 146.15933344769462 avg return: -108.64899577788884\n",
            "epoch: 11 steps: 2874 return: 6.142562558208127 avg return: -99.0830325832141\n",
            "epoch: 12 steps: 3162 return: -88.84275190126736 avg return: -98.29531868460282\n",
            "epoch: 13 steps: 3364 return: -11.391059883679233 avg return: -92.08787162739398\n",
            "epoch: 14 steps: 3500 return: -46.114559503378516 avg return: -89.02298415245963\n",
            "epoch: 15 steps: 4051 return: -160.33619221713977 avg return: -93.48005965650214\n",
            "epoch: 16 steps: 4421 return: -40.10871796925159 avg return: -90.3405689690168\n",
            "epoch: 17 steps: 4600 return: -47.1821981538071 avg return: -87.94288170150516\n",
            "epoch: 18 steps: 4729 return: -5.109490216885135 avg return: -83.5832295181041\n",
            "epoch: 19 steps: 4986 return: -27.715702791240673 avg return: -80.78985318176093\n",
            "epoch: 20 steps: 5048 return: -11.669832376728678 avg return: -77.49842361961653\n",
            "epoch: 21 steps: 5142 return: -34.24034222595327 avg return: -75.53214719263184\n",
            "epoch: 22 steps: 5326 return: 12.056159714589242 avg return: -71.72395993579615\n",
            "epoch: 23 steps: 5548 return: 9.261688489847629 avg return: -68.34955791806098\n",
            "epoch: 24 steps: 5730 return: 16.5990276759032 avg return: -64.95161449430242\n",
            "epoch: 25 steps: 5913 return: -33.37962102003702 avg return: -63.73730705298452\n",
            "epoch: 26 steps: 6141 return: 39.82371508065745 avg return: -59.901713640627406\n",
            "epoch: 27 steps: 6350 return: -289.2535108683486 avg return: -68.09284925590316\n",
            "epoch: 28 steps: 6455 return: -49.54385951111007 avg return: -67.45322891987581\n",
            "epoch: 29 steps: 6534 return: -29.922389960544336 avg return: -66.20220095456476\n",
            "epoch: 30 steps: 6725 return: 15.43537101836526 avg return: -63.568730890921856\n",
            "epoch: 31 steps: 6838 return: -57.78057933876082 avg return: -63.38785115491683\n",
            "epoch: 32 steps: 6981 return: -20.130630462582687 avg return: -62.07702628545216\n",
            "epoch: 33 steps: 7265 return: 34.375813410072396 avg return: -59.240178059113205\n",
            "epoch: 34 steps: 7407 return: -4.628636871772102 avg return: -57.67984831090346\n",
            "epoch: 35 steps: 7650 return: 1.782470578925043 avg return: -56.02811723063044\n",
            "epoch: 36 steps: 7767 return: 23.706717866465226 avg return: -53.8731216874657\n",
            "epoch: 37 steps: 7897 return: 3.869640561071151 avg return: -52.353575312504205\n",
            "epoch: 38 steps: 7967 return: -3.070251797385808 avg return: -51.08990035057809\n",
            "epoch: 39 steps: 8141 return: 5.8332219857106224 avg return: -49.66682229217086\n",
            "epoch: 40 steps: 8449 return: -158.74120738810535 avg return: -52.32717314816927\n",
            "epoch: 41 steps: 8587 return: 22.215629715211406 avg return: -50.55234450856497\n",
            "epoch: 42 steps: 8910 return: -226.1714585537954 avg return: -54.6365099514773\n",
            "epoch: 43 steps: 9030 return: -32.90311537787032 avg return: -54.1425691657135\n",
            "epoch: 44 steps: 9155 return: 11.842605964447486 avg return: -52.67623194059881\n",
            "epoch: 45 steps: 9255 return: 2.1907461590268156 avg return: -51.483471547128694\n",
            "epoch: 46 steps: 9500 return: 292.5506032563918 avg return: -44.1635976151389\n",
            "epoch: 47 steps: 9637 return: 39.51784876728698 avg return: -42.420234148838354\n",
            "epoch: 48 steps: 9764 return: -173.36073860408013 avg return: -45.09248934180248\n",
            "epoch: 49 steps: 9931 return: 221.30479084190864 avg return: -39.76454373812825\n",
            "epoch: 50 steps: 10054 return: 37.947942377879286 avg return: -39.62268959323077\n",
            "epoch: 51 steps: 10134 return: -7.271222778900139 avg return: -38.68419240117642\n",
            "epoch: 52 steps: 10261 return: 8.594244502924667 avg return: -35.17566070793749\n",
            "epoch: 53 steps: 10392 return: -63.94589328235632 avg return: -32.15410729799703\n",
            "epoch: 54 steps: 10584 return: 243.43753152178212 avg return: -23.29690555335268\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d77903456856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    218\u001b[0m     agent = Agent(environment=env, state_dim=env.observation_space.shape, action_dim=4,\n\u001b[1;32m    219\u001b[0m                   q_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0], action_dim=4), batch_size=256)\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-d77903456856>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, environment_steps, training_steps, pre_sampling_steps)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d77903456856>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reply_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}