{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoOEob3gB00H",
    "outputId": "ea3ce82f-9acf-4dc2-a36e-e6a273a3274d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: gym 0.25.2\n",
      "Uninstalling gym-0.25.2:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.9/dist-packages/gym-0.25.2.dist-info/*\n",
      "    /usr/local/lib/python3.9/dist-packages/gym/*\n",
      "Proceed (Y/n)? y\n",
      "  Successfully uninstalled gym-0.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gym[mujoco]\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m721.7/721.7 KB\u001B[0m \u001B[31m20.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[mujoco]) (6.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[mujoco]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[mujoco]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[mujoco]) (1.22.4)\n",
      "Collecting imageio>=2.14.1\n",
      "  Downloading imageio-2.26.0-py3-none-any.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m56.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting mujoco==2.2\n",
      "  Downloading mujoco-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m35.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.9/dist-packages (from mujoco==2.2->gym[mujoco]) (3.1.6)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mujoco==2.2->gym[mujoco]) (1.4.0)\n",
      "Collecting glfw\n",
      "  Downloading glfw-2.5.6-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.6/207.6 KB\u001B[0m \u001B[31m18.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from imageio>=2.14.1->gym[mujoco]) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.15.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827648 sha256=cbd7fff514e449aba19e2dc2834db364a85c2053a67f0f952f8f826ff020a96c\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\n",
      "Successfully built gym\n",
      "Installing collected packages: glfw, mujoco, imageio, gym\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.9.0\n",
      "    Uninstalling imageio-2.9.0:\n",
      "      Successfully uninstalled imageio-2.9.0\n",
      "Successfully installed glfw-2.5.6 gym-0.26.2 imageio-2.26.0 mujoco-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym[mujoco]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9hF7N706aXK",
    "outputId": "46ad6949-bcc6-409f-9549-358add0965d2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EpisodeBuffer:\n",
    "\n",
    "    def __init__(self, gamma, gae_lambda):\n",
    "        self._gamma = gamma\n",
    "        self._gae_lambda = gae_lambda\n",
    "        self._s = []\n",
    "        self._a = []\n",
    "        self._r = []\n",
    "        self._v = []\n",
    "        self._p = []\n",
    "        self._d = []\n",
    "\n",
    "    def add(self, s, a, r, v, p, d):\n",
    "        self._s.append(tf.convert_to_tensor(s, dtype=tf.float32))\n",
    "        self._a.append(tf.convert_to_tensor(a, dtype=tf.float32))\n",
    "        self._r.append(tf.convert_to_tensor(r, dtype=tf.float32))\n",
    "        self._v.append(tf.convert_to_tensor(v, dtype=tf.float32))\n",
    "        self._p.append(tf.convert_to_tensor(p, dtype=tf.float32))\n",
    "        self._d.append(tf.convert_to_tensor(d, dtype=tf.float32))\n",
    "\n",
    "    # generalized advantage estimate\n",
    "    def estimate_advantage(self, rewards, values, dones):  # TODO: rework\n",
    "        advantage = np.zeros_like(rewards, dtype=np.float32)\n",
    "        for t in range(len(rewards) - 1):\n",
    "            discount = 1\n",
    "            a_t = 0\n",
    "            for k in range(t, len(rewards) - 1):\n",
    "                a_t += discount * (rewards[k] + self._gamma * values[k + 1] * (1 - dones[k]) - values[k])\n",
    "                discount *= self._gamma * self._gae_lambda\n",
    "            advantage[t] = a_t\n",
    "        return advantage\n",
    "\n",
    "    def get_episode(self):\n",
    "        adv = self.estimate_advantage(self._r, self._v, self._d)\n",
    "        g = adv + np.asarray(self._v)\n",
    "        return (tf.convert_to_tensor(self._s), tf.convert_to_tensor(self._a), tf.convert_to_tensor(g),\n",
    "                tf.convert_to_tensor(adv), tf.convert_to_tensor(self._p))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def create_policy_network(learning_rate, state_dim, action_dim):\n",
    "    inputs = keras.Input(shape=state_dim)\n",
    "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    mu = Dense(action_dim, activation=None)(x)\n",
    "    sigma = Dense(action_dim, activation=tf.nn.softplus)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=(mu, sigma))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_value_network(learning_rate, state_dim):\n",
    "    inputs = keras.Input(shape=state_dim)\n",
    "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    out = Dense(1, activation=None)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "# from EpisodeBuffer import EpisodeBuffer\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, environment, actor_network_generator, critic_network_generator, updates_per_episode=80,\n",
    "                 epsilon=0.2, gae_lambda=0.95, learning_rate=0.0003, gamma=0.99, alpha=0.2, kld_threshold=0.05):\n",
    "        self._updates_per_episode = updates_per_episode\n",
    "        self._environment = environment\n",
    "        self._epsilon = epsilon\n",
    "        self._gae_lambda = gae_lambda\n",
    "        self._gamma = gamma\n",
    "        self._alpha = alpha\n",
    "        self._learning_rate = learning_rate\n",
    "        self._mse = tf.keras.losses.MeanSquaredError()\n",
    "        self._policy_network = actor_network_generator(learning_rate)\n",
    "        self._value_network = critic_network_generator(learning_rate)\n",
    "        self._kld_threshold = kld_threshold\n",
    "\n",
    "    def distribution_form_policy(self, state):\n",
    "        mu, sigma = self._policy_network(state)\n",
    "        return tfd.Normal(mu, sigma)\n",
    "\n",
    "    def sample_actions_form_policy(self, state):\n",
    "        distribution = self.distribution_form_policy(state)\n",
    "        actions = distribution.sample()\n",
    "        log_probs = self.log_probs_form_distribution(distribution, actions)\n",
    "        return actions, log_probs\n",
    "\n",
    "    def log_probs_form_policy(self, state, actions):\n",
    "        distribution = self.distribution_form_policy(state)\n",
    "        return self.log_probs_form_distribution(distribution, actions), distribution.entropy()\n",
    "\n",
    "    def log_probs_form_distribution(self, distribution, actions):\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "        return tfm.reduce_sum(log_probs, axis=-1, keepdims=True)\n",
    "\n",
    "    def act_stochastic(self, state):\n",
    "        actions_prime, log_probs = self.sample_actions_form_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return self._act(actions_prime) + (log_probs,)\n",
    "\n",
    "    def _act(self, actions):\n",
    "        observation_prime, reward, terminated, truncated, _ = self._environment.step(actions[0])\n",
    "        return actions, observation_prime, reward, terminated or truncated\n",
    "\n",
    "    def learn2(self, episode):\n",
    "        s, a, r_sum, adv, porb_old_policy = episode\n",
    "        for _ in range(self._updates_per_episode):\n",
    "            if self.train_step_actor(s, a, adv, porb_old_policy):\n",
    "                break\n",
    "        for _ in range(self._updates_per_episode):\n",
    "            self.train_step_critic(s, r_sum)\n",
    "\n",
    "    @tf.function\n",
    "    def learn(self, episode):\n",
    "        s, a, r_sum, adv, porb_old_policy = episode\n",
    "\n",
    "        st = False\n",
    "        it = 0\n",
    "        c = lambda stop, i: (not stop) or i < self._updates_per_episode\n",
    "        b = lambda stop, i: (self.train_step_actor(s, a, adv, porb_old_policy), i + 1)\n",
    "        tf.while_loop(c, b, [st, it])\n",
    "\n",
    "        for _ in range(self._updates_per_episode):\n",
    "            self.train_step_critic(s, r_sum)\n",
    "\n",
    "    def train_step_actor(self, s, a, adv, porb_old_policy):\n",
    "        early_stoppling = False\n",
    "        with tf.GradientTape() as tape:\n",
    "            porb_current_policy, entropy = self.log_probs_form_policy(s, a)\n",
    "            kld = tf.math.reduce_mean(porb_current_policy - porb_old_policy)  # aproximated Kullback Leibler Divergence\n",
    "            if tfm.abs(kld) > self._kld_threshold:  # early stoppling if KLD is too high\n",
    "                early_stoppling = True\n",
    "            else:\n",
    "                # prob of current policy / prob of old policy (log probs: p/p2 = log(p)-log(p2)\n",
    "                p = tf.math.exp(porb_current_policy - porb_old_policy)  # exp() to un do log(p)\n",
    "                clipped_p = tf.clip_by_value(p, 1 - self._epsilon, 1 + self._epsilon)\n",
    "                policy_loss = -tfm.reduce_mean(tfm.minimum(p * adv, clipped_p * adv))\n",
    "                # entropy_loss = -tfm.reduce_mean(-porb_current_policy)  # approximate entropy\n",
    "                entropy_loss = -tfm.reduce_mean(entropy)\n",
    "                loss = policy_loss + self._alpha * entropy_loss\n",
    "\n",
    "                gradients = tape.gradient(loss, self._policy_network.trainable_variables)\n",
    "                self._policy_network.optimizer.apply_gradients(zip(gradients, self._policy_network.trainable_variables))\n",
    "        return early_stoppling\n",
    "\n",
    "    def train_step_critic(self, s, r_sum):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prev_v = self._value_network(s)\n",
    "            loss = self._mse(r_sum, prev_v)\n",
    "        gradients = tape.gradient(loss, self._value_network.trainable_variables)\n",
    "        self._value_network.optimizer.apply_gradients(zip(gradients, self._value_network.trainable_variables))\n",
    "\n",
    "    def sample_to_episode_buffer(self, max_steps_per_episode):\n",
    "        buffer = EpisodeBuffer(self._gamma, self._gae_lambda)\n",
    "        s, _ = self._environment.reset()\n",
    "        d = 0\n",
    "        ret = 0\n",
    "        i = 0\n",
    "        while not d and (max_steps_per_episode is None or i < max_steps_per_episode):\n",
    "            a, s_p, r, d, p = self.act_stochastic(s)\n",
    "            ret += r\n",
    "            v = self._value_network(tf.convert_to_tensor([s], dtype=tf.float32))\n",
    "            buffer.add(s, tf.squeeze(a, 1), [r], tf.squeeze(v, 1), tf.squeeze(p, 1), d)\n",
    "            s = s_p\n",
    "            i += 1\n",
    "        return buffer, ret\n",
    "\n",
    "    def train(self, epochs, max_steps_per_episode=None):\n",
    "        print(\"start training!\")\n",
    "        rets = []\n",
    "        for e in range(epochs):\n",
    "            buffer, ret = self.sample_to_episode_buffer(max_steps_per_episode)\n",
    "            rets.append(ret)\n",
    "            print(\"epoch:\", e, \"return of episode:\", ret, \"avg 100:\", np.average(rets[-100:]))\n",
    "            episode = buffer.get_episode()\n",
    "            self.learn(episode)\n",
    "        print(\"training finished!\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# from Agent import Agent\n",
    "# from GenericMLPs1D import create_policy_network, create_value_network\n",
    "import gym\n",
    "from functools import partial\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.keras.backend.clear_session()\n",
    "    env = gym.make('InvertedPendulum-v4')\n",
    "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space.shape[0], \"action_scaling:\",\n",
    "          env.action_space.high)\n",
    "    agent = Agent(environment=env,\n",
    "                  actor_network_generator=partial(create_policy_network, state_dim=env.observation_space.shape[0],\n",
    "                                                  action_dim=env.action_space.shape[0]),\n",
    "                  critic_network_generator=partial(create_value_network, state_dim=env.observation_space.shape))\n",
    "    agent.train(epochs=1000, max_steps_per_episode=200)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u3kl_SYnB-E_",
    "outputId": "aa05655f-7c9f-4eec-e66c-280077d147a6"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "state_dim= (4,) action_dim= 1 action_scaling: [3.]\n",
      "start training!\n",
      "epoch: 0 return of episode: 15.0 avg 100: 15.0\n",
      "epoch: 1 return of episode: 13.0 avg 100: 14.0\n",
      "epoch: 2 return of episode: 30.0 avg 100: 19.333333333333332\n",
      "epoch: 3 return of episode: 27.0 avg 100: 21.25\n",
      "epoch: 4 return of episode: 26.0 avg 100: 22.2\n",
      "epoch: 5 return of episode: 10.0 avg 100: 20.166666666666668\n",
      "epoch: 6 return of episode: 18.0 avg 100: 19.857142857142858\n",
      "epoch: 7 return of episode: 22.0 avg 100: 20.125\n",
      "epoch: 8 return of episode: 6.0 avg 100: 18.555555555555557\n",
      "epoch: 9 return of episode: 36.0 avg 100: 20.3\n",
      "epoch: 10 return of episode: 17.0 avg 100: 20.0\n",
      "epoch: 11 return of episode: 26.0 avg 100: 20.5\n",
      "epoch: 12 return of episode: 19.0 avg 100: 20.384615384615383\n",
      "epoch: 13 return of episode: 43.0 avg 100: 22.0\n",
      "epoch: 14 return of episode: 55.0 avg 100: 24.2\n",
      "epoch: 15 return of episode: 44.0 avg 100: 25.4375\n",
      "epoch: 16 return of episode: 49.0 avg 100: 26.823529411764707\n",
      "epoch: 17 return of episode: 61.0 avg 100: 28.72222222222222\n",
      "epoch: 18 return of episode: 63.0 avg 100: 30.526315789473685\n",
      "epoch: 19 return of episode: 75.0 avg 100: 32.75\n",
      "epoch: 20 return of episode: 129.0 avg 100: 37.333333333333336\n",
      "epoch: 21 return of episode: 81.0 avg 100: 39.31818181818182\n",
      "epoch: 22 return of episode: 75.0 avg 100: 40.869565217391305\n",
      "epoch: 23 return of episode: 103.0 avg 100: 43.458333333333336\n",
      "epoch: 24 return of episode: 89.0 avg 100: 45.28\n",
      "epoch: 25 return of episode: 149.0 avg 100: 49.26923076923077\n",
      "epoch: 26 return of episode: 142.0 avg 100: 52.7037037037037\n",
      "epoch: 27 return of episode: 148.0 avg 100: 56.107142857142854\n",
      "epoch: 28 return of episode: 162.0 avg 100: 59.758620689655174\n",
      "epoch: 29 return of episode: 170.0 avg 100: 63.43333333333333\n",
      "epoch: 30 return of episode: 133.0 avg 100: 65.6774193548387\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:5 out of the last 87 calls to <function Agent.train_step_actor at 0x7f51f028f670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 31 return of episode: 145.0 avg 100: 68.15625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:6 out of the last 90 calls to <function Agent.train_step_actor at 0x7f51f028f670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 32 return of episode: 183.0 avg 100: 71.63636363636364\n",
      "epoch: 33 return of episode: 221.0 avg 100: 76.02941176470588\n",
      "epoch: 34 return of episode: 210.0 avg 100: 79.85714285714286\n",
      "epoch: 35 return of episode: 161.0 avg 100: 82.11111111111111\n",
      "epoch: 36 return of episode: 222.0 avg 100: 85.89189189189189\n",
      "epoch: 37 return of episode: 127.0 avg 100: 86.97368421052632\n",
      "epoch: 38 return of episode: 181.0 avg 100: 89.38461538461539\n",
      "epoch: 39 return of episode: 165.0 avg 100: 91.275\n",
      "epoch: 40 return of episode: 139.0 avg 100: 92.4390243902439\n",
      "epoch: 41 return of episode: 186.0 avg 100: 94.66666666666667\n",
      "epoch: 42 return of episode: 113.0 avg 100: 95.09302325581395\n",
      "epoch: 43 return of episode: 98.0 avg 100: 95.1590909090909\n",
      "epoch: 44 return of episode: 89.0 avg 100: 95.02222222222223\n",
      "epoch: 45 return of episode: 91.0 avg 100: 94.93478260869566\n",
      "epoch: 46 return of episode: 98.0 avg 100: 95.0\n",
      "epoch: 47 return of episode: 100.0 avg 100: 95.10416666666667\n",
      "epoch: 48 return of episode: 92.0 avg 100: 95.04081632653062\n",
      "epoch: 49 return of episode: 89.0 avg 100: 94.92\n",
      "epoch: 50 return of episode: 113.0 avg 100: 95.27450980392157\n",
      "epoch: 51 return of episode: 129.0 avg 100: 95.92307692307692\n",
      "epoch: 52 return of episode: 134.0 avg 100: 96.64150943396227\n",
      "epoch: 53 return of episode: 181.0 avg 100: 98.20370370370371\n",
      "epoch: 54 return of episode: 222.0 avg 100: 100.45454545454545\n",
      "epoch: 55 return of episode: 199.0 avg 100: 102.21428571428571\n",
      "epoch: 56 return of episode: 196.0 avg 100: 103.85964912280701\n",
      "epoch: 57 return of episode: 218.0 avg 100: 105.82758620689656\n",
      "epoch: 58 return of episode: 243.0 avg 100: 108.15254237288136\n",
      "epoch: 59 return of episode: 410.0 avg 100: 113.18333333333334\n",
      "epoch: 60 return of episode: 350.0 avg 100: 117.06557377049181\n",
      "epoch: 61 return of episode: 564.0 avg 100: 124.2741935483871\n",
      "epoch: 62 return of episode: 551.0 avg 100: 131.04761904761904\n",
      "epoch: 63 return of episode: 1000.0 avg 100: 144.625\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-825059ced6b1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    218\u001B[0m                                                   action_dim=env.action_space.shape[0]),\n\u001B[1;32m    219\u001B[0m                   critic_network_generator=partial(create_value_network, state_dim=env.observation_space.shape))\n\u001B[0;32m--> 220\u001B[0;31m     \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-5-825059ced6b1>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, epochs, max_steps_per_episode)\u001B[0m\n\u001B[1;32m    199\u001B[0m             \u001B[0mrets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mret\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"epoch:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"return of episode:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"avg 100:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maverage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m             \u001B[0mepisode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_episode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepisode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"training finished!\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-825059ced6b1>\u001B[0m in \u001B[0;36mget_episode\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_episode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0madv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_advantage_estimator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_r\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_v\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_d\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calc_rewards_to_go\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_r\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_v\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         return (tf.convert_to_tensor(self._s), tf.convert_to_tensor(self._a), tf.convert_to_tensor(self._r),\n",
      "\u001B[0;32m<ipython-input-5-825059ced6b1>\u001B[0m in \u001B[0;36mestimate_advantage\u001B[0;34m(self, rewards, values, dones)\u001B[0m\n\u001B[1;32m     88\u001B[0m             \u001B[0ma_t\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrewards\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m                 \u001B[0ma_t\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdiscount\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mrewards\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gamma\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mdones\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m                 \u001B[0mdiscount\u001B[0m \u001B[0;34m*=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gamma\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gae_lambda\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m             \u001B[0madvantage\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ma_t\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mr_binary_op_wrapper\u001B[0;34m(y, x)\u001B[0m\n\u001B[1;32m   1439\u001B[0m       \u001B[0;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1440\u001B[0m       \u001B[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1441\u001B[0;31m       \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmaybe_promote_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_same_dtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1442\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1443\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mmaybe_promote_tensors\u001B[0;34m(force_same_dtype, *tensors)\u001B[0m\n\u001B[1;32m   1372\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mtensor\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtensors\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1373\u001B[0m       promoted_tensors.append(\n\u001B[0;32m-> 1374\u001B[0;31m           ops.convert_to_tensor(tensor, dtype, name=\"x\"))\n\u001B[0m\u001B[1;32m   1375\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mpromoted_tensors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1376\u001B[0m   result_type = np_dtypes._result_type(\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/profiler/trace.py\u001B[0m in \u001B[0;36mwrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    181\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrace_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mtrace_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m           \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 183\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    184\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mconvert_to_tensor\u001B[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[1;32m   1592\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1593\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1594\u001B[0;31m     \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1595\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1596\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_compatible_with\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/dtypes.py\u001B[0m in \u001B[0;36mas_dtype\u001B[0;34m(type_value)\u001B[0m\n\u001B[1;32m    737\u001B[0m   \"\"\"\n\u001B[1;32m    738\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtype_value\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 739\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_INTERN_TABLE\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype_value\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_datatype_enum\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    741\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtype_value\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ]
}