{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoOEob3gB00H",
    "outputId": "ef0c1974-7a10-46ed-aab9-e34f5000a796"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: gym 0.25.2\n",
      "Uninstalling gym-0.25.2:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.7/dist-packages/gym-0.25.2.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/gym/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled gym-0.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gym[mujoco]\n",
      "  Downloading gym-0.26.1.tar.gz (719 kB)\n",
      "\u001B[K     |████████████████████████████████| 719 kB 7.5 MB/s \n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "    Preparing wheel metadata ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.5.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[mujoco]) (1.21.6)\n",
      "Collecting imageio>=2.14.1\n",
      "  Downloading imageio-2.22.0-py3-none-any.whl (3.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.4 MB 39.1 MB/s \n",
      "\u001B[?25hCollecting mujoco==2.2.0\n",
      "  Downloading mujoco-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.5 MB 49.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (3.1.6)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.2.0)\n",
      "Collecting glfw\n",
      "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
      "\u001B[K     |████████████████████████████████| 207 kB 43.9 MB/s \n",
      "\u001B[?25hCollecting pillow>=8.3.2\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.1 MB 52.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (4.1.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.26.1-py3-none-any.whl size=826211 sha256=b73e931c92ef5841787084b5af5b256f0f3734942d6ab291bdc8b2a5a649f0de\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/f0/10/6f06af57d047770ee4b45f9408dbb90bb55916892e8e9fbc86\n",
      "Successfully built gym\n",
      "Installing collected packages: pillow, glfw, mujoco, imageio, gym\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 7.1.2\n",
      "    Uninstalling Pillow-7.1.2:\n",
      "      Successfully uninstalled Pillow-7.1.2\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.9.0\n",
      "    Uninstalling imageio-2.9.0:\n",
      "      Successfully uninstalled imageio-2.9.0\n",
      "Successfully installed glfw-2.5.5 gym-0.26.1 imageio-2.22.0 mujoco-2.2.0 pillow-9.2.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip3 install gym[mujoco]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "id": "b9hF7N706aXK",
    "outputId": "df8057df-5ac4-4fad-e728-6875745793a9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EpisodeBuffer:\n",
    "\n",
    "    def __init__(self, advantage_estimator, calc_rewards_to_go):\n",
    "        self._advantage_estimator = advantage_estimator\n",
    "        self._calc_rewards_to_go = calc_rewards_to_go\n",
    "        self._s = []\n",
    "        self._a = []\n",
    "        self._r = []\n",
    "        self._v = []\n",
    "        self._d = []\n",
    "\n",
    "    def add(self, s, a, r, v, d):\n",
    "        self._s.append(tf.convert_to_tensor(s, dtype=tf.float32))\n",
    "        self._a.append(tf.convert_to_tensor(a, dtype=tf.float32))\n",
    "        self._r.append(tf.convert_to_tensor(r, dtype=tf.float32))\n",
    "        self._v.append(tf.convert_to_tensor(v, dtype=tf.float32))\n",
    "        self._d.append(tf.convert_to_tensor(d, dtype=tf.int32))\n",
    "\n",
    "    def get_as_data_set(self):\n",
    "        g = self._calc_rewards_to_go(self._r)\n",
    "        adv = self._advantage_estimator(self._r, self._v, self._d)\n",
    "        return tf.data.Dataset.from_tensor_slices((self._s, self._a, self._r, g, adv)).batch(1)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._s)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def create_policy_network(learning_rate, state_dim, action_dim):\n",
    "    inputs = keras.Input(shape=state_dim)\n",
    "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    mu = Dense(action_dim, activation=None)(x)\n",
    "    sigma = Dense(action_dim, activation=tf.nn.softplus)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=(mu, sigma))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_value_network(learning_rate, state_dim):\n",
    "    inputs = keras.Input(shape=state_dim)\n",
    "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    out = Dense(1, activation=None)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "# from EpisodeBuffer import EpisodeBuffer\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, environment, actor_network_generator, critic_network_generator,\n",
    "                 gae_lambda=0.95, learning_rate=0.0003, gamma=0.99):\n",
    "        self._environment = environment\n",
    "        self._gae_lambda = gae_lambda\n",
    "        self._gamma = gamma\n",
    "        self._learning_rate = learning_rate\n",
    "        self._mse = tf.keras.losses.MeanSquaredError()\n",
    "        self._policy_network = actor_network_generator(learning_rate)\n",
    "        self._value_network = critic_network_generator(learning_rate)\n",
    "\n",
    "    # generalized advantage estimate\n",
    "    def estimate_advantage(self, rewards, values, dones):  # TODO: rework\n",
    "        advantage = np.zeros(len(rewards), dtype=np.float32)\n",
    "        for t in range(len(rewards) - 1):\n",
    "            discount = 1\n",
    "            a_t = 0\n",
    "            for k in range(t, len(rewards) - 1):\n",
    "                a_t += discount * (rewards[k] + self._gamma * values[k + 1] * (1 - dones[k]) - values[k])\n",
    "                discount *= self._gamma * self._gae_lambda\n",
    "            advantage[t] = a_t\n",
    "        return advantage\n",
    "\n",
    "    def calc_rewards_to_go(self, rewards):\n",
    "        g = np.zeros_like(rewards, dtype=np.float32)  # TODO: better implementation for discounting (cumsum, tf.scan)\n",
    "        for t in range(len(rewards)):\n",
    "            g_sum = 0\n",
    "            gamma_t = 1\n",
    "            for k in range(t, len(rewards)):\n",
    "                g_sum += rewards[k] * gamma_t  # .numpy()\n",
    "                gamma_t *= self._gamma\n",
    "            g[t] = g_sum\n",
    "        return g\n",
    "\n",
    "    @tf.function\n",
    "    def distribution_form_policy(self, state):\n",
    "        mu, sigma = self._policy_network(state)\n",
    "        return tfd.Normal(mu, sigma)\n",
    "\n",
    "    @tf.function\n",
    "    def sample_actions_form_policy(self, state):\n",
    "        distribution = self.distribution_form_policy(state)\n",
    "        actions = distribution.sample()\n",
    "        return actions\n",
    "\n",
    "    @tf.function\n",
    "    def log_probs_form_policy(self, state, actions):\n",
    "        distribution = self.distribution_form_policy(state)\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "        log_probs = tfm.reduce_sum(log_probs, axis=-1, keepdims=True)\n",
    "        return log_probs\n",
    "\n",
    "    def act_deterministic(self, state):\n",
    "        actions_prime, _ = self._actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return self._act(actions_prime)\n",
    "\n",
    "    def act_stochastic(self, state):\n",
    "        actions_prime = self.sample_actions_form_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        return self._act(actions_prime)\n",
    "\n",
    "    def _act(self, actions):\n",
    "        observation_prime, reward, done, what, _ = self._environment.step(actions[0])\n",
    "        return actions, observation_prime, reward, done\n",
    "\n",
    "    # @tf.function\n",
    "    def learn(self, episode, episode_size):\n",
    "        self.train_step_actor(episode)\n",
    "        self.train_step_critic(episode, episode_size)\n",
    "\n",
    "    # @tf.function\n",
    "    def train_step_actor(self, episode):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0\n",
    "            for s, a, _, _, adv in episode:\n",
    "                prob_of_a = self.log_probs_form_policy(s, a)\n",
    "                loss -= prob_of_a * adv\n",
    "        gradients = tape.gradient(loss, self._policy_network.trainable_variables)\n",
    "        self._policy_network.optimizer.apply_gradients(zip(gradients, self._policy_network.trainable_variables))\n",
    "\n",
    "    # @tf.function\n",
    "    def train_step_critic(self, episode, episode_size):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0\n",
    "            for s, _, _, r_sum, _ in episode:\n",
    "                prev_v = self._value_network(s)\n",
    "                loss += self._mse(r_sum, prev_v)\n",
    "            loss = loss / episode_size\n",
    "        gradients = tape.gradient(loss, self._value_network.trainable_variables)\n",
    "        self._value_network.optimizer.apply_gradients(zip(gradients, self._value_network.trainable_variables))\n",
    "\n",
    "    def sample_to_episode_buffer(self):\n",
    "        buffer = EpisodeBuffer(self.estimate_advantage, self.calc_rewards_to_go)\n",
    "        s, _ = self._environment.reset()\n",
    "        d = 0\n",
    "        ret = 0\n",
    "        while not d:\n",
    "            a, s_p, r, d = self.act_stochastic(s)\n",
    "            ret += r\n",
    "            v = self._value_network(tf.convert_to_tensor([s], dtype=tf.float32))\n",
    "            buffer.add(s, a, r, v, d)\n",
    "            s = s_p\n",
    "        return buffer, ret\n",
    "\n",
    "    def train(self, epochs):\n",
    "        print(\"start training!\")\n",
    "        rets = []\n",
    "        for e in range(epochs):\n",
    "            print(\"epoch:\", e)\n",
    "            buffer, ret = self.sample_to_episode_buffer()\n",
    "            rets.append(ret)\n",
    "            print(\"return of episode:\", ret, \"avg 100:\", np.average(rets[-100:]))\n",
    "            episode = buffer.get_as_data_set()\n",
    "            self.learn(episode, buffer.size())\n",
    "        print(\"training finished!\")\n",
    "\n",
    "\n",
    "# from Agent import Agent\n",
    "# from GenericMLPs1D import create_policy_network, create_value_network\n",
    "import gym\n",
    "from functools import partial\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.keras.backend.clear_session()\n",
    "    env = gym.make('InvertedPendulum-v4')\n",
    "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space.shape[0], \"action_scaling:\",\n",
    "          env.action_space.high)\n",
    "    agent = Agent(environment=env,\n",
    "                  actor_network_generator=partial(create_policy_network, state_dim=env.observation_space.shape[0],\n",
    "                                                  action_dim=env.action_space.shape[0]),\n",
    "                  critic_network_generator=partial(create_value_network, state_dim=env.observation_space.shape))\n",
    "    agent.train(10000)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u3kl_SYnB-E_",
    "outputId": "e97f8c5d-c93e-4672-f466-13d246a6de63"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "state_dim= (4,) action_dim= 1 action_scaling: [3.]\n",
      "start training!\n",
      "epoch: 0\n",
      "return of episode: 6.0 avg 100: 6.0\n",
      "epoch: 1\n",
      "return of episode: 5.0 avg 100: 5.5\n",
      "epoch: 2\n",
      "return of episode: 6.0 avg 100: 5.666666666666667\n",
      "epoch: 3\n",
      "return of episode: 23.0 avg 100: 10.0\n",
      "epoch: 4\n",
      "return of episode: 8.0 avg 100: 9.6\n",
      "epoch: 5\n",
      "return of episode: 5.0 avg 100: 8.833333333333334\n",
      "epoch: 6\n",
      "return of episode: 10.0 avg 100: 9.0\n",
      "epoch: 7\n",
      "return of episode: 6.0 avg 100: 8.625\n",
      "epoch: 8\n",
      "return of episode: 9.0 avg 100: 8.666666666666666\n",
      "epoch: 9\n",
      "return of episode: 4.0 avg 100: 8.2\n",
      "epoch: 10\n",
      "return of episode: 30.0 avg 100: 10.181818181818182\n",
      "epoch: 11\n",
      "return of episode: 11.0 avg 100: 10.25\n",
      "epoch: 12\n",
      "return of episode: 7.0 avg 100: 10.0\n",
      "epoch: 13\n",
      "return of episode: 16.0 avg 100: 10.428571428571429\n",
      "epoch: 14\n",
      "return of episode: 6.0 avg 100: 10.133333333333333\n",
      "epoch: 15\n",
      "return of episode: 13.0 avg 100: 10.3125\n",
      "epoch: 16\n",
      "return of episode: 6.0 avg 100: 10.058823529411764\n",
      "epoch: 17\n",
      "return of episode: 18.0 avg 100: 10.5\n",
      "epoch: 18\n",
      "return of episode: 10.0 avg 100: 10.473684210526315\n",
      "epoch: 19\n",
      "return of episode: 13.0 avg 100: 10.6\n",
      "epoch: 20\n",
      "return of episode: 5.0 avg 100: 10.333333333333334\n",
      "epoch: 21\n",
      "return of episode: 14.0 avg 100: 10.5\n",
      "epoch: 22\n",
      "return of episode: 14.0 avg 100: 10.652173913043478\n",
      "epoch: 23\n",
      "return of episode: 6.0 avg 100: 10.458333333333334\n",
      "epoch: 24\n",
      "return of episode: 7.0 avg 100: 10.32\n",
      "epoch: 25\n",
      "return of episode: 11.0 avg 100: 10.346153846153847\n",
      "epoch: 26\n",
      "return of episode: 8.0 avg 100: 10.25925925925926\n",
      "epoch: 27\n",
      "return of episode: 6.0 avg 100: 10.107142857142858\n",
      "epoch: 28\n",
      "return of episode: 10.0 avg 100: 10.10344827586207\n",
      "epoch: 29\n",
      "return of episode: 10.0 avg 100: 10.1\n",
      "epoch: 30\n",
      "return of episode: 6.0 avg 100: 9.96774193548387\n",
      "epoch: 31\n",
      "return of episode: 6.0 avg 100: 9.84375\n",
      "epoch: 32\n",
      "return of episode: 18.0 avg 100: 10.090909090909092\n",
      "epoch: 33\n",
      "return of episode: 13.0 avg 100: 10.176470588235293\n",
      "epoch: 34\n",
      "return of episode: 34.0 avg 100: 10.857142857142858\n",
      "epoch: 35\n",
      "return of episode: 28.0 avg 100: 11.333333333333334\n",
      "epoch: 36\n",
      "return of episode: 13.0 avg 100: 11.378378378378379\n",
      "epoch: 37\n",
      "return of episode: 42.0 avg 100: 12.18421052631579\n",
      "epoch: 38\n",
      "return of episode: 8.0 avg 100: 12.076923076923077\n",
      "epoch: 39\n",
      "return of episode: 7.0 avg 100: 11.95\n",
      "epoch: 40\n",
      "return of episode: 9.0 avg 100: 11.878048780487806\n",
      "epoch: 41\n",
      "return of episode: 6.0 avg 100: 11.738095238095237\n",
      "epoch: 42\n",
      "return of episode: 18.0 avg 100: 11.883720930232558\n",
      "epoch: 43\n",
      "return of episode: 27.0 avg 100: 12.227272727272727\n",
      "epoch: 44\n",
      "return of episode: 8.0 avg 100: 12.133333333333333\n",
      "epoch: 45\n",
      "return of episode: 5.0 avg 100: 11.978260869565217\n",
      "epoch: 46\n",
      "return of episode: 25.0 avg 100: 12.25531914893617\n",
      "epoch: 47\n",
      "return of episode: 19.0 avg 100: 12.395833333333334\n",
      "epoch: 48\n",
      "return of episode: 18.0 avg 100: 12.510204081632653\n",
      "epoch: 49\n",
      "return of episode: 9.0 avg 100: 12.44\n",
      "epoch: 50\n",
      "return of episode: 7.0 avg 100: 12.333333333333334\n",
      "epoch: 51\n",
      "return of episode: 6.0 avg 100: 12.211538461538462\n",
      "epoch: 52\n",
      "return of episode: 9.0 avg 100: 12.150943396226415\n",
      "epoch: 53\n",
      "return of episode: 22.0 avg 100: 12.333333333333334\n",
      "epoch: 54\n",
      "return of episode: 11.0 avg 100: 12.309090909090909\n",
      "epoch: 55\n",
      "return of episode: 9.0 avg 100: 12.25\n",
      "epoch: 56\n",
      "return of episode: 13.0 avg 100: 12.263157894736842\n",
      "epoch: 57\n",
      "return of episode: 34.0 avg 100: 12.637931034482758\n",
      "epoch: 58\n",
      "return of episode: 15.0 avg 100: 12.677966101694915\n",
      "epoch: 59\n",
      "return of episode: 14.0 avg 100: 12.7\n",
      "epoch: 60\n",
      "return of episode: 15.0 avg 100: 12.737704918032787\n",
      "epoch: 61\n",
      "return of episode: 15.0 avg 100: 12.774193548387096\n",
      "epoch: 62\n",
      "return of episode: 10.0 avg 100: 12.73015873015873\n",
      "epoch: 63\n",
      "return of episode: 6.0 avg 100: 12.625\n",
      "epoch: 64\n",
      "return of episode: 29.0 avg 100: 12.876923076923077\n",
      "epoch: 65\n",
      "return of episode: 7.0 avg 100: 12.787878787878787\n",
      "epoch: 66\n",
      "return of episode: 32.0 avg 100: 13.074626865671641\n",
      "epoch: 67\n",
      "return of episode: 7.0 avg 100: 12.985294117647058\n",
      "epoch: 68\n",
      "return of episode: 25.0 avg 100: 13.159420289855072\n",
      "epoch: 69\n",
      "return of episode: 17.0 avg 100: 13.214285714285714\n",
      "epoch: 70\n",
      "return of episode: 41.0 avg 100: 13.605633802816902\n",
      "epoch: 71\n",
      "return of episode: 24.0 avg 100: 13.75\n",
      "epoch: 72\n",
      "return of episode: 14.0 avg 100: 13.753424657534246\n",
      "epoch: 73\n",
      "return of episode: 16.0 avg 100: 13.783783783783784\n",
      "epoch: 74\n",
      "return of episode: 12.0 avg 100: 13.76\n",
      "epoch: 75\n",
      "return of episode: 26.0 avg 100: 13.921052631578947\n",
      "epoch: 76\n",
      "return of episode: 13.0 avg 100: 13.909090909090908\n",
      "epoch: 77\n",
      "return of episode: 5.0 avg 100: 13.794871794871796\n",
      "epoch: 78\n",
      "return of episode: 33.0 avg 100: 14.037974683544304\n",
      "epoch: 79\n",
      "return of episode: 13.0 avg 100: 14.025\n",
      "epoch: 80\n",
      "return of episode: 15.0 avg 100: 14.037037037037036\n",
      "epoch: 81\n",
      "return of episode: 9.0 avg 100: 13.975609756097562\n",
      "epoch: 82\n",
      "return of episode: 45.0 avg 100: 14.349397590361447\n",
      "epoch: 83\n",
      "return of episode: 23.0 avg 100: 14.452380952380953\n",
      "epoch: 84\n",
      "return of episode: 12.0 avg 100: 14.423529411764706\n",
      "epoch: 85\n",
      "return of episode: 42.0 avg 100: 14.744186046511627\n",
      "epoch: 86\n",
      "return of episode: 44.0 avg 100: 15.080459770114942\n",
      "epoch: 87\n",
      "return of episode: 19.0 avg 100: 15.125\n",
      "epoch: 88\n",
      "return of episode: 40.0 avg 100: 15.404494382022472\n",
      "epoch: 89\n",
      "return of episode: 53.0 avg 100: 15.822222222222223\n",
      "epoch: 90\n",
      "return of episode: 60.0 avg 100: 16.307692307692307\n",
      "epoch: 91\n",
      "return of episode: 37.0 avg 100: 16.532608695652176\n",
      "epoch: 92\n",
      "return of episode: 30.0 avg 100: 16.677419354838708\n",
      "epoch: 93\n",
      "return of episode: 22.0 avg 100: 16.73404255319149\n",
      "epoch: 94\n",
      "return of episode: 51.0 avg 100: 17.094736842105263\n",
      "epoch: 95\n",
      "return of episode: 28.0 avg 100: 17.208333333333332\n",
      "epoch: 96\n",
      "return of episode: 40.0 avg 100: 17.443298969072163\n",
      "epoch: 97\n",
      "return of episode: 48.0 avg 100: 17.755102040816325\n",
      "epoch: 98\n",
      "return of episode: 44.0 avg 100: 18.02020202020202\n",
      "epoch: 99\n",
      "return of episode: 61.0 avg 100: 18.45\n",
      "epoch: 100\n",
      "return of episode: 21.0 avg 100: 18.6\n",
      "epoch: 101\n",
      "return of episode: 27.0 avg 100: 18.82\n",
      "epoch: 102\n",
      "return of episode: 91.0 avg 100: 19.67\n",
      "epoch: 103\n",
      "return of episode: 41.0 avg 100: 19.85\n",
      "epoch: 104\n",
      "return of episode: 58.0 avg 100: 20.35\n",
      "epoch: 105\n",
      "return of episode: 89.0 avg 100: 21.19\n",
      "epoch: 106\n",
      "return of episode: 62.0 avg 100: 21.71\n",
      "epoch: 107\n",
      "return of episode: 56.0 avg 100: 22.21\n",
      "epoch: 108\n",
      "return of episode: 47.0 avg 100: 22.59\n",
      "epoch: 109\n",
      "return of episode: 24.0 avg 100: 22.79\n",
      "epoch: 110\n",
      "return of episode: 38.0 avg 100: 22.87\n",
      "epoch: 111\n",
      "return of episode: 74.0 avg 100: 23.5\n",
      "epoch: 112\n",
      "return of episode: 49.0 avg 100: 23.92\n",
      "epoch: 113\n",
      "return of episode: 40.0 avg 100: 24.16\n",
      "epoch: 114\n",
      "return of episode: 63.0 avg 100: 24.73\n",
      "epoch: 115\n",
      "return of episode: 56.0 avg 100: 25.16\n",
      "epoch: 116\n",
      "return of episode: 49.0 avg 100: 25.59\n",
      "epoch: 117\n",
      "return of episode: 85.0 avg 100: 26.26\n",
      "epoch: 118\n",
      "return of episode: 96.0 avg 100: 27.12\n",
      "epoch: 119\n",
      "return of episode: 17.0 avg 100: 27.16\n",
      "epoch: 120\n",
      "return of episode: 46.0 avg 100: 27.57\n",
      "epoch: 121\n",
      "return of episode: 81.0 avg 100: 28.24\n",
      "epoch: 122\n",
      "return of episode: 49.0 avg 100: 28.59\n",
      "epoch: 123\n",
      "return of episode: 40.0 avg 100: 28.93\n",
      "epoch: 124\n",
      "return of episode: 55.0 avg 100: 29.41\n",
      "epoch: 125\n",
      "return of episode: 86.0 avg 100: 30.16\n",
      "epoch: 126\n",
      "return of episode: 45.0 avg 100: 30.53\n",
      "epoch: 127\n",
      "return of episode: 15.0 avg 100: 30.62\n",
      "epoch: 128\n",
      "return of episode: 30.0 avg 100: 30.82\n",
      "epoch: 129\n",
      "return of episode: 51.0 avg 100: 31.23\n",
      "epoch: 130\n",
      "return of episode: 49.0 avg 100: 31.66\n",
      "epoch: 131\n",
      "return of episode: 99.0 avg 100: 32.59\n",
      "epoch: 132\n",
      "return of episode: 60.0 avg 100: 33.01\n",
      "epoch: 133\n",
      "return of episode: 85.0 avg 100: 33.73\n",
      "epoch: 134\n",
      "return of episode: 47.0 avg 100: 33.86\n",
      "epoch: 135\n",
      "return of episode: 81.0 avg 100: 34.39\n",
      "epoch: 136\n",
      "return of episode: 81.0 avg 100: 35.07\n",
      "epoch: 137\n",
      "return of episode: 91.0 avg 100: 35.56\n",
      "epoch: 138\n",
      "return of episode: 51.0 avg 100: 35.99\n",
      "epoch: 139\n",
      "return of episode: 143.0 avg 100: 37.35\n",
      "epoch: 140\n",
      "return of episode: 158.0 avg 100: 38.84\n",
      "epoch: 141\n",
      "return of episode: 106.0 avg 100: 39.84\n",
      "epoch: 142\n",
      "return of episode: 79.0 avg 100: 40.45\n",
      "epoch: 143\n",
      "return of episode: 82.0 avg 100: 41.0\n",
      "epoch: 144\n",
      "return of episode: 81.0 avg 100: 41.73\n",
      "epoch: 145\n",
      "return of episode: 48.0 avg 100: 42.16\n",
      "epoch: 146\n",
      "return of episode: 45.0 avg 100: 42.36\n",
      "epoch: 147\n",
      "return of episode: 35.0 avg 100: 42.52\n",
      "epoch: 148\n",
      "return of episode: 40.0 avg 100: 42.74\n",
      "epoch: 149\n",
      "return of episode: 17.0 avg 100: 42.82\n",
      "epoch: 150\n",
      "return of episode: 26.0 avg 100: 43.01\n",
      "epoch: 151\n",
      "return of episode: 24.0 avg 100: 43.19\n",
      "epoch: 152\n",
      "return of episode: 23.0 avg 100: 43.33\n",
      "epoch: 153\n",
      "return of episode: 16.0 avg 100: 43.27\n",
      "epoch: 154\n",
      "return of episode: 20.0 avg 100: 43.36\n",
      "epoch: 155\n",
      "return of episode: 22.0 avg 100: 43.49\n",
      "epoch: 156\n",
      "return of episode: 18.0 avg 100: 43.54\n",
      "epoch: 157\n",
      "return of episode: 14.0 avg 100: 43.34\n",
      "epoch: 158\n",
      "return of episode: 21.0 avg 100: 43.4\n",
      "epoch: 159\n",
      "return of episode: 19.0 avg 100: 43.45\n",
      "epoch: 160\n",
      "return of episode: 14.0 avg 100: 43.44\n",
      "epoch: 161\n",
      "return of episode: 18.0 avg 100: 43.47\n",
      "epoch: 162\n",
      "return of episode: 17.0 avg 100: 43.54\n",
      "epoch: 163\n",
      "return of episode: 19.0 avg 100: 43.67\n",
      "epoch: 164\n",
      "return of episode: 16.0 avg 100: 43.54\n",
      "epoch: 165\n",
      "return of episode: 16.0 avg 100: 43.63\n",
      "epoch: 166\n",
      "return of episode: 16.0 avg 100: 43.47\n",
      "epoch: 167\n",
      "return of episode: 16.0 avg 100: 43.56\n",
      "epoch: 168\n",
      "return of episode: 17.0 avg 100: 43.48\n",
      "epoch: 169\n",
      "return of episode: 21.0 avg 100: 43.52\n",
      "epoch: 170\n",
      "return of episode: 21.0 avg 100: 43.32\n",
      "epoch: 171\n",
      "return of episode: 16.0 avg 100: 43.24\n",
      "epoch: 172\n",
      "return of episode: 19.0 avg 100: 43.29\n",
      "epoch: 173\n",
      "return of episode: 20.0 avg 100: 43.33\n",
      "epoch: 174\n",
      "return of episode: 13.0 avg 100: 43.34\n",
      "epoch: 175\n",
      "return of episode: 22.0 avg 100: 43.3\n",
      "epoch: 176\n",
      "return of episode: 16.0 avg 100: 43.33\n",
      "epoch: 177\n",
      "return of episode: 19.0 avg 100: 43.47\n",
      "epoch: 178\n",
      "return of episode: 20.0 avg 100: 43.34\n",
      "epoch: 179\n",
      "return of episode: 16.0 avg 100: 43.37\n",
      "epoch: 180\n",
      "return of episode: 19.0 avg 100: 43.41\n",
      "epoch: 181\n",
      "return of episode: 21.0 avg 100: 43.53\n",
      "epoch: 182\n",
      "return of episode: 16.0 avg 100: 43.24\n",
      "epoch: 183\n",
      "return of episode: 21.0 avg 100: 43.22\n",
      "epoch: 184\n",
      "return of episode: 17.0 avg 100: 43.27\n",
      "epoch: 185\n",
      "return of episode: 16.0 avg 100: 43.01\n",
      "epoch: 186\n",
      "return of episode: 28.0 avg 100: 42.85\n",
      "epoch: 187\n",
      "return of episode: 17.0 avg 100: 42.83\n",
      "epoch: 188\n",
      "return of episode: 21.0 avg 100: 42.64\n",
      "epoch: 189\n",
      "return of episode: 17.0 avg 100: 42.28\n",
      "epoch: 190\n",
      "return of episode: 22.0 avg 100: 41.9\n",
      "epoch: 191\n",
      "return of episode: 20.0 avg 100: 41.73\n",
      "epoch: 192\n",
      "return of episode: 19.0 avg 100: 41.62\n",
      "epoch: 193\n",
      "return of episode: 18.0 avg 100: 41.58\n",
      "epoch: 194\n",
      "return of episode: 17.0 avg 100: 41.24\n",
      "epoch: 195\n",
      "return of episode: 27.0 avg 100: 41.23\n",
      "epoch: 196\n",
      "return of episode: 21.0 avg 100: 41.04\n",
      "epoch: 197\n",
      "return of episode: 18.0 avg 100: 40.74\n",
      "epoch: 198\n",
      "return of episode: 17.0 avg 100: 40.47\n",
      "epoch: 199\n",
      "return of episode: 19.0 avg 100: 40.05\n",
      "epoch: 200\n",
      "return of episode: 22.0 avg 100: 40.06\n",
      "epoch: 201\n",
      "return of episode: 20.0 avg 100: 39.99\n",
      "epoch: 202\n",
      "return of episode: 20.0 avg 100: 39.28\n",
      "epoch: 203\n",
      "return of episode: 19.0 avg 100: 39.06\n",
      "epoch: 204\n",
      "return of episode: 18.0 avg 100: 38.66\n",
      "epoch: 205\n",
      "return of episode: 16.0 avg 100: 37.93\n",
      "epoch: 206\n",
      "return of episode: 17.0 avg 100: 37.48\n",
      "epoch: 207\n",
      "return of episode: 15.0 avg 100: 37.07\n",
      "epoch: 208\n",
      "return of episode: 21.0 avg 100: 36.81\n",
      "epoch: 209\n",
      "return of episode: 25.0 avg 100: 36.82\n",
      "epoch: 210\n",
      "return of episode: 22.0 avg 100: 36.66\n",
      "epoch: 211\n",
      "return of episode: 22.0 avg 100: 36.14\n",
      "epoch: 212\n",
      "return of episode: 20.0 avg 100: 35.85\n",
      "epoch: 213\n",
      "return of episode: 15.0 avg 100: 35.6\n",
      "epoch: 214\n",
      "return of episode: 17.0 avg 100: 35.14\n",
      "epoch: 215\n",
      "return of episode: 13.0 avg 100: 34.71\n",
      "epoch: 216\n",
      "return of episode: 17.0 avg 100: 34.39\n",
      "epoch: 217\n",
      "return of episode: 25.0 avg 100: 33.79\n",
      "epoch: 218\n",
      "return of episode: 24.0 avg 100: 33.07\n",
      "epoch: 219\n",
      "return of episode: 19.0 avg 100: 33.09\n",
      "epoch: 220\n",
      "return of episode: 19.0 avg 100: 32.82\n",
      "epoch: 221\n",
      "return of episode: 19.0 avg 100: 32.2\n",
      "epoch: 222\n",
      "return of episode: 23.0 avg 100: 31.94\n",
      "epoch: 223\n",
      "return of episode: 22.0 avg 100: 31.76\n",
      "epoch: 224\n",
      "return of episode: 26.0 avg 100: 31.47\n",
      "epoch: 225\n",
      "return of episode: 19.0 avg 100: 30.8\n",
      "epoch: 226\n",
      "return of episode: 16.0 avg 100: 30.51\n",
      "epoch: 227\n",
      "return of episode: 18.0 avg 100: 30.54\n",
      "epoch: 228\n",
      "return of episode: 17.0 avg 100: 30.41\n",
      "epoch: 229\n",
      "return of episode: 22.0 avg 100: 30.12\n",
      "epoch: 230\n",
      "return of episode: 23.0 avg 100: 29.86\n",
      "epoch: 231\n",
      "return of episode: 17.0 avg 100: 29.04\n",
      "epoch: 232\n",
      "return of episode: 21.0 avg 100: 28.65\n",
      "epoch: 233\n",
      "return of episode: 26.0 avg 100: 28.06\n",
      "epoch: 234\n",
      "return of episode: 20.0 avg 100: 27.79\n",
      "epoch: 235\n",
      "return of episode: 22.0 avg 100: 27.2\n",
      "epoch: 236\n",
      "return of episode: 18.0 avg 100: 26.57\n",
      "epoch: 237\n",
      "return of episode: 15.0 avg 100: 25.81\n",
      "epoch: 238\n",
      "return of episode: 17.0 avg 100: 25.47\n",
      "epoch: 239\n",
      "return of episode: 25.0 avg 100: 24.29\n",
      "epoch: 240\n",
      "return of episode: 21.0 avg 100: 22.92\n",
      "epoch: 241\n",
      "return of episode: 24.0 avg 100: 22.1\n",
      "epoch: 242\n",
      "return of episode: 20.0 avg 100: 21.51\n",
      "epoch: 243\n",
      "return of episode: 23.0 avg 100: 20.92\n",
      "epoch: 244\n",
      "return of episode: 21.0 avg 100: 20.32\n",
      "epoch: 245\n",
      "return of episode: 21.0 avg 100: 20.05\n",
      "epoch: 246\n",
      "return of episode: 19.0 avg 100: 19.79\n",
      "epoch: 247\n",
      "return of episode: 20.0 avg 100: 19.64\n",
      "epoch: 248\n",
      "return of episode: 18.0 avg 100: 19.42\n",
      "epoch: 249\n",
      "return of episode: 15.0 avg 100: 19.4\n",
      "epoch: 250\n",
      "return of episode: 20.0 avg 100: 19.34\n",
      "epoch: 251\n",
      "return of episode: 21.0 avg 100: 19.31\n",
      "epoch: 252\n",
      "return of episode: 16.0 avg 100: 19.24\n",
      "epoch: 253\n",
      "return of episode: 25.0 avg 100: 19.33\n",
      "epoch: 254\n",
      "return of episode: 21.0 avg 100: 19.34\n",
      "epoch: 255\n",
      "return of episode: 26.0 avg 100: 19.38\n",
      "epoch: 256\n",
      "return of episode: 21.0 avg 100: 19.41\n",
      "epoch: 257\n",
      "return of episode: 24.0 avg 100: 19.51\n",
      "epoch: 258\n",
      "return of episode: 25.0 avg 100: 19.55\n",
      "epoch: 259\n",
      "return of episode: 37.0 avg 100: 19.73\n",
      "epoch: 260\n",
      "return of episode: 26.0 avg 100: 19.85\n",
      "epoch: 261\n",
      "return of episode: 22.0 avg 100: 19.89\n",
      "epoch: 262\n",
      "return of episode: 24.0 avg 100: 19.96\n",
      "epoch: 263\n",
      "return of episode: 20.0 avg 100: 19.97\n",
      "epoch: 264\n",
      "return of episode: 25.0 avg 100: 20.06\n",
      "epoch: 265\n",
      "return of episode: 31.0 avg 100: 20.21\n",
      "epoch: 266\n",
      "return of episode: 20.0 avg 100: 20.25\n",
      "epoch: 267\n",
      "return of episode: 20.0 avg 100: 20.29\n",
      "epoch: 268\n",
      "return of episode: 34.0 avg 100: 20.46\n",
      "epoch: 269\n",
      "return of episode: 36.0 avg 100: 20.61\n",
      "epoch: 270\n",
      "return of episode: 29.0 avg 100: 20.69\n",
      "epoch: 271\n",
      "return of episode: 31.0 avg 100: 20.84\n",
      "epoch: 272\n",
      "return of episode: 21.0 avg 100: 20.86\n",
      "epoch: 273\n",
      "return of episode: 26.0 avg 100: 20.92\n",
      "epoch: 274\n",
      "return of episode: 27.0 avg 100: 21.06\n",
      "epoch: 275\n",
      "return of episode: 41.0 avg 100: 21.25\n",
      "epoch: 276\n",
      "return of episode: 29.0 avg 100: 21.38\n",
      "epoch: 277\n",
      "return of episode: 31.0 avg 100: 21.5\n",
      "epoch: 278\n",
      "return of episode: 24.0 avg 100: 21.54\n",
      "epoch: 279\n",
      "return of episode: 32.0 avg 100: 21.7\n",
      "epoch: 280\n",
      "return of episode: 31.0 avg 100: 21.82\n",
      "epoch: 281\n",
      "return of episode: 25.0 avg 100: 21.86\n",
      "epoch: 282\n",
      "return of episode: 31.0 avg 100: 22.01\n",
      "epoch: 283\n",
      "return of episode: 31.0 avg 100: 22.11\n",
      "epoch: 284\n",
      "return of episode: 27.0 avg 100: 22.21\n",
      "epoch: 285\n",
      "return of episode: 31.0 avg 100: 22.36\n",
      "epoch: 286\n",
      "return of episode: 42.0 avg 100: 22.5\n",
      "epoch: 287\n",
      "return of episode: 34.0 avg 100: 22.67\n",
      "epoch: 288\n",
      "return of episode: 27.0 avg 100: 22.73\n",
      "epoch: 289\n",
      "return of episode: 32.0 avg 100: 22.88\n",
      "epoch: 290\n",
      "return of episode: 28.0 avg 100: 22.94\n",
      "epoch: 291\n",
      "return of episode: 26.0 avg 100: 23.0\n",
      "epoch: 292\n",
      "return of episode: 28.0 avg 100: 23.09\n",
      "epoch: 293\n",
      "return of episode: 33.0 avg 100: 23.24\n",
      "epoch: 294\n",
      "return of episode: 26.0 avg 100: 23.33\n",
      "epoch: 295\n",
      "return of episode: 29.0 avg 100: 23.35\n",
      "epoch: 296\n",
      "return of episode: 39.0 avg 100: 23.53\n",
      "epoch: 297\n",
      "return of episode: 23.0 avg 100: 23.58\n",
      "epoch: 298\n",
      "return of episode: 34.0 avg 100: 23.75\n",
      "epoch: 299\n",
      "return of episode: 38.0 avg 100: 23.94\n",
      "epoch: 300\n",
      "return of episode: 45.0 avg 100: 24.17\n",
      "epoch: 301\n",
      "return of episode: 30.0 avg 100: 24.27\n",
      "epoch: 302\n",
      "return of episode: 29.0 avg 100: 24.36\n",
      "epoch: 303\n",
      "return of episode: 38.0 avg 100: 24.55\n",
      "epoch: 304\n",
      "return of episode: 24.0 avg 100: 24.61\n",
      "epoch: 305\n",
      "return of episode: 46.0 avg 100: 24.91\n",
      "epoch: 306\n",
      "return of episode: 34.0 avg 100: 25.08\n",
      "epoch: 307\n",
      "return of episode: 34.0 avg 100: 25.27\n",
      "epoch: 308\n",
      "return of episode: 42.0 avg 100: 25.48\n",
      "epoch: 309\n",
      "return of episode: 42.0 avg 100: 25.65\n",
      "epoch: 310\n",
      "return of episode: 40.0 avg 100: 25.83\n",
      "epoch: 311\n",
      "return of episode: 34.0 avg 100: 25.95\n",
      "epoch: 312\n",
      "return of episode: 34.0 avg 100: 26.09\n",
      "epoch: 313\n",
      "return of episode: 32.0 avg 100: 26.26\n",
      "epoch: 314\n",
      "return of episode: 51.0 avg 100: 26.6\n",
      "epoch: 315\n",
      "return of episode: 40.0 avg 100: 26.87\n",
      "epoch: 316\n",
      "return of episode: 69.0 avg 100: 27.39\n",
      "epoch: 317\n",
      "return of episode: 49.0 avg 100: 27.63\n",
      "epoch: 318\n",
      "return of episode: 47.0 avg 100: 27.86\n",
      "epoch: 319\n",
      "return of episode: 48.0 avg 100: 28.15\n",
      "epoch: 320\n",
      "return of episode: 61.0 avg 100: 28.57\n",
      "epoch: 321\n",
      "return of episode: 57.0 avg 100: 28.95\n",
      "epoch: 322\n",
      "return of episode: 111.0 avg 100: 29.83\n",
      "epoch: 323\n",
      "return of episode: 45.0 avg 100: 30.06\n",
      "epoch: 324\n",
      "return of episode: 74.0 avg 100: 30.54\n",
      "epoch: 325\n",
      "return of episode: 69.0 avg 100: 31.04\n",
      "epoch: 326\n",
      "return of episode: 53.0 avg 100: 31.41\n",
      "epoch: 327\n",
      "return of episode: 41.0 avg 100: 31.64\n",
      "epoch: 328\n",
      "return of episode: 45.0 avg 100: 31.92\n",
      "epoch: 329\n",
      "return of episode: 207.0 avg 100: 33.77\n",
      "epoch: 330\n",
      "return of episode: 87.0 avg 100: 34.41\n",
      "epoch: 331\n",
      "return of episode: 86.0 avg 100: 35.1\n",
      "epoch: 332\n",
      "return of episode: 57.0 avg 100: 35.46\n",
      "epoch: 333\n",
      "return of episode: 193.0 avg 100: 37.13\n",
      "epoch: 334\n",
      "return of episode: 83.0 avg 100: 37.76\n",
      "epoch: 335\n",
      "return of episode: 118.0 avg 100: 38.72\n",
      "epoch: 336\n",
      "return of episode: 40.0 avg 100: 38.94\n",
      "epoch: 337\n",
      "return of episode: 74.0 avg 100: 39.53\n",
      "epoch: 338\n",
      "return of episode: 53.0 avg 100: 39.89\n",
      "epoch: 339\n",
      "return of episode: 76.0 avg 100: 40.4\n",
      "epoch: 340\n",
      "return of episode: 73.0 avg 100: 40.92\n",
      "epoch: 341\n",
      "return of episode: 33.0 avg 100: 41.01\n",
      "epoch: 342\n",
      "return of episode: 38.0 avg 100: 41.19\n",
      "epoch: 343\n",
      "return of episode: 51.0 avg 100: 41.47\n",
      "epoch: 344\n",
      "return of episode: 127.0 avg 100: 42.53\n",
      "epoch: 345\n",
      "return of episode: 52.0 avg 100: 42.84\n",
      "epoch: 346\n",
      "return of episode: 47.0 avg 100: 43.12\n",
      "epoch: 347\n",
      "return of episode: 15.0 avg 100: 43.07\n",
      "epoch: 348\n",
      "return of episode: 45.0 avg 100: 43.34\n",
      "epoch: 349\n",
      "return of episode: 41.0 avg 100: 43.6\n",
      "epoch: 350\n",
      "return of episode: 22.0 avg 100: 43.62\n",
      "epoch: 351\n",
      "return of episode: 15.0 avg 100: 43.56\n",
      "epoch: 352\n",
      "return of episode: 53.0 avg 100: 43.93\n",
      "epoch: 353\n",
      "return of episode: 21.0 avg 100: 43.89\n",
      "epoch: 354\n",
      "return of episode: 44.0 avg 100: 44.12\n",
      "epoch: 355\n",
      "return of episode: 26.0 avg 100: 44.12\n",
      "epoch: 356\n",
      "return of episode: 18.0 avg 100: 44.09\n",
      "epoch: 357\n",
      "return of episode: 42.0 avg 100: 44.27\n",
      "epoch: 358\n",
      "return of episode: 48.0 avg 100: 44.5\n",
      "epoch: 359\n",
      "return of episode: 16.0 avg 100: 44.29\n",
      "epoch: 360\n",
      "return of episode: 47.0 avg 100: 44.5\n",
      "epoch: 361\n",
      "return of episode: 15.0 avg 100: 44.43\n",
      "epoch: 362\n",
      "return of episode: 29.0 avg 100: 44.48\n",
      "epoch: 363\n",
      "return of episode: 34.0 avg 100: 44.62\n",
      "epoch: 364\n",
      "return of episode: 19.0 avg 100: 44.56\n",
      "epoch: 365\n",
      "return of episode: 21.0 avg 100: 44.46\n",
      "epoch: 366\n",
      "return of episode: 52.0 avg 100: 44.78\n",
      "epoch: 367\n",
      "return of episode: 9.0 avg 100: 44.67\n",
      "epoch: 368\n",
      "return of episode: 17.0 avg 100: 44.5\n",
      "epoch: 369\n",
      "return of episode: 42.0 avg 100: 44.56\n",
      "epoch: 370\n",
      "return of episode: 37.0 avg 100: 44.64\n",
      "epoch: 371\n",
      "return of episode: 43.0 avg 100: 44.76\n",
      "epoch: 372\n",
      "return of episode: 56.0 avg 100: 45.11\n",
      "epoch: 373\n",
      "return of episode: 16.0 avg 100: 45.01\n",
      "epoch: 374\n",
      "return of episode: 47.0 avg 100: 45.21\n",
      "epoch: 375\n",
      "return of episode: 48.0 avg 100: 45.28\n",
      "epoch: 376\n",
      "return of episode: 18.0 avg 100: 45.17\n",
      "epoch: 377\n",
      "return of episode: 43.0 avg 100: 45.29\n",
      "epoch: 378\n",
      "return of episode: 45.0 avg 100: 45.5\n",
      "epoch: 379\n",
      "return of episode: 45.0 avg 100: 45.63\n",
      "epoch: 380\n",
      "return of episode: 41.0 avg 100: 45.73\n",
      "epoch: 381\n",
      "return of episode: 30.0 avg 100: 45.78\n",
      "epoch: 382\n",
      "return of episode: 11.0 avg 100: 45.58\n",
      "epoch: 383\n",
      "return of episode: 43.0 avg 100: 45.7\n",
      "epoch: 384\n",
      "return of episode: 46.0 avg 100: 45.89\n",
      "epoch: 385\n",
      "return of episode: 46.0 avg 100: 46.04\n",
      "epoch: 386\n",
      "return of episode: 45.0 avg 100: 46.07\n",
      "epoch: 387\n",
      "return of episode: 38.0 avg 100: 46.11\n",
      "epoch: 388\n",
      "return of episode: 33.0 avg 100: 46.17\n",
      "epoch: 389\n",
      "return of episode: 42.0 avg 100: 46.27\n",
      "epoch: 390\n",
      "return of episode: 39.0 avg 100: 46.38\n",
      "epoch: 391\n",
      "return of episode: 36.0 avg 100: 46.48\n",
      "epoch: 392\n",
      "return of episode: 28.0 avg 100: 46.48\n",
      "epoch: 393\n",
      "return of episode: 15.0 avg 100: 46.3\n",
      "epoch: 394\n",
      "return of episode: 48.0 avg 100: 46.52\n",
      "epoch: 395\n",
      "return of episode: 22.0 avg 100: 46.45\n",
      "epoch: 396\n",
      "return of episode: 13.0 avg 100: 46.19\n",
      "epoch: 397\n",
      "return of episode: 46.0 avg 100: 46.42\n",
      "epoch: 398\n",
      "return of episode: 48.0 avg 100: 46.56\n",
      "epoch: 399\n",
      "return of episode: 45.0 avg 100: 46.63\n",
      "epoch: 400\n",
      "return of episode: 43.0 avg 100: 46.61\n",
      "epoch: 401\n",
      "return of episode: 48.0 avg 100: 46.79\n",
      "epoch: 402\n",
      "return of episode: 40.0 avg 100: 46.9\n",
      "epoch: 403\n",
      "return of episode: 52.0 avg 100: 47.04\n",
      "epoch: 404\n",
      "return of episode: 56.0 avg 100: 47.36\n",
      "epoch: 405\n",
      "return of episode: 67.0 avg 100: 47.57\n",
      "epoch: 406\n",
      "return of episode: 50.0 avg 100: 47.73\n",
      "epoch: 407\n",
      "return of episode: 51.0 avg 100: 47.9\n",
      "epoch: 408\n",
      "return of episode: 49.0 avg 100: 47.97\n",
      "epoch: 409\n",
      "return of episode: 45.0 avg 100: 48.0\n",
      "epoch: 410\n",
      "return of episode: 65.0 avg 100: 48.25\n",
      "epoch: 411\n",
      "return of episode: 48.0 avg 100: 48.39\n",
      "epoch: 412\n",
      "return of episode: 42.0 avg 100: 48.47\n",
      "epoch: 413\n",
      "return of episode: 41.0 avg 100: 48.56\n",
      "epoch: 414\n",
      "return of episode: 61.0 avg 100: 48.66\n",
      "epoch: 415\n",
      "return of episode: 61.0 avg 100: 48.87\n",
      "epoch: 416\n",
      "return of episode: 46.0 avg 100: 48.64\n",
      "epoch: 417\n",
      "return of episode: 82.0 avg 100: 48.97\n",
      "epoch: 418\n",
      "return of episode: 45.0 avg 100: 48.95\n",
      "epoch: 419\n",
      "return of episode: 48.0 avg 100: 48.95\n",
      "epoch: 420\n",
      "return of episode: 53.0 avg 100: 48.87\n",
      "epoch: 421\n",
      "return of episode: 43.0 avg 100: 48.73\n",
      "epoch: 422\n",
      "return of episode: 57.0 avg 100: 48.19\n",
      "epoch: 423\n",
      "return of episode: 74.0 avg 100: 48.48\n",
      "epoch: 424\n",
      "return of episode: 61.0 avg 100: 48.35\n",
      "epoch: 425\n",
      "return of episode: 113.0 avg 100: 48.79\n",
      "epoch: 426\n",
      "return of episode: 48.0 avg 100: 48.74\n",
      "epoch: 427\n",
      "return of episode: 114.0 avg 100: 49.47\n",
      "epoch: 428\n",
      "return of episode: 65.0 avg 100: 49.67\n",
      "epoch: 429\n",
      "return of episode: 123.0 avg 100: 48.83\n",
      "epoch: 430\n",
      "return of episode: 81.0 avg 100: 48.77\n",
      "epoch: 431\n",
      "return of episode: 91.0 avg 100: 48.82\n",
      "epoch: 432\n",
      "return of episode: 42.0 avg 100: 48.67\n",
      "epoch: 433\n",
      "return of episode: 43.0 avg 100: 47.17\n",
      "epoch: 434\n",
      "return of episode: 73.0 avg 100: 47.07\n",
      "epoch: 435\n",
      "return of episode: 115.0 avg 100: 47.04\n",
      "epoch: 436\n",
      "return of episode: 50.0 avg 100: 47.14\n",
      "epoch: 437\n",
      "return of episode: 61.0 avg 100: 47.01\n",
      "epoch: 438\n",
      "return of episode: 37.0 avg 100: 46.85\n",
      "epoch: 439\n",
      "return of episode: 34.0 avg 100: 46.43\n",
      "epoch: 440\n",
      "return of episode: 198.0 avg 100: 47.68\n",
      "epoch: 441\n",
      "return of episode: 90.0 avg 100: 48.25\n",
      "epoch: 442\n",
      "return of episode: 94.0 avg 100: 48.81\n",
      "epoch: 443\n",
      "return of episode: 47.0 avg 100: 48.77\n",
      "epoch: 444\n",
      "return of episode: 52.0 avg 100: 48.02\n",
      "epoch: 445\n",
      "return of episode: 210.0 avg 100: 49.6\n",
      "epoch: 446\n",
      "return of episode: 118.0 avg 100: 50.31\n",
      "epoch: 447\n",
      "return of episode: 48.0 avg 100: 50.64\n",
      "epoch: 448\n",
      "return of episode: 39.0 avg 100: 50.58\n",
      "epoch: 449\n",
      "return of episode: 42.0 avg 100: 50.59\n",
      "epoch: 450\n",
      "return of episode: 45.0 avg 100: 50.82\n",
      "epoch: 451\n",
      "return of episode: 122.0 avg 100: 51.89\n",
      "epoch: 452\n",
      "return of episode: 82.0 avg 100: 52.18\n",
      "epoch: 453\n",
      "return of episode: 157.0 avg 100: 53.54\n",
      "epoch: 454\n",
      "return of episode: 55.0 avg 100: 53.65\n",
      "epoch: 455\n",
      "return of episode: 42.0 avg 100: 53.81\n",
      "epoch: 456\n",
      "return of episode: 119.0 avg 100: 54.82\n",
      "epoch: 457\n",
      "return of episode: 110.0 avg 100: 55.5\n",
      "epoch: 458\n",
      "return of episode: 103.0 avg 100: 56.05\n",
      "epoch: 459\n",
      "return of episode: 41.0 avg 100: 56.3\n",
      "epoch: 460\n",
      "return of episode: 114.0 avg 100: 56.97\n",
      "epoch: 461\n",
      "return of episode: 183.0 avg 100: 58.65\n",
      "epoch: 462\n",
      "return of episode: 38.0 avg 100: 58.74\n",
      "epoch: 463\n",
      "return of episode: 103.0 avg 100: 59.43\n",
      "epoch: 464\n",
      "return of episode: 63.0 avg 100: 59.87\n",
      "epoch: 465\n",
      "return of episode: 217.0 avg 100: 61.83\n",
      "epoch: 466\n",
      "return of episode: 78.0 avg 100: 62.09\n",
      "epoch: 467\n",
      "return of episode: 202.0 avg 100: 64.02\n",
      "epoch: 468\n",
      "return of episode: 255.0 avg 100: 66.4\n",
      "epoch: 469\n",
      "return of episode: 72.0 avg 100: 66.7\n",
      "epoch: 470\n",
      "return of episode: 82.0 avg 100: 67.15\n",
      "epoch: 471\n",
      "return of episode: 107.0 avg 100: 67.79\n",
      "epoch: 472\n",
      "return of episode: 109.0 avg 100: 68.32\n",
      "epoch: 473\n",
      "return of episode: 108.0 avg 100: 69.24\n",
      "epoch: 474\n",
      "return of episode: 140.0 avg 100: 70.17\n",
      "epoch: 475\n",
      "return of episode: 198.0 avg 100: 71.67\n",
      "epoch: 476\n",
      "return of episode: 114.0 avg 100: 72.63\n",
      "epoch: 477\n",
      "return of episode: 120.0 avg 100: 73.4\n",
      "epoch: 478\n",
      "return of episode: 42.0 avg 100: 73.37\n",
      "epoch: 479\n",
      "return of episode: 95.0 avg 100: 73.87\n",
      "epoch: 480\n",
      "return of episode: 51.0 avg 100: 73.97\n",
      "epoch: 481\n",
      "return of episode: 88.0 avg 100: 74.55\n",
      "epoch: 482\n",
      "return of episode: 177.0 avg 100: 76.21\n",
      "epoch: 483\n",
      "return of episode: 117.0 avg 100: 76.95\n",
      "epoch: 484\n",
      "return of episode: 125.0 avg 100: 77.74\n",
      "epoch: 485\n",
      "return of episode: 107.0 avg 100: 78.35\n",
      "epoch: 486\n",
      "return of episode: 90.0 avg 100: 78.8\n",
      "epoch: 487\n",
      "return of episode: 133.0 avg 100: 79.75\n",
      "epoch: 488\n",
      "return of episode: 81.0 avg 100: 80.23\n",
      "epoch: 489\n",
      "return of episode: 179.0 avg 100: 81.6\n",
      "epoch: 490\n",
      "return of episode: 182.0 avg 100: 83.03\n",
      "epoch: 491\n",
      "return of episode: 100.0 avg 100: 83.67\n",
      "epoch: 492\n",
      "return of episode: 131.0 avg 100: 84.7\n",
      "epoch: 493\n",
      "return of episode: 148.0 avg 100: 86.03\n",
      "epoch: 494\n",
      "return of episode: 110.0 avg 100: 86.65\n",
      "epoch: 495\n",
      "return of episode: 74.0 avg 100: 87.17\n",
      "epoch: 496\n",
      "return of episode: 108.0 avg 100: 88.12\n",
      "epoch: 497\n",
      "return of episode: 147.0 avg 100: 89.13\n",
      "epoch: 498\n",
      "return of episode: 144.0 avg 100: 90.09\n",
      "epoch: 499\n",
      "return of episode: 123.0 avg 100: 90.87\n",
      "epoch: 500\n",
      "return of episode: 91.0 avg 100: 91.35\n",
      "epoch: 501\n",
      "return of episode: 74.0 avg 100: 91.61\n",
      "epoch: 502\n",
      "return of episode: 124.0 avg 100: 92.45\n",
      "epoch: 503\n",
      "return of episode: 83.0 avg 100: 92.76\n",
      "epoch: 504\n",
      "return of episode: 146.0 avg 100: 93.66\n",
      "epoch: 505\n",
      "return of episode: 149.0 avg 100: 94.48\n",
      "epoch: 506\n",
      "return of episode: 37.0 avg 100: 94.35\n",
      "epoch: 507\n",
      "return of episode: 90.0 avg 100: 94.74\n",
      "epoch: 508\n",
      "return of episode: 150.0 avg 100: 95.75\n",
      "epoch: 509\n",
      "return of episode: 57.0 avg 100: 95.87\n",
      "epoch: 510\n",
      "return of episode: 84.0 avg 100: 96.06\n",
      "epoch: 511\n",
      "return of episode: 98.0 avg 100: 96.56\n",
      "epoch: 512\n",
      "return of episode: 82.0 avg 100: 96.96\n",
      "epoch: 513\n",
      "return of episode: 71.0 avg 100: 97.26\n",
      "epoch: 514\n",
      "return of episode: 43.0 avg 100: 97.08\n",
      "epoch: 515\n",
      "return of episode: 39.0 avg 100: 96.86\n",
      "epoch: 516\n",
      "return of episode: 35.0 avg 100: 96.75\n",
      "epoch: 517\n",
      "return of episode: 38.0 avg 100: 96.31\n",
      "epoch: 518\n",
      "return of episode: 46.0 avg 100: 96.32\n",
      "epoch: 519\n",
      "return of episode: 40.0 avg 100: 96.24\n",
      "epoch: 520\n",
      "return of episode: 12.0 avg 100: 95.83\n",
      "epoch: 521\n",
      "return of episode: 18.0 avg 100: 95.58\n",
      "epoch: 522\n",
      "return of episode: 18.0 avg 100: 95.19\n",
      "epoch: 523\n",
      "return of episode: 35.0 avg 100: 94.8\n",
      "epoch: 524\n",
      "return of episode: 42.0 avg 100: 94.61\n",
      "epoch: 525\n",
      "return of episode: 12.0 avg 100: 93.6\n",
      "epoch: 526\n",
      "return of episode: 43.0 avg 100: 93.55\n",
      "epoch: 527\n",
      "return of episode: 32.0 avg 100: 92.73\n",
      "epoch: 528\n",
      "return of episode: 34.0 avg 100: 92.42\n",
      "epoch: 529\n",
      "return of episode: 29.0 avg 100: 91.48\n",
      "epoch: 530\n",
      "return of episode: 33.0 avg 100: 91.0\n",
      "epoch: 531\n",
      "return of episode: 31.0 avg 100: 90.4\n",
      "epoch: 532\n",
      "return of episode: 36.0 avg 100: 90.34\n",
      "epoch: 533\n",
      "return of episode: 28.0 avg 100: 90.19\n",
      "epoch: 534\n",
      "return of episode: 32.0 avg 100: 89.78\n",
      "epoch: 535\n",
      "return of episode: 38.0 avg 100: 89.01\n",
      "epoch: 536\n",
      "return of episode: 72.0 avg 100: 89.23\n",
      "epoch: 537\n",
      "return of episode: 81.0 avg 100: 89.43\n",
      "epoch: 538\n",
      "return of episode: 86.0 avg 100: 89.92\n",
      "epoch: 539\n",
      "return of episode: 55.0 avg 100: 90.13\n",
      "epoch: 540\n",
      "return of episode: 54.0 avg 100: 88.69\n",
      "epoch: 541\n",
      "return of episode: 50.0 avg 100: 88.29\n",
      "epoch: 542\n",
      "return of episode: 56.0 avg 100: 87.91\n",
      "epoch: 543\n",
      "return of episode: 94.0 avg 100: 88.38\n",
      "epoch: 544\n",
      "return of episode: 64.0 avg 100: 88.5\n",
      "epoch: 545\n",
      "return of episode: 76.0 avg 100: 87.16\n",
      "epoch: 546\n",
      "return of episode: 33.0 avg 100: 86.31\n",
      "epoch: 547\n",
      "return of episode: 39.0 avg 100: 86.22\n",
      "epoch: 548\n",
      "return of episode: 7.0 avg 100: 85.9\n",
      "epoch: 549\n",
      "return of episode: 7.0 avg 100: 85.55\n",
      "epoch: 550\n",
      "return of episode: 5.0 avg 100: 85.15\n",
      "epoch: 551\n",
      "return of episode: 32.0 avg 100: 84.25\n",
      "epoch: 552\n",
      "return of episode: 8.0 avg 100: 83.51\n",
      "epoch: 553\n",
      "return of episode: 7.0 avg 100: 82.01\n",
      "epoch: 554\n",
      "return of episode: 7.0 avg 100: 81.53\n",
      "epoch: 555\n",
      "return of episode: 5.0 avg 100: 81.16\n",
      "epoch: 556\n",
      "return of episode: 18.0 avg 100: 80.15\n",
      "epoch: 557\n",
      "return of episode: 5.0 avg 100: 79.1\n",
      "epoch: 558\n",
      "return of episode: 3.0 avg 100: 78.1\n",
      "epoch: 559\n",
      "return of episode: 5.0 avg 100: 77.74\n",
      "epoch: 560\n",
      "return of episode: 5.0 avg 100: 76.65\n",
      "epoch: 561\n",
      "return of episode: 5.0 avg 100: 74.87\n",
      "epoch: 562\n",
      "return of episode: 6.0 avg 100: 74.55\n",
      "epoch: 563\n",
      "return of episode: 7.0 avg 100: 73.59\n",
      "epoch: 564\n",
      "return of episode: 6.0 avg 100: 73.02\n",
      "epoch: 565\n",
      "return of episode: 5.0 avg 100: 70.9\n",
      "epoch: 566\n",
      "return of episode: 5.0 avg 100: 70.17\n",
      "epoch: 567\n",
      "return of episode: 5.0 avg 100: 68.2\n",
      "epoch: 568\n",
      "return of episode: 6.0 avg 100: 65.71\n",
      "epoch: 569\n",
      "return of episode: 5.0 avg 100: 65.04\n",
      "epoch: 570\n",
      "return of episode: 6.0 avg 100: 64.28\n",
      "epoch: 571\n",
      "return of episode: 9.0 avg 100: 63.3\n",
      "epoch: 572\n",
      "return of episode: 7.0 avg 100: 62.28\n",
      "epoch: 573\n",
      "return of episode: 4.0 avg 100: 61.24\n",
      "epoch: 574\n",
      "return of episode: 5.0 avg 100: 59.89\n",
      "epoch: 575\n",
      "return of episode: 6.0 avg 100: 57.97\n",
      "epoch: 576\n",
      "return of episode: 4.0 avg 100: 56.87\n",
      "epoch: 577\n",
      "return of episode: 6.0 avg 100: 55.73\n",
      "epoch: 578\n",
      "return of episode: 42.0 avg 100: 55.73\n",
      "epoch: 579\n",
      "return of episode: 7.0 avg 100: 54.85\n",
      "epoch: 580\n",
      "return of episode: 5.0 avg 100: 54.39\n",
      "epoch: 581\n",
      "return of episode: 6.0 avg 100: 53.57\n",
      "epoch: 582\n",
      "return of episode: 20.0 avg 100: 52.0\n",
      "epoch: 583\n",
      "return of episode: 13.0 avg 100: 50.96\n",
      "epoch: 584\n",
      "return of episode: 7.0 avg 100: 49.78\n",
      "epoch: 585\n",
      "return of episode: 5.0 avg 100: 48.76\n",
      "epoch: 586\n",
      "return of episode: 13.0 avg 100: 47.99\n",
      "epoch: 587\n",
      "return of episode: 5.0 avg 100: 46.71\n",
      "epoch: 588\n",
      "return of episode: 9.0 avg 100: 45.99\n",
      "epoch: 589\n",
      "return of episode: 5.0 avg 100: 44.25\n",
      "epoch: 590\n",
      "return of episode: 37.0 avg 100: 42.8\n",
      "epoch: 591\n",
      "return of episode: 8.0 avg 100: 41.88\n",
      "epoch: 592\n",
      "return of episode: 41.0 avg 100: 40.98\n",
      "epoch: 593\n",
      "return of episode: 69.0 avg 100: 40.19\n",
      "epoch: 594\n",
      "return of episode: 43.0 avg 100: 39.52\n",
      "epoch: 595\n",
      "return of episode: 53.0 avg 100: 39.31\n",
      "epoch: 596\n",
      "return of episode: 55.0 avg 100: 38.78\n",
      "epoch: 597\n",
      "return of episode: 76.0 avg 100: 38.07\n",
      "epoch: 598\n",
      "return of episode: 48.0 avg 100: 37.11\n",
      "epoch: 599\n",
      "return of episode: 39.0 avg 100: 36.27\n",
      "epoch: 600\n",
      "return of episode: 48.0 avg 100: 35.84\n",
      "epoch: 601\n",
      "return of episode: 5.0 avg 100: 35.15\n",
      "epoch: 602\n",
      "return of episode: 60.0 avg 100: 34.51\n",
      "epoch: 603\n",
      "return of episode: 44.0 avg 100: 34.12\n",
      "epoch: 604\n",
      "return of episode: 39.0 avg 100: 33.05\n",
      "epoch: 605\n",
      "return of episode: 44.0 avg 100: 32.0\n",
      "epoch: 606\n",
      "return of episode: 54.0 avg 100: 32.17\n",
      "epoch: 607\n",
      "return of episode: 47.0 avg 100: 31.74\n",
      "epoch: 608\n",
      "return of episode: 66.0 avg 100: 30.9\n",
      "epoch: 609\n",
      "return of episode: 12.0 avg 100: 30.45\n",
      "epoch: 610\n",
      "return of episode: 81.0 avg 100: 30.42\n",
      "epoch: 611\n",
      "return of episode: 94.0 avg 100: 30.38\n",
      "epoch: 612\n",
      "return of episode: 81.0 avg 100: 30.37\n",
      "epoch: 613\n",
      "return of episode: 68.0 avg 100: 30.34\n",
      "epoch: 614\n",
      "return of episode: 59.0 avg 100: 30.5\n",
      "epoch: 615\n",
      "return of episode: 55.0 avg 100: 30.66\n",
      "epoch: 616\n",
      "return of episode: 63.0 avg 100: 30.94\n",
      "epoch: 617\n",
      "return of episode: 120.0 avg 100: 31.76\n",
      "epoch: 618\n",
      "return of episode: 88.0 avg 100: 32.18\n",
      "epoch: 619\n",
      "return of episode: 99.0 avg 100: 32.77\n",
      "epoch: 620\n",
      "return of episode: 126.0 avg 100: 33.91\n",
      "epoch: 621\n",
      "return of episode: 72.0 avg 100: 34.45\n",
      "epoch: 622\n",
      "return of episode: 125.0 avg 100: 35.52\n",
      "epoch: 623\n",
      "return of episode: 158.0 avg 100: 36.75\n",
      "epoch: 624\n",
      "return of episode: 124.0 avg 100: 37.57\n",
      "epoch: 625\n",
      "return of episode: 132.0 avg 100: 38.77\n",
      "epoch: 626\n",
      "return of episode: 120.0 avg 100: 39.54\n",
      "epoch: 627\n",
      "return of episode: 115.0 avg 100: 40.37\n",
      "epoch: 628\n",
      "return of episode: 92.0 avg 100: 40.95\n",
      "epoch: 629\n",
      "return of episode: 134.0 avg 100: 42.0\n",
      "epoch: 630\n",
      "return of episode: 109.0 avg 100: 42.76\n",
      "epoch: 631\n",
      "return of episode: 126.0 avg 100: 43.71\n",
      "epoch: 632\n",
      "return of episode: 109.0 avg 100: 44.44\n",
      "epoch: 633\n",
      "return of episode: 99.0 avg 100: 45.15\n",
      "epoch: 634\n",
      "return of episode: 105.0 avg 100: 45.88\n",
      "epoch: 635\n",
      "return of episode: 88.0 avg 100: 46.38\n",
      "epoch: 636\n",
      "return of episode: 81.0 avg 100: 46.47\n",
      "epoch: 637\n",
      "return of episode: 85.0 avg 100: 46.51\n",
      "epoch: 638\n",
      "return of episode: 81.0 avg 100: 46.46\n",
      "epoch: 639\n",
      "return of episode: 81.0 avg 100: 46.72\n",
      "epoch: 640\n",
      "return of episode: 73.0 avg 100: 46.91\n",
      "epoch: 641\n",
      "return of episode: 51.0 avg 100: 46.92\n",
      "epoch: 642\n",
      "return of episode: 63.0 avg 100: 46.99\n",
      "epoch: 643\n",
      "return of episode: 72.0 avg 100: 46.77\n",
      "epoch: 644\n",
      "return of episode: 78.0 avg 100: 46.91\n",
      "epoch: 645\n",
      "return of episode: 64.0 avg 100: 46.79\n",
      "epoch: 646\n",
      "return of episode: 40.0 avg 100: 46.86\n",
      "epoch: 647\n",
      "return of episode: 54.0 avg 100: 47.01\n",
      "epoch: 648\n",
      "return of episode: 68.0 avg 100: 47.62\n",
      "epoch: 649\n",
      "return of episode: 90.0 avg 100: 48.45\n",
      "epoch: 650\n",
      "return of episode: 45.0 avg 100: 48.85\n",
      "epoch: 651\n",
      "return of episode: 80.0 avg 100: 49.33\n",
      "epoch: 652\n",
      "return of episode: 8.0 avg 100: 49.33\n",
      "epoch: 653\n",
      "return of episode: 65.0 avg 100: 49.91\n",
      "epoch: 654\n",
      "return of episode: 60.0 avg 100: 50.44\n",
      "epoch: 655\n",
      "return of episode: 53.0 avg 100: 50.92\n",
      "epoch: 656\n",
      "return of episode: 46.0 avg 100: 51.2\n",
      "epoch: 657\n",
      "return of episode: 70.0 avg 100: 51.85\n",
      "epoch: 658\n",
      "return of episode: 8.0 avg 100: 51.9\n",
      "epoch: 659\n",
      "return of episode: 75.0 avg 100: 52.6\n",
      "epoch: 660\n",
      "return of episode: 56.0 avg 100: 53.11\n",
      "epoch: 661\n",
      "return of episode: 84.0 avg 100: 53.9\n",
      "epoch: 662\n",
      "return of episode: 60.0 avg 100: 54.44\n",
      "epoch: 663\n",
      "return of episode: 72.0 avg 100: 55.09\n",
      "epoch: 664\n",
      "return of episode: 82.0 avg 100: 55.85\n",
      "epoch: 665\n",
      "return of episode: 29.0 avg 100: 56.09\n",
      "epoch: 666\n",
      "return of episode: 120.0 avg 100: 57.24\n",
      "epoch: 667\n",
      "return of episode: 110.0 avg 100: 58.29\n",
      "epoch: 668\n",
      "return of episode: 51.0 avg 100: 58.74\n",
      "epoch: 669\n",
      "return of episode: 88.0 avg 100: 59.57\n",
      "epoch: 670\n",
      "return of episode: 130.0 avg 100: 60.81\n",
      "epoch: 671\n",
      "return of episode: 35.0 avg 100: 61.07\n",
      "epoch: 672\n",
      "return of episode: 61.0 avg 100: 61.61\n",
      "epoch: 673\n",
      "return of episode: 65.0 avg 100: 62.22\n",
      "epoch: 674\n",
      "return of episode: 82.0 avg 100: 62.99\n",
      "epoch: 675\n",
      "return of episode: 70.0 avg 100: 63.63\n",
      "epoch: 676\n",
      "return of episode: 84.0 avg 100: 64.43\n",
      "epoch: 677\n",
      "return of episode: 117.0 avg 100: 65.54\n",
      "epoch: 678\n",
      "return of episode: 124.0 avg 100: 66.36\n",
      "epoch: 679\n",
      "return of episode: 33.0 avg 100: 66.62\n",
      "epoch: 680\n",
      "return of episode: 104.0 avg 100: 67.61\n",
      "epoch: 681\n",
      "return of episode: 55.0 avg 100: 68.1\n",
      "epoch: 682\n",
      "return of episode: 54.0 avg 100: 68.44\n",
      "epoch: 683\n",
      "return of episode: 81.0 avg 100: 69.12\n",
      "epoch: 684\n",
      "return of episode: 45.0 avg 100: 69.5\n",
      "epoch: 685\n",
      "return of episode: 87.0 avg 100: 70.32\n",
      "epoch: 686\n",
      "return of episode: 93.0 avg 100: 71.12\n",
      "epoch: 687\n",
      "return of episode: 71.0 avg 100: 71.78\n",
      "epoch: 688\n",
      "return of episode: 106.0 avg 100: 72.75\n",
      "epoch: 689\n",
      "return of episode: 50.0 avg 100: 73.2\n",
      "epoch: 690\n",
      "return of episode: 84.0 avg 100: 73.67\n",
      "epoch: 691\n",
      "return of episode: 25.0 avg 100: 73.84\n",
      "epoch: 692\n",
      "return of episode: 172.0 avg 100: 75.15\n",
      "epoch: 693\n",
      "return of episode: 90.0 avg 100: 75.36\n",
      "epoch: 694\n",
      "return of episode: 46.0 avg 100: 75.39\n",
      "epoch: 695\n",
      "return of episode: 57.0 avg 100: 75.43\n",
      "epoch: 696\n",
      "return of episode: 89.0 avg 100: 75.77\n",
      "epoch: 697\n",
      "return of episode: 101.0 avg 100: 76.02\n",
      "epoch: 698\n",
      "return of episode: 34.0 avg 100: 75.88\n",
      "epoch: 699\n",
      "return of episode: 110.0 avg 100: 76.59\n",
      "epoch: 700\n",
      "return of episode: 133.0 avg 100: 77.44\n",
      "epoch: 701\n",
      "return of episode: 144.0 avg 100: 78.83\n",
      "epoch: 702\n",
      "return of episode: 124.0 avg 100: 79.47\n",
      "epoch: 703\n",
      "return of episode: 313.0 avg 100: 82.16\n",
      "epoch: 704\n",
      "return of episode: 271.0 avg 100: 84.48\n",
      "epoch: 705\n",
      "return of episode: 136.0 avg 100: 85.4\n",
      "epoch: 706\n",
      "return of episode: 83.0 avg 100: 85.69\n",
      "epoch: 707\n",
      "return of episode: 46.0 avg 100: 85.68\n",
      "epoch: 708\n",
      "return of episode: 45.0 avg 100: 85.47\n",
      "epoch: 709\n",
      "return of episode: 137.0 avg 100: 86.72\n",
      "epoch: 710\n",
      "return of episode: 215.0 avg 100: 88.06\n",
      "epoch: 711\n",
      "return of episode: 210.0 avg 100: 89.22\n",
      "epoch: 712\n",
      "return of episode: 202.0 avg 100: 90.43\n",
      "epoch: 713\n",
      "return of episode: 198.0 avg 100: 91.73\n",
      "epoch: 714\n",
      "return of episode: 182.0 avg 100: 92.96\n",
      "epoch: 715\n",
      "return of episode: 261.0 avg 100: 95.02\n",
      "epoch: 716\n",
      "return of episode: 174.0 avg 100: 96.13\n",
      "epoch: 717\n",
      "return of episode: 254.0 avg 100: 97.47\n",
      "epoch: 718\n",
      "return of episode: 190.0 avg 100: 98.49\n",
      "epoch: 719\n",
      "return of episode: 82.0 avg 100: 98.32\n",
      "epoch: 720\n",
      "return of episode: 190.0 avg 100: 98.96\n",
      "epoch: 721\n",
      "return of episode: 238.0 avg 100: 100.62\n",
      "epoch: 722\n",
      "return of episode: 177.0 avg 100: 101.14\n",
      "epoch: 723\n",
      "return of episode: 223.0 avg 100: 101.79\n",
      "epoch: 724\n",
      "return of episode: 243.0 avg 100: 102.98\n",
      "epoch: 725\n",
      "return of episode: 218.0 avg 100: 103.84\n",
      "epoch: 726\n",
      "return of episode: 259.0 avg 100: 105.23\n",
      "epoch: 727\n",
      "return of episode: 102.0 avg 100: 105.1\n",
      "epoch: 728\n",
      "return of episode: 280.0 avg 100: 106.98\n",
      "epoch: 729\n",
      "return of episode: 87.0 avg 100: 106.51\n",
      "epoch: 730\n",
      "return of episode: 376.0 avg 100: 109.18\n",
      "epoch: 731\n",
      "return of episode: 285.0 avg 100: 110.77\n",
      "epoch: 732\n",
      "return of episode: 249.0 avg 100: 112.17\n",
      "epoch: 733\n",
      "return of episode: 400.0 avg 100: 115.18\n",
      "epoch: 734\n",
      "return of episode: 306.0 avg 100: 117.19\n",
      "epoch: 735\n",
      "return of episode: 563.0 avg 100: 121.94\n",
      "epoch: 736\n",
      "return of episode: 261.0 avg 100: 123.74\n",
      "epoch: 737\n",
      "return of episode: 229.0 avg 100: 125.18\n",
      "epoch: 738\n",
      "return of episode: 572.0 avg 100: 130.09\n",
      "epoch: 739\n",
      "return of episode: 666.0 avg 100: 135.94\n",
      "epoch: 740\n",
      "return of episode: 650.0 avg 100: 141.71\n",
      "epoch: 741\n",
      "return of episode: 253.0 avg 100: 143.73\n",
      "epoch: 742\n",
      "return of episode: 209.0 avg 100: 145.19\n",
      "epoch: 743\n",
      "return of episode: 212.0 avg 100: 146.59\n",
      "epoch: 744\n",
      "return of episode: 321.0 avg 100: 149.02\n",
      "epoch: 745\n",
      "return of episode: 206.0 avg 100: 150.44\n",
      "epoch: 746\n",
      "return of episode: 272.0 avg 100: 152.76\n",
      "epoch: 747\n",
      "return of episode: 154.0 avg 100: 153.76\n",
      "epoch: 748\n",
      "return of episode: 500.0 avg 100: 158.08\n",
      "epoch: 749\n",
      "return of episode: 569.0 avg 100: 162.87\n",
      "epoch: 750\n",
      "return of episode: 359.0 avg 100: 166.01\n",
      "epoch: 751\n",
      "return of episode: 116.0 avg 100: 166.37\n",
      "epoch: 752\n",
      "return of episode: 292.0 avg 100: 169.21\n",
      "epoch: 753\n",
      "return of episode: 197.0 avg 100: 170.53\n",
      "epoch: 754\n",
      "return of episode: 556.0 avg 100: 175.49\n",
      "epoch: 755\n",
      "return of episode: 314.0 avg 100: 178.1\n",
      "epoch: 756\n",
      "return of episode: 355.0 avg 100: 181.19\n",
      "epoch: 757\n",
      "return of episode: 157.0 avg 100: 182.06\n",
      "epoch: 758\n",
      "return of episode: 182.0 avg 100: 183.8\n",
      "epoch: 759\n",
      "return of episode: 206.0 avg 100: 185.11\n",
      "epoch: 760\n",
      "return of episode: 254.0 avg 100: 187.09\n",
      "epoch: 761\n",
      "return of episode: 541.0 avg 100: 191.66\n",
      "epoch: 762\n",
      "return of episode: 172.0 avg 100: 192.78\n",
      "epoch: 763\n",
      "return of episode: 435.0 avg 100: 196.41\n",
      "epoch: 764\n",
      "return of episode: 151.0 avg 100: 197.1\n",
      "epoch: 765\n",
      "return of episode: 202.0 avg 100: 198.83\n",
      "epoch: 766\n",
      "return of episode: 139.0 avg 100: 199.02\n",
      "epoch: 767\n",
      "return of episode: 151.0 avg 100: 199.43\n",
      "epoch: 768\n",
      "return of episode: 144.0 avg 100: 200.36\n",
      "epoch: 769\n",
      "return of episode: 333.0 avg 100: 202.81\n",
      "epoch: 770\n",
      "return of episode: 205.0 avg 100: 203.56\n",
      "epoch: 771\n",
      "return of episode: 313.0 avg 100: 206.34\n",
      "epoch: 772\n",
      "return of episode: 231.0 avg 100: 208.04\n",
      "epoch: 773\n",
      "return of episode: 208.0 avg 100: 209.47\n",
      "epoch: 774\n",
      "return of episode: 314.0 avg 100: 211.79\n",
      "epoch: 775\n",
      "return of episode: 132.0 avg 100: 212.41\n",
      "epoch: 776\n",
      "return of episode: 365.0 avg 100: 215.22\n",
      "epoch: 777\n",
      "return of episode: 682.0 avg 100: 220.87\n",
      "epoch: 778\n",
      "return of episode: 296.0 avg 100: 222.59\n",
      "epoch: 779\n",
      "return of episode: 291.0 avg 100: 225.17\n",
      "epoch: 780\n",
      "return of episode: 245.0 avg 100: 226.58\n",
      "epoch: 781\n",
      "return of episode: 529.0 avg 100: 231.32\n",
      "epoch: 782\n",
      "return of episode: 289.0 avg 100: 233.67\n",
      "epoch: 783\n",
      "return of episode: 639.0 avg 100: 239.25\n",
      "epoch: 784\n",
      "return of episode: 1248.0 avg 100: 251.28\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-869a8a26a62c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    197\u001B[0m                                                   action_dim=env.action_space.shape[0]),\n\u001B[1;32m    198\u001B[0m                   critic_network_generator=partial(create_value_network, state_dim=env.observation_space.shape))\n\u001B[0;32m--> 199\u001B[0;31m     \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-27-869a8a26a62c>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m    178\u001B[0m             \u001B[0mrets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mret\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"return of episode:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"avg 100:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maverage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 180\u001B[0;31m             \u001B[0mepisode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_as_data_set\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    181\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepisode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"training finished!\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-27-869a8a26a62c>\u001B[0m in \u001B[0;36mget_as_data_set\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_as_data_set\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calc_rewards_to_go\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_r\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0madv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_advantage_estimator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_r\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_v\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_d\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_tensor_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_s\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_a\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_r\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-27-869a8a26a62c>\u001B[0m in \u001B[0;36mestimate_advantage\u001B[0;34m(self, rewards, values, dones)\u001B[0m\n\u001B[1;32m     85\u001B[0m             \u001B[0ma_t\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrewards\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m                 \u001B[0ma_t\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mdiscount\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mrewards\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gamma\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdones\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m                 \u001B[0mdiscount\u001B[0m \u001B[0;34m*=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gamma\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gae_lambda\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0madvantage\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ma_t\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mr_binary_op_wrapper\u001B[0;34m(y, x)\u001B[0m\n\u001B[1;32m   1439\u001B[0m       \u001B[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1440\u001B[0m       \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmaybe_promote_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforce_same_dtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1441\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1442\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1443\u001B[0m   \u001B[0;31m# Propagate func.__doc__ to the wrappers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36m_mul_dispatch\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   1764\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msparse_tensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSparseTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew_vals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1765\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1766\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mmultiply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1767\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1768\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mop_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1080\u001B[0m       \u001B[0;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1081\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1082\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1083\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1084\u001B[0m         \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mmultiply\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m    527\u001B[0m   \"\"\"\n\u001B[1;32m    528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 529\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    530\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mmul\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   6575\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6576\u001B[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0;32m-> 6577\u001B[0;31m         _ctx, \"Mul\", name, x, y)\n\u001B[0m\u001B[1;32m   6578\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6579\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ]
}
