{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoOEob3gB00H",
    "outputId": "0d092d2a-0cd3-4cc7-813e-d50e41751aed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: gym 0.25.2\n",
      "Uninstalling gym-0.25.2:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.10/dist-packages/gym-0.25.2.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/gym/*\n",
      "Proceed (Y/n)? y\n",
      "  Successfully uninstalled gym-0.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gym[mujoco]\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m721.7/721.7 kB\u001B[0m \u001B[31m14.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (0.0.8)\n",
      "Collecting mujoco==2.2 (from gym[mujoco])\n",
      "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m53.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[mujoco]) (2.25.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2->gym[mujoco]) (1.4.0)\n",
      "Collecting glfw (from mujoco==2.2->gym[mujoco])\n",
      "  Downloading glfw-2.5.9-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.8/207.8 kB\u001B[0m \u001B[31m22.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2->gym[mujoco]) (3.1.6)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[mujoco]) (8.4.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827633 sha256=2c98f84c3f60f45e037d3b13e1da1860f890e8f0061d49a574f454013c0ba12a\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: glfw, mujoco, gym\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed glfw-2.5.9 gym-0.26.2 mujoco-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym[mujoco]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9hF7N706aXK",
    "outputId": "f7d6c171-b059-47c2-f2a1-d8db81e4777f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def create_policy_network(state_dim, action_dim):\n",
    "    inputs = keras.Input(shape=state_dim)\n",
    "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    mu = Dense(action_dim, activation=None)(x)\n",
    "    sigma = Dense(action_dim, activation=tf.nn.softplus)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=(mu, sigma))\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_value_network(state_dim):\n",
    "    inputs = keras.Input(shape=state_dim)\n",
    "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    x = Dense(256, activation=tf.nn.relu)(x)\n",
    "    out = Dense(1, activation=None)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, batch_size, shuffle_buffer_size=1024):\n",
    "        self._episodes = []\n",
    "        self._batch_size = batch_size\n",
    "        self._shuffle_buffer_size = shuffle_buffer_size\n",
    "\n",
    "    def add_episodes(self, episodes):\n",
    "        self._episodes.append(episodes)\n",
    "\n",
    "    def get_as_dataset_of_length(self, dataset_size=10000):\n",
    "        choice_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.squeeze(tf.random.categorical([[float(x) for x in range(len(self._episodes))]], dataset_size)))\n",
    "        episodes = [ds.shuffle(self._shuffle_buffer_size).repeat() for ds in self._episodes]\n",
    "        return tf.data.Dataset.choose_from_datasets(episodes, choice_dataset).batch(self._batch_size)\n",
    "\n",
    "    def get_as_dataset(self):\n",
    "        ds = self._episodes[0]\n",
    "        for e in self._episodes[1:len(self._episodes)]:\n",
    "            ds = ds.concatenate(e)\n",
    "        return ds.shuffle(self._shuffle_buffer_size).batch(self._batch_size)\n",
    "\n",
    "    def get_as_dataset_repeated(self, count=4):\n",
    "        return self.get_as_dataset().repeat(count)\n",
    "\n",
    "    def clear(self):\n",
    "        self._episodes.clear()\n",
    "\n",
    "\n",
    "# from ReplayBuffer import ReplayBuffer\n",
    "# from Policy import Policy\n",
    "# from RollOutWorker import RollOutWorker\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, environments, actor_network_generator, critic_network_generator, updates_per_episode=80,\n",
    "                 epsilon=0.2, gae_lambda=0.95, learning_rate=0.0003, gamma=0.99, alpha=0.2, kld_threshold=0.05,\n",
    "                 normalize_adv=False):\n",
    "        self._updates_per_epoch = updates_per_episode\n",
    "        self._epsilon = epsilon\n",
    "        self._gae_lambda = gae_lambda\n",
    "        self._gamma = gamma\n",
    "        self._alpha = alpha\n",
    "        self._learning_rate = learning_rate\n",
    "        self._mse = tf.keras.losses.MeanSquaredError()\n",
    "        self._policy_network = actor_network_generator()\n",
    "        self._value_network = critic_network_generator()\n",
    "        self._optimizer_policy = Adam(learning_rate=learning_rate)  # TODO: one optimizer for both?\n",
    "        self._optimizer_value = Adam(learning_rate=learning_rate)\n",
    "        self._kld_threshold = kld_threshold\n",
    "        self._normalize_adv = normalize_adv\n",
    "        self._policy = Policy(self._policy_network)\n",
    "        # option for multiple workers for future parallelization\n",
    "        if not isinstance(environments, Iterable):\n",
    "            environments = [environments]\n",
    "        self._workers = [RollOutWorker(self._policy, self._value_network, env, self._gamma, self._gae_lambda)\n",
    "                         for env in environments]\n",
    "\n",
    "    @tf.function\n",
    "    def learn(self, data_set):\n",
    "        kld, actor_loss, critic_loss = 0.0, 0.0, 0.0\n",
    "        kld_next, actor_loss_next = 0.0, 0.0\n",
    "        i = 0.0\n",
    "        for s, a, ret, adv, prob_old_policy in data_set:\n",
    "            early_stopping, kld_next, actor_loss_next = self.train_step_actor(s, a, adv, prob_old_policy)\n",
    "            if early_stopping:\n",
    "                break\n",
    "            kld += kld_next\n",
    "            actor_loss += actor_loss_next\n",
    "            critic_loss += self.train_step_critic(s, ret)\n",
    "            i += 1\n",
    "        return kld / i, actor_loss / i, critic_loss / i, i, kld_next\n",
    "\n",
    "    # Alternative that does not terminate the training of the value network if KLD is too high\n",
    "    @tf.function\n",
    "    def learn2(self, data_set):\n",
    "        kld, actor_loss, critic_loss = 0.0, 0.0, 0.0\n",
    "        kld_next, actor_loss_next = 0.0, 0.0\n",
    "        i = 0.0\n",
    "        j = 0.0\n",
    "        for s, a, _, adv, prob_old_policy in data_set:\n",
    "            early_stopping, kld_next, actor_loss_next = self.train_step_actor(s, a, adv, prob_old_policy)\n",
    "            if early_stopping:\n",
    "                break\n",
    "            kld += kld_next\n",
    "            actor_loss += actor_loss_next\n",
    "            i += 1\n",
    "        for s, _, ret, _, _ in data_set:\n",
    "            critic_loss += self.train_step_critic(s, ret)\n",
    "            j += 1\n",
    "        return kld / i, actor_loss / i, critic_loss / j, i, kld_next\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_actor(self, s, a, adv, prob_old_policy):\n",
    "        early_stopping = False\n",
    "        loss = 0.0\n",
    "        if self._normalize_adv:\n",
    "            adv = (adv - tfm.reduce_mean(adv)) / (tfm.reduce_std(adv) + 1e-8)\n",
    "        with tf.GradientTape() as tape:\n",
    "            distribution = self._policy.distribution_from_policy(s)\n",
    "            prob_current_policy = self._policy.log_probs_from_distribution(distribution, a)\n",
    "            log_ratio = prob_current_policy - prob_old_policy\n",
    "            kld = tf.math.reduce_mean((tf.math.exp(log_ratio) - 1) - log_ratio)\n",
    "            if kld > self._kld_threshold:  # early stoppling if KLD is too high\n",
    "                early_stopping = True\n",
    "            else:\n",
    "                # prob of current policy / prob of old policy (log probs: p/p2 = log(p)-log(p2)\n",
    "                p = tf.math.exp(prob_current_policy - prob_old_policy)  # exp() to un do log(p)\n",
    "                clipped_p = tf.clip_by_value(p, 1 - self._epsilon, 1 + self._epsilon)\n",
    "                policy_loss = -tfm.reduce_mean(tfm.minimum(p * adv, clipped_p * adv))\n",
    "                # entropy_loss = -tfm.reduce_mean(-prob_current_policy)  # approximate entropy\n",
    "                entropy_loss = -tfm.reduce_mean(distribution.entropy())\n",
    "                loss = policy_loss + self._alpha * entropy_loss\n",
    "\n",
    "                gradients = tape.gradient(loss, self._policy_network.trainable_variables)\n",
    "                self._optimizer_policy.apply_gradients(zip(gradients, self._policy_network.trainable_variables))\n",
    "        return early_stopping, kld, loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_critic(self, s, r_sum):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prev_v = self._value_network(s)\n",
    "            loss = self._mse(r_sum, prev_v)\n",
    "        gradients = tape.gradient(loss, self._value_network.trainable_variables)\n",
    "        self._optimizer_value.apply_gradients(zip(gradients, self._value_network.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def train(self, epochs, batch_size=64, sub_epochs=4, steps_per_trajectory=1024):\n",
    "        print(\"start training!\")\n",
    "        rets = []\n",
    "        replay_buffer = ReplayBuffer(batch_size)\n",
    "        for e in range(epochs):\n",
    "            trajectories = [worker.sample_trajectories(steps_per_trajectory) for worker in\n",
    "                            self._workers]\n",
    "            ac_ret = 0.0\n",
    "            ac_dones = 0\n",
    "            for episodes, ret, dones in trajectories:\n",
    "                replay_buffer.add_episodes(episodes)\n",
    "                ac_ret += ret\n",
    "                ac_dones += dones\n",
    "            ac_ret = ac_ret / len(self._workers)\n",
    "            rets.append(ac_ret)\n",
    "            print(\"epoch:\", e, \"return of episode:\", ac_ret, \"avg 10:\", np.average(rets[-10:]), \"dones:\", ac_dones)\n",
    "            kld, actor_loss, critic_loss, i, last_kld = self.learn(replay_buffer.get_as_dataset_repeated(sub_epochs))\n",
    "            print(\n",
    "                f\"kld: {kld}, actor_loss: {actor_loss}, critic_loss: {critic_loss}, updates: {i}, last_kld: {last_kld}\")\n",
    "            replay_buffer.clear()\n",
    "        print(\"training finished!\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, policy_network):\n",
    "        self._policy_network = policy_network\n",
    "\n",
    "    def distribution_from_policy(self, state):\n",
    "        mu, sigma = self._policy_network(state)\n",
    "        return tfd.Normal(mu, sigma)\n",
    "\n",
    "    def sample_actions_from_policy(self, state):\n",
    "        distribution = self.distribution_from_policy(state)\n",
    "        actions = distribution.sample()\n",
    "        log_probs = self.log_probs_from_distribution(distribution, actions)\n",
    "        return actions, log_probs\n",
    "\n",
    "    def log_probs_from_distribution(self, distribution, actions):\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "        return tfm.reduce_sum(log_probs, axis=-1, keepdims=True)\n",
    "\n",
    "    def act_stochastic(self, state, environment):\n",
    "        actions_prime, log_probs = self.sample_actions_from_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        observation_prime, reward, terminated, truncated, _ = environment.step(actions_prime[0])\n",
    "        return actions_prime, observation_prime, reward, terminated or truncated, log_probs\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RollOutWorker:\n",
    "\n",
    "    def __init__(self, policy, value_network, environment, gamma, gae_lambda):\n",
    "        self._policy = policy\n",
    "        self._value_network = value_network\n",
    "        self._environment = environment\n",
    "        self._gamma = gamma\n",
    "        self._gae_lambda = gae_lambda\n",
    "        self._s = []\n",
    "        self._a = []\n",
    "        self._r = []\n",
    "        self._v = []\n",
    "        self._p = []\n",
    "        self._d = []\n",
    "\n",
    "        self.__s, _ = self._environment.reset()\n",
    "        self.__d = 0\n",
    "        self.__d_p = 0.0\n",
    "        self.__s_p = None\n",
    "        self.__ret = 0\n",
    "\n",
    "    def add(self, s, a, r, v, p, d):\n",
    "        self._s.append(tf.convert_to_tensor(s, dtype=tf.float32))\n",
    "        self._a.append(tf.convert_to_tensor(a, dtype=tf.float32))\n",
    "        self._r.append(tf.convert_to_tensor(r, dtype=tf.float32))\n",
    "        self._v.append(tf.convert_to_tensor(v, dtype=tf.float32))\n",
    "        self._p.append(tf.convert_to_tensor(p, dtype=tf.float32))\n",
    "        self._d.append(tf.convert_to_tensor(d, dtype=tf.float32))\n",
    "\n",
    "    def clear(self):\n",
    "        self._s.clear()\n",
    "        self._a.clear()\n",
    "        self._r.clear()\n",
    "        self._v.clear()\n",
    "        self._p.clear()\n",
    "        self._d.clear()\n",
    "\n",
    "    # generalized advantage estimate (taken from https://ppo-details.cleanrl.dev//2021/11/05/ppo-implementation-details/)\n",
    "    def estimate_advantage(self, rewards, values, dones, next_done, next_value):  # TODO: rework\n",
    "        adv = np.zeros_like(rewards)\n",
    "        last_gae_lamda = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_non_terminal = 1.0 - next_done\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[t + 1]\n",
    "                next_values = values[t + 1]\n",
    "            delta = rewards[t] + self._gamma * next_values * next_non_terminal - values[t]\n",
    "            adv[t] = last_gae_lamda = delta + self._gamma * self._gae_lambda * next_non_terminal * last_gae_lamda\n",
    "        return adv\n",
    "\n",
    "    # TODO: Make Vectorized!!!\n",
    "    def sample_trajectories(self, steps_per_trajectory):\n",
    "        ack_ret = 0\n",
    "        x = 0\n",
    "        for _ in range(steps_per_trajectory):\n",
    "            a, self.__s_p, r, self.__d_p, p = self._policy.act_stochastic(self.__s, self._environment)\n",
    "            self.__d_p = float(self.__d_p)\n",
    "            self.__ret += r\n",
    "            v = self._value_network(tf.convert_to_tensor([self.__s], dtype=tf.float32))\n",
    "            self.add(self.__s, tf.squeeze(a, 1), [r], tf.squeeze(v, 1), tf.squeeze(p, 1), self.__d)\n",
    "            if self.__d_p:\n",
    "                x += 1\n",
    "                ack_ret += self.__ret\n",
    "                self.__ret = 0\n",
    "                self.__s, _ = self._environment.reset()\n",
    "            self.__s = self.__s_p\n",
    "            self.__d = self.__d_p\n",
    "        v_p = self._value_network(tf.convert_to_tensor([self.__s_p], dtype=tf.float32))\n",
    "        adv = self.estimate_advantage(self._r, self._v, self._d, next_done=self.__d_p, next_value=v_p)\n",
    "        g = adv + np.asarray(self._v)  # TD(lambda)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(self._s), tf.convert_to_tensor(self._a),\n",
    "                                                 tf.convert_to_tensor(g), tf.convert_to_tensor(adv),\n",
    "                                                 tf.convert_to_tensor(self._p)))\n",
    "        self.clear()\n",
    "        return ds, self.__ret if x < 1 else ack_ret / x, x\n",
    "\n",
    "\n",
    "# from Agent import Agent\n",
    "# from GenericMLPs1D import create_policy_network, create_value_network\n",
    "import gym\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.keras.backend.clear_session()\n",
    "    env = gym.make('InvertedPendulum-v4')\n",
    "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space.shape[0], \"action_scaling:\",\n",
    "          env.action_space.high)\n",
    "    agent = Agent(environments=env,\n",
    "                  actor_network_generator=partial(create_policy_network, state_dim=env.observation_space.shape[0],\n",
    "                                                  action_dim=env.action_space.shape[0]),\n",
    "                  critic_network_generator=partial(create_value_network, state_dim=env.observation_space.shape))\n",
    "\n",
    "    agent.train(epochs=1000, batch_size=64, sub_epochs=4, steps_per_trajectory=640)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3kl_SYnB-E_",
    "outputId": "d7b17819-6c68-4c04-abc5-3b172f64df6d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "state_dim= (4,) action_dim= 1 action_scaling: [3.]\n",
      "start training!\n",
      "epoch: 0 return of episode: 9.696969696969697 avg 10: 9.696969696969697 dones: 66\n",
      "kld: 0.0056070988066494465, actor_loss: -5.286477565765381, critic_loss: 26.44529151916504, updates: 40.0, last_kld: 0.002829697448760271\n",
      "epoch: 1 return of episode: 10.258064516129032 avg 10: 9.977517106549364 dones: 62\n",
      "kld: 0.008141379803419113, actor_loss: -3.0000336170196533, critic_loss: 22.622722625732422, updates: 40.0, last_kld: 0.003732385579496622\n",
      "epoch: 2 return of episode: 13.040816326530612 avg 10: 10.998616846543113 dones: 49\n",
      "kld: 0.006017050705850124, actor_loss: -3.509716033935547, critic_loss: 36.59486389160156, updates: 10.0, last_kld: 0.06666044890880585\n",
      "epoch: 3 return of episode: 14.906976744186046 avg 10: 11.975706820953846 dones: 43\n",
      "kld: 0.014186044223606586, actor_loss: -2.4896230697631836, critic_loss: 33.40525436401367, updates: 27.0, last_kld: 0.18440169095993042\n",
      "epoch: 4 return of episode: 20.419354838709676 avg 10: 13.664436424505013 dones: 31\n",
      "kld: 0.01258829515427351, actor_loss: -3.6450929641723633, critic_loss: 49.55583572387695, updates: 17.0, last_kld: 0.05203568935394287\n",
      "epoch: 5 return of episode: 26.26086956521739 avg 10: 15.763841947957076 dones: 23\n",
      "kld: 0.013460192829370499, actor_loss: -3.303027868270874, critic_loss: 67.40312957763672, updates: 13.0, last_kld: 0.06615927070379257\n",
      "epoch: 6 return of episode: 45.714285714285715 avg 10: 20.04247677171831 dones: 14\n",
      "kld: 0.014757842756807804, actor_loss: -4.888184547424316, critic_loss: 81.65849304199219, updates: 11.0, last_kld: 0.0699571818113327\n",
      "epoch: 7 return of episode: 61.09090909090909 avg 10: 25.17353081161716 dones: 11\n",
      "kld: 0.012589110061526299, actor_loss: -3.3832130432128906, critic_loss: 102.76154327392578, updates: 7.0, last_kld: 0.06538241356611252\n",
      "epoch: 8 return of episode: 70.0 avg 10: 30.15424961032636 dones: 9\n",
      "kld: 0.006699923425912857, actor_loss: -2.646758794784546, critic_loss: 106.19537353515625, updates: 6.0, last_kld: 0.0583450011909008\n",
      "epoch: 9 return of episode: 51.416666666666664 avg 10: 32.28049131596039 dones: 12\n",
      "kld: 0.017472246661782265, actor_loss: 0.4076269865036011, critic_loss: 139.39144897460938, updates: 9.0, last_kld: 0.3461049795150757\n",
      "epoch: 10 return of episode: 73.66666666666667 avg 10: 38.677461012930095 dones: 9\n",
      "kld: 0.016063284128904343, actor_loss: -2.3634049892425537, critic_loss: 83.71874237060547, updates: 18.0, last_kld: 0.05189551040530205\n",
      "epoch: 11 return of episode: 93.57142857142857 avg 10: 47.00879741846005 dones: 7\n",
      "kld: 0.011802410706877708, actor_loss: -3.2753353118896484, critic_loss: 109.9263916015625, updates: 4.0, last_kld: 0.06000314652919769\n",
      "epoch: 12 return of episode: 111.6 avg 10: 56.86471578580698 dones: 5\n",
      "kld: 0.007233952637761831, actor_loss: -4.53323221206665, critic_loss: 127.8988037109375, updates: 5.0, last_kld: 0.05332942306995392\n",
      "epoch: 13 return of episode: 87.125 avg 10: 64.08651811138839 dones: 8\n",
      "kld: 0.014500603079795837, actor_loss: 0.6121855974197388, critic_loss: 173.75498962402344, updates: 8.0, last_kld: 0.08854223787784576\n",
      "epoch: 14 return of episode: 103.33333333333333 avg 10: 72.37791596085074 dones: 6\n",
      "kld: 0.009882602840662003, actor_loss: -1.341079592704773, critic_loss: 115.62024688720703, updates: 6.0, last_kld: 0.1709441840648651\n",
      "epoch: 15 return of episode: 91.42857142857143 avg 10: 78.89468614718615 dones: 7\n",
      "kld: 0.017307203263044357, actor_loss: 0.30066850781440735, critic_loss: 116.37956237792969, updates: 5.0, last_kld: 0.06776946783065796\n",
      "epoch: 16 return of episode: 115.33333333333333 avg 10: 85.85659090909091 dones: 6\n",
      "kld: 0.01146695762872696, actor_loss: -0.26005783677101135, critic_loss: 166.20608520507812, updates: 7.0, last_kld: 0.05103576183319092\n",
      "epoch: 17 return of episode: 122.2 avg 10: 91.9675 dones: 5\n",
      "kld: 0.009486615657806396, actor_loss: -2.738056182861328, critic_loss: 101.12660217285156, updates: 8.0, last_kld: 0.05204232037067413\n",
      "epoch: 18 return of episode: 152.75 avg 10: 100.2425 dones: 4\n",
      "kld: 0.004933287389576435, actor_loss: -2.560119867324829, critic_loss: 112.64588165283203, updates: 7.0, last_kld: 0.10858403891324997\n",
      "epoch: 19 return of episode: 181.33333333333334 avg 10: 113.23416666666667 dones: 3\n",
      "kld: 0.010053995065391064, actor_loss: -3.044185161590576, critic_loss: 92.74746704101562, updates: 4.0, last_kld: 0.06155042722821236\n",
      "epoch: 20 return of episode: 173.75 avg 10: 123.24249999999999 dones: 4\n",
      "kld: 0.012911666184663773, actor_loss: -1.0883302688598633, critic_loss: 57.86692810058594, updates: 22.0, last_kld: 0.06844789534807205\n",
      "epoch: 21 return of episode: 171.75 avg 10: 131.06035714285713 dones: 4\n",
      "kld: 0.02246720716357231, actor_loss: -0.06752239167690277, critic_loss: 120.47782135009766, updates: 19.0, last_kld: 0.07164772599935532\n",
      "epoch: 22 return of episode: 333.0 avg 10: 153.20035714285714 dones: 2\n",
      "kld: 0.005783659406006336, actor_loss: -4.210278511047363, critic_loss: 32.535091400146484, updates: 40.0, last_kld: 0.0036918444093316793\n",
      "epoch: 23 return of episode: 326.5 avg 10: 177.13785714285714 dones: 2\n",
      "kld: 0.00825381837785244, actor_loss: -2.476818323135376, critic_loss: 37.383697509765625, updates: 40.0, last_kld: 0.00205000932328403\n",
      "epoch: 24 return of episode: 233.0 avg 10: 190.1045238095238 dones: 1\n",
      "kld: 0.010595126077532768, actor_loss: -2.556863307952881, critic_loss: 16.020587921142578, updates: 40.0, last_kld: 0.013053219765424728\n",
      "epoch: 25 return of episode: 409.0 avg 10: 221.86166666666668 dones: 2\n",
      "kld: 0.00633587408810854, actor_loss: -0.2057660073041916, critic_loss: 95.67643737792969, updates: 10.0, last_kld: 0.11723460257053375\n",
      "epoch: 26 return of episode: 695.0 avg 10: 279.8283333333333 dones: 1\n",
      "kld: 0.013349806889891624, actor_loss: -3.3183445930480957, critic_loss: 19.120832443237305, updates: 40.0, last_kld: 0.018662456423044205\n",
      "epoch: 27 return of episode: 830.0 avg 10: 350.60833333333335 dones: 0\n",
      "kld: 0.015897106379270554, actor_loss: -3.9852142333984375, critic_loss: 4.1837358474731445, updates: 40.0, last_kld: 0.03205101937055588\n",
      "epoch: 28 return of episode: 1000.0 avg 10: 435.33333333333337 dones: 1\n",
      "kld: 0.015412086620926857, actor_loss: -0.7272682189941406, critic_loss: 73.56151580810547, updates: 11.0, last_kld: 0.11930472403764725\n",
      "epoch: 29 return of episode: 874.0 avg 10: 504.6 dones: 1\n",
      "kld: 0.01312338374555111, actor_loss: -0.7970479726791382, critic_loss: 20.29330062866211, updates: 2.0, last_kld: 0.10925957560539246\n",
      "epoch: 30 return of episode: 200.25 avg 10: 507.25 dones: 4\n",
      "kld: 0.012422211468219757, actor_loss: 2.599541664123535, critic_loss: 68.45516204833984, updates: 2.0, last_kld: 0.06183863803744316\n",
      "epoch: 31 return of episode: 148.75 avg 10: 504.95 dones: 4\n",
      "kld: 0.016069665551185608, actor_loss: 3.5698459148406982, critic_loss: 61.99068832397461, updates: 16.0, last_kld: 0.10351024568080902\n",
      "epoch: 32 return of episode: 157.0 avg 10: 487.35 dones: 4\n",
      "kld: 0.01039545051753521, actor_loss: 2.604376792907715, critic_loss: 51.959083557128906, updates: 10.0, last_kld: 0.05217840522527695\n",
      "epoch: 33 return of episode: 216.0 avg 10: 476.3 dones: 3\n",
      "kld: 0.014838960953056812, actor_loss: -0.549821138381958, critic_loss: 68.25462341308594, updates: 5.0, last_kld: 0.13513459265232086\n",
      "epoch: 34 return of episode: 505.0 avg 10: 503.5 dones: 1\n",
      "kld: 0.010552064515650272, actor_loss: -3.072230815887451, critic_loss: 20.14824867248535, updates: 40.0, last_kld: 0.0037132971920073032\n",
      "epoch: 35 return of episode: 736.0 avg 10: 536.2 dones: 1\n",
      "kld: 0.004629658069461584, actor_loss: -2.013697624206543, critic_loss: 15.716524124145508, updates: 40.0, last_kld: 0.0059181032702326775\n",
      "epoch: 36 return of episode: 803.0 avg 10: 547.0 dones: 0\n",
      "kld: 0.007165995426476002, actor_loss: -1.5176453590393066, critic_loss: 1.5884809494018555, updates: 40.0, last_kld: 0.011232932098209858\n",
      "epoch: 37 return of episode: 1000.0 avg 10: 564.0 dones: 1\n",
      "kld: 0.011324471794068813, actor_loss: 1.7816754579544067, critic_loss: 117.29519653320312, updates: 13.0, last_kld: 0.06057990714907646\n",
      "epoch: 38 return of episode: 1000.0 avg 10: 564.0 dones: 1\n",
      "kld: 0.0163028035312891, actor_loss: 1.4830886125564575, critic_loss: 105.0323486328125, updates: 8.0, last_kld: 0.08306072652339935\n",
      "epoch: 39 return of episode: 723.0 avg 10: 548.9 dones: 0\n",
      "kld: 0.00760781392455101, actor_loss: -1.558406114578247, critic_loss: 1.9159294366836548, updates: 40.0, last_kld: 0.00938335806131363\n",
      "epoch: 40 return of episode: 585.5 avg 10: 587.425 dones: 2\n",
      "kld: 0.012618392705917358, actor_loss: 4.3863844871521, critic_loss: 230.3573455810547, updates: 6.0, last_kld: 0.05455601215362549\n",
      "epoch: 41 return of episode: 832.0 avg 10: 655.75 dones: 0\n",
      "kld: 0.018000749871134758, actor_loss: -2.2931580543518066, critic_loss: 2.801084041595459, updates: 40.0, last_kld: 0.027004893869161606\n",
      "epoch: 42 return of episode: 1000.0 avg 10: 740.05 dones: 1\n",
      "kld: 0.018898671492934227, actor_loss: 0.2185383290052414, critic_loss: 107.71900177001953, updates: 24.0, last_kld: 0.08533471822738647\n",
      "epoch: 43 return of episode: 1000.0 avg 10: 818.45 dones: 1\n",
      "kld: 0.021280519664287567, actor_loss: 0.24908004701137543, critic_loss: 94.17964172363281, updates: 11.0, last_kld: 0.12391162663698196\n",
      "epoch: 44 return of episode: 752.0 avg 10: 843.15 dones: 0\n",
      "kld: 0.006189192645251751, actor_loss: -2.1948301792144775, critic_loss: 1.8028494119644165, updates: 40.0, last_kld: 0.002285290975123644\n",
      "epoch: 45 return of episode: 1000.0 avg 10: 869.55 dones: 1\n",
      "kld: 0.01698671095073223, actor_loss: 0.24939782917499542, critic_loss: 101.9770736694336, updates: 31.0, last_kld: 0.050933197140693665\n",
      "epoch: 46 return of episode: 1000.0 avg 10: 889.25 dones: 1\n",
      "kld: 0.00824064016342163, actor_loss: 0.9269111752510071, critic_loss: 119.54469299316406, updates: 7.0, last_kld: 0.06834381818771362\n",
      "epoch: 47 return of episode: 672.0 avg 10: 856.45 dones: 0\n",
      "kld: 0.00565330358222127, actor_loss: -2.03629994392395, critic_loss: 1.0860596895217896, updates: 40.0, last_kld: 0.007387848105281591\n",
      "epoch: 48 return of episode: 1000.0 avg 10: 856.45 dones: 1\n",
      "kld: 0.014209464192390442, actor_loss: 0.6242620944976807, critic_loss: 105.79424285888672, updates: 22.0, last_kld: 0.06799723953008652\n",
      "epoch: 49 return of episode: 952.0 avg 10: 879.35 dones: 0\n",
      "kld: 0.0031718015670776367, actor_loss: -1.891126036643982, critic_loss: 1.959954023361206, updates: 40.0, last_kld: 0.0029599762056022882\n",
      "training finished!\n"
     ]
    }
   ]
  }
 ]
}