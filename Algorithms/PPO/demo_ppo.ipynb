{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PPO DEMO\n",
    "This notebook guides you on how to use the PPO implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing your one ActorCriticPolicy with actor and critic networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementing a gaussian policy for continuous actions spaces by inheriting form GaussianActorCriticPolicy and using the tensorflow subclassing methode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from policies import GaussianActorCriticPolicy\n",
    "\n",
    "\n",
    "class MyMlpGaussianActorCriticPolicyIndependentSigma(GaussianActorCriticPolicy):\n",
    "\n",
    "    def create_actor_critic_network(self):\n",
    "        class MyMlpModel(keras.Model):\n",
    "\n",
    "            def __init__(self, action_dim):\n",
    "                super(MyMlpModel, self).__init__()\n",
    "\n",
    "                self.mu_0 = Dense(256, activation=tf.nn.relu)\n",
    "                self.mu_1 = Dense(128, activation=tf.nn.relu)\n",
    "                self.mu_2 = Dense(64, activation=tf.nn.relu)\n",
    "                self.mu_out = Dense(action_dim, activation=None)\n",
    "\n",
    "                self.sigma = tf.Variable(initial_value=tf.zeros(action_dim), trainable=True)\n",
    "\n",
    "                self.v_0 = Dense(256, activation=tf.nn.relu)\n",
    "                self.v_1 = Dense(128, activation=tf.nn.relu)\n",
    "                self.v_2 = Dense(64, activation=tf.nn.relu)\n",
    "                self.value = Dense(1, activation=None)\n",
    "\n",
    "            @tf.function\n",
    "            def call(self, inputs):\n",
    "                x = self.mu_0(inputs)\n",
    "                x = self.mu_1(x)\n",
    "                x = self.mu_2(x)\n",
    "                mu = self.mu_out(x)\n",
    "\n",
    "                y = self.v_0(inputs)\n",
    "                y = self.v_1(y)\n",
    "                y = self.v_2(y)\n",
    "                va = self.value(y)\n",
    "                return mu, self.sigma, va\n",
    "\n",
    "        return MyMlpModel(self._action_dim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementing policy for discrete actions spaces by inheriting form DiscreteActorCriticPolicy and using the tensorflow functional API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from policies import DiscreteActorCriticPolicy\n",
    "\n",
    "\n",
    "class MyMlpDiscreteActorCriticPolicy(DiscreteActorCriticPolicy):\n",
    "    def __init__(self, state_dim, n_actions, shared_networks=False):\n",
    "        self._shared_networks = shared_networks\n",
    "        super().__init__(state_dim, n_actions)\n",
    "\n",
    "    def create_actor_critic_network(self):\n",
    "        if self._shared_networks:\n",
    "            return self._creat_network()\n",
    "        else:\n",
    "            return self._creat_network_separate()\n",
    "\n",
    "    def _creat_network(self):\n",
    "        inputs = keras.Input(shape=self._state_dim)\n",
    "        x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "        x = Dense(128, activation=tf.nn.relu)(x)\n",
    "        x = Dense(64, activation=tf.nn.relu)(x)\n",
    "        value = Dense(1, activation=None)(x)\n",
    "        logits = Dense(self._n_actions, activation=None)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=(logits, value))\n",
    "        return model\n",
    "\n",
    "    def _creat_network_separate(self):\n",
    "        inputs = keras.Input(shape=self._state_dim)\n",
    "\n",
    "        x = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "        x = Dense(128, activation=tf.nn.relu)(x)\n",
    "        x = Dense(64, activation=tf.nn.relu)(x)\n",
    "        logits = Dense(self._n_actions, activation=None)(x)\n",
    "\n",
    "        y = Dense(256, activation=tf.nn.relu)(inputs)\n",
    "        y = Dense(128, activation=tf.nn.relu)(y)\n",
    "        y = Dense(64, activation=tf.nn.relu)(y)\n",
    "        value = Dense(1, activation=None)(y)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=(logits, value))\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "\n",
    "env_name = \"InvertedPendulum-v4\"  # define environment to use\n",
    "# env_name = \"CartPole-v1\"  # discrete alternative (requires to install gymnasium[box2d])\n",
    "discrete = False\n",
    "num_envs = 4  # number of vectorized environments\n",
    "network_type = \"mlp\"  # define the network type that should be used\n",
    "log_dir = f'logs/{env_name}/PPO_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the environment!\n",
    "Training with RNNs is also possible using the FrameStack wrapper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FrameStack\n",
    "\n",
    "window_size = None  # number of environment observation unified in one window\n",
    "if window_size is not None:\n",
    "    envs = [lambda: FrameStack(gym.make(env_name), window_size) for _ in range(num_envs)]\n",
    "    env = gym.vector.SyncVectorEnv(envs)  # oder: env = gym.vector.AsyncVectorEnv(envs)\n",
    "else:\n",
    "    env = gym.vector.make(env_name, num_envs=num_envs, asynchronous=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the policy to be used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import default policies for CNNs and LSTMs\n",
    "from policies import CnnGaussianActorCriticPolicyIndependentSigma, \\\n",
    "    LstmGaussianActorCriticPolicy\n",
    "\n",
    "if discrete:\n",
    "    if network_type == \"mlp\":\n",
    "        policy = MyMlpDiscreteActorCriticPolicy(n_actions=env.single_action_space.n,\n",
    "                                                state_dim=env.single_observation_space.shape)\n",
    "    else:\n",
    "        raise Exception(f\"Unknown network type {network_type}\")\n",
    "else:\n",
    "    if network_type == \"cnn\":\n",
    "        policy = CnnGaussianActorCriticPolicyIndependentSigma(action_dim=env.single_action_space.shape[0],\n",
    "                                                              state_dim=env.single_observation_space.shape,\n",
    "                                                              action_space=env.single_action_space)\n",
    "    elif network_type == \"rnn\":\n",
    "        policy = LstmGaussianActorCriticPolicy(action_dim=env.single_action_space.shape[0],\n",
    "                                               state_dim=env.single_observation_space.shape,\n",
    "                                               action_space=env.single_action_space)\n",
    "    elif network_type == \"mlp\":\n",
    "        policy = MyMlpGaussianActorCriticPolicyIndependentSigma(action_dim=env.single_action_space.shape[0],\n",
    "                                                                state_dim=env.single_observation_space.shape,\n",
    "                                                                action_space=env.single_action_space)\n",
    "    else:\n",
    "        raise Exception(f\"Unknown network type {network_type}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using tensorboard to monitor your progress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start the training and define hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "agent = Agent(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    normalize_adv=False,\n",
    "    log_dir=log_dir,\n",
    "    verbose=True,\n",
    "    batch_size=256,\n",
    "    data_set_repeats=4,\n",
    "    steps_per_epoch=2048\n",
    ")\n",
    "agent.train(epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
